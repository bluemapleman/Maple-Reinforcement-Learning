{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Box(4,)\n",
      "[ 0.78354603  0.92732555 -0.23311697  0.58345008]\n",
      "[ 0.05778984  0.13608912  0.85119325 -0.85792786]\n",
      "[-0.82574141 -0.9595632   0.66523969  0.55631351]\n",
      "[ 0.74002427  0.95723671  0.59831715 -0.07704128]\n",
      "[ 0.56105834 -0.76345116  0.27984205 -0.71329343]\n",
      "[ 0.88933784  0.04369664 -0.17067613 -0.47088876]\n",
      "[ 0.5484674  -0.08769934  0.1368679  -0.9624204 ]\n",
      "[ 0.23527099  0.22419144  0.23386799  0.88749617]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env=gym.make('BipedalWalker-v2')\n",
    "\n",
    "# env=gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "print(env.action_space)\n",
    "s=env.reset()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "#     env.render()\n",
    "#     \n",
    "    a=env.action_space.sample();\n",
    "    s1,r,done,_=env.step(a)\n",
    "    print(a)\n",
    "#     \n",
    "#     print(s1)\n",
    "#     print(r)\n",
    "#     print(done)\n",
    "#     \n",
    "#     if done:\n",
    "#         s=env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"markdown-body\">\n",
    "          <h2>\n",
    "<a id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Overview</h2>\n",
    "<h3>\n",
    "<a id=\"user-content-details\" class=\"anchor\" href=\"#details\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Details</h3>\n",
    "<ul>\n",
    "<li>Name: BipedalWalker-v2</li>\n",
    "<li>Category: Box2D</li>\n",
    "<li><a href=\"https://github.com/openai/gym/wiki/Leaderboard#bipedalwalker-v2\">Leaderboard Page</a></li>\n",
    "</ul>\n",
    "<h3>\n",
    "<a id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Description</h3>\n",
    "<p>Get a 2D biped walker to walk through rough terrain.</p>\n",
    "<h3>\n",
    "<a id=\"user-content-environment\" class=\"anchor\" href=\"#environment\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Environment</h3>\n",
    "<h2>\n",
    "<a id=\"user-content-observation\" class=\"anchor\" href=\"#observation\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Observation</h2>\n",
    "<p>Type: Box(24)</p>\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Num</th>\n",
    "<th>Observation</th>\n",
    "<th>Min</th>\n",
    "<th>Max</th>\n",
    "<th>Mean</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>hull_angle</td>\n",
    "<td>0</td>\n",
    "<td>2*pi</td>\n",
    "<td>0.5</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>hull_angularVelocity</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>vel_x</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>vel_y</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td>\n",
    "<td>hip_joint_1_angle</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td>\n",
    "<td>hip_joint_1_speed</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>6</td>\n",
    "<td>knee_joint_1_angle</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>7</td>\n",
    "<td>knee_joint_1_speed</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>8</td>\n",
    "<td>leg_1_ground_contact_flag</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>9</td>\n",
    "<td>hip_joint_2_angle</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10</td>\n",
    "<td>hip_joint_2_speed</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>11</td>\n",
    "<td>knee_joint_2_angle</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>12</td>\n",
    "<td>knee_joint_2_speed</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>13</td>\n",
    "<td>leg_2_ground_contact_flag</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>14-23</td>\n",
    "<td>10 lidar readings</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<h2>\n",
    "<a id=\"user-content-actions\" class=\"anchor\" href=\"#actions\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Actions</h2>\n",
    "<p>Type: Box(4) - Torque control(default) / Velocity control - Change inside <code>/envs/box2d/bipedal_walker.py</code> line 363</p>\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Num</th>\n",
    "<th>Name</th>\n",
    "<th>Min</th>\n",
    "<th>Max</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>Hip_1 (Torque / Velocity)</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>Knee_1 (Torque / Velocity)</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>Hip_2 (Torque / Velocity)</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>Knee_2 (Torque / Velocity)</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<h2>\n",
    "<a id=\"user-content-reward\" class=\"anchor\" href=\"#reward\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reward</h2>\n",
    "<p>Reward is given for moving forward, total 300+ points up to the far end. If the robot falls, it gets -100. Applying motor torque costs a small amount of points, more optimal agent will get better score. State consists of hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements. There's no coordinates in the state vector.</p>\n",
    "<h2>\n",
    "<a id=\"user-content-starting-state\" class=\"anchor\" href=\"#starting-state\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Starting State</h2>\n",
    "<p>Random position upright and mostly straight legs.</p>\n",
    "<h2>\n",
    "<a id=\"user-content-episode-termination\" class=\"anchor\" href=\"#episode-termination\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Episode Termination</h2>\n",
    "<p>The episode ends when the robot body touches ground or the robot reaches far right side of the environment.</p>\n",
    "<h2>\n",
    "<a id=\"user-content-solved-requirements\" class=\"anchor\" href=\"#solved-requirements\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Solved Requirements</h2>\n",
    "<p>BipedalWalker-v2 defines \"solving\" as getting average reward of 300 over 100 consecutive trials</p>\n",
    "\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN-based Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-59d708cfb23d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# 获得行动后新的状态、回报、游戏是否结束等信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# 通过将新的状态向量输入到网络中获得新状态对应的Q值。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_KNEE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_HIP\u001b[0m     \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxMotorTorque\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMOTORS_TORQUE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_KNEE\u001b[0m    \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "\n",
    "# 实现基于神经网络的Q-Learning\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# BipedalWalker环境\n",
    "env=gym.make('BipedalWalker-v2')\n",
    "\n",
    "N_S=env.observation_space.shape[0]\n",
    "\n",
    "# 该环境的action不是Discrete的行动，而是Box，即组合行动\n",
    "N_A=env.action_space.shape[0]\n",
    "\n",
    "# Q网络方法\n",
    "\n",
    "\n",
    "# 初始化tf计算图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 下面的几行代码建立了网络的前馈部分，它将用于选择行动\n",
    "inputs1 = tf.placeholder(shape=[1,N_S],dtype=tf.float32) # 定义输入向量；shape=[1,16]表示一个1*16的列向量，对应着冰湖环境中的16个方格/状态\n",
    "W = tf.Variable(tf.random_uniform([N_S,N_A],0,0.01)) # 定义权重矩阵（此处输入向量是不断要输入新的值的，因此一般用placeholder来承载；而需要训练的参数如w,b都应该用Variable类来定义，因为Variable对象菜会在tf后续的训练中被优化）\n",
    "Qout = tf.matmul(inputs1,W) # 输入向量*输入层到输出层的权重矩阵\n",
    "predict = tf.argmax(Qout,1) # 预测将要选择的行动\n",
    "\n",
    "# 下面的几行代码可以获得预测Q值与目标Q值间差值的平方和加总的损失。\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,N_A],dtype=tf.float32) # 定义更新后的Q值向量\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout)) #计算差值向量的总和（损失函数的定义）\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1) # 用梯度下降来训练，学习率设置为0.1\n",
    "updateModel = trainer.minimize(loss) # 训练模型（trianer最小化损失函数），minimize会自动更新Trainable的Variable类型的参数以最小化损失函数\n",
    "\n",
    "# 训练网络\n",
    "init = tf.initializers.global_variables() # 初始化全局参数\n",
    "\n",
    "# 设置学习参数\n",
    "y = .99  # 折现率\n",
    "e = 0.1  # 随机行动的概率\n",
    "num_episodes = 2000\n",
    "\n",
    "# 创建列表以包含每个episode对应的总回报与总步数。\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "# 启动session；session是运行operation和对Tensor进行估值的必须环境\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # 实际执行初始化\n",
    "    for i in range(num_episodes):\n",
    "        # 初始化环境并获得初始状态\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        \n",
    "        # Q网络\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            \n",
    "            # 基于Q网络的输出结果，贪婪地选择一个行动（有一定的概率选择随机行动）\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.reshape(s,[1,N_S])})\n",
    "            \n",
    "            # 基于随机数决定是否随机行动\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "            \n",
    "            \n",
    "            print(a[0])\n",
    "            # 获得行动后新的状态、回报、游戏是否结束等信息\n",
    "            s1,r,d,_ = env.step(a[0])\n",
    "\n",
    "            # 通过将新的状态向量输入到网络中获得新状态对应的Q值。\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:np.reshape(s1,[1,N_S])})\n",
    "\n",
    "            # 获得最大的Q值\n",
    "            # Recall: Q(s,a) = Q(s,a) + lr*(r + y*max_a(Q(s1,a)) - Q(s,a))\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            \n",
    "\n",
    "            # 用目标和预测的Q值训练网络\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.reshape(s,[1,N_S]),nextQ:targetQ})\n",
    "            \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                # 随着训练的进行，逐渐减少选择随机行为的概率\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "print(\"成功得分的局数占比: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testAgent(QTable=None,test_episodes=30):\n",
    "    jList = []\n",
    "    rList = []\n",
    "    \n",
    "    trained=True\n",
    "    # random agent\n",
    "    if QTable is None:\n",
    "        trained=False\n",
    "        \n",
    "    for i in range(test_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        \n",
    "        while j<99:\n",
    "\n",
    "            j+=1\n",
    "            \n",
    "            if trained:\n",
    "                # 基于Q表贪婪地选择一个最优行动（这里去掉了之前训练过程中加入的噪音干扰）\n",
    "                a = np.argmax(Q[s,:])\n",
    "            else:\n",
    "                a=env.action_space.sample()\n",
    "            \n",
    "            s,r,d,_=env.step(a)\n",
    "            \n",
    "            rAll+=r;\n",
    "            \n",
    "            # 判断游戏是否已经结束\n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    return jList,rList\n",
    "print('基于学习到的Q值表，测试agent的性能：')\n",
    "print()\n",
    "\n",
    "print('随机行动/未经训练的agent得分率：')\n",
    "randomAgentResult=testAgent()\n",
    "print(sum(randomAgentResult[1])/len(randomAgentResult[1]))\n",
    "\n",
    "\n",
    "print('Q-Learning agent得分率：')\n",
    "QLearningAgentResult=testAgent(Q)\n",
    "print(sum(QLearningAgentResult[1])/len(QLearningAgentResult[1]))\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
