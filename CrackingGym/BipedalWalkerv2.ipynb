{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env=gym.make('BipedalWalker-v2')\n",
    "\n",
    "# env=gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "print(env.action_space)\n",
    "s=env.reset()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2000):\n",
    "    env.render()\n",
    "#     \n",
    "    a=env.action_space.sample();\n",
    "    s1,r,done,_=env.step(a)\n",
    "#     print(a)\n",
    "#     \n",
    "#     print(s1)\n",
    "#     print(r)\n",
    "#     print(done)\n",
    "#     \n",
    "    if done:\n",
    "        s=env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"markdown-body\">\n",
    "          <h2>\n",
    "<a id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Overview</h2>\n",
    "<h3>\n",
    "<a id=\"user-content-details\" class=\"anchor\" href=\"#details\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Details</h3>\n",
    "<ul>\n",
    "<li>Name: BipedalWalker-v2</li>\n",
    "<li>Category: Box2D</li>\n",
    "<li><a href=\"https://github.com/openai/gym/wiki/Leaderboard#bipedalwalker-v2\">Leaderboard Page</a></li>\n",
    "</ul>\n",
    "<h3>\n",
    "<a id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Description</h3>\n",
    "<p>Get a 2D biped walker to walk through rough terrain.</p>\n",
    "<h3>\n",
    "<a id=\"user-content-environment\" class=\"anchor\" href=\"#environment\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Environment</h3>\n",
    "<h2>\n",
    "<a id=\"user-content-observation\" class=\"anchor\" href=\"#observation\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Observation</h2>\n",
    "<p>Type: Box(24)</p>\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Num</th>\n",
    "<th>Observation</th>\n",
    "<th>Min</th>\n",
    "<th>Max</th>\n",
    "<th>Mean</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>hull_angle</td>\n",
    "<td>0</td>\n",
    "<td>2*pi</td>\n",
    "<td>0.5</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>hull_angularVelocity</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>vel_x</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>vel_y</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td>\n",
    "<td>hip_joint_1_angle</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td>\n",
    "<td>hip_joint_1_speed</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>6</td>\n",
    "<td>knee_joint_1_angle</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>7</td>\n",
    "<td>knee_joint_1_speed</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>8</td>\n",
    "<td>leg_1_ground_contact_flag</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>9</td>\n",
    "<td>hip_joint_2_angle</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10</td>\n",
    "<td>hip_joint_2_speed</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>11</td>\n",
    "<td>knee_joint_2_angle</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>12</td>\n",
    "<td>knee_joint_2_speed</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>13</td>\n",
    "<td>leg_2_ground_contact_flag</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>14-23</td>\n",
    "<td>10 lidar readings</td>\n",
    "<td>-inf</td>\n",
    "<td>+inf</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<h2>\n",
    "<a id=\"user-content-actions\" class=\"anchor\" href=\"#actions\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Actions</h2>\n",
    "<p>Type: Box(4) - Torque control(default) / Velocity control - Change inside <code>/envs/box2d/bipedal_walker.py</code> line 363</p>\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Num</th>\n",
    "<th>Name</th>\n",
    "<th>Min</th>\n",
    "<th>Max</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>Hip_1 (Torque / Velocity)</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>Knee_1 (Torque / Velocity)</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>Hip_2 (Torque / Velocity)</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>Knee_2 (Torque / Velocity)</td>\n",
    "<td>-1</td>\n",
    "<td>+1</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<h2>\n",
    "<a id=\"user-content-reward\" class=\"anchor\" href=\"#reward\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reward</h2>\n",
    "<p>Reward is given for moving forward, total 300+ points up to the far end. If the robot falls, it gets -100. Applying motor torque costs a small amount of points, more optimal agent will get better score. State consists of hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements. There's no coordinates in the state vector.</p>\n",
    "<h2>\n",
    "<a id=\"user-content-starting-state\" class=\"anchor\" href=\"#starting-state\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Starting State</h2>\n",
    "<p>Random position upright and mostly straight legs.</p>\n",
    "<h2>\n",
    "<a id=\"user-content-episode-termination\" class=\"anchor\" href=\"#episode-termination\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Episode Termination</h2>\n",
    "<p>The episode ends when the robot body touches ground or the robot reaches far right side of the environment.</p>\n",
    "<h2>\n",
    "<a id=\"user-content-solved-requirements\" class=\"anchor\" href=\"#solved-requirements\" aria-hidden=\"true\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Solved Requirements</h2>\n",
    "<p>BipedalWalker-v2 defines \"solving\" as getting average reward of 300 over 100 consecutive trials</p>\n",
    "\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym：BipedalWalker-v2环境解决方案分析\n",
    "\n",
    "BipedalWalker-v2的action和Observation/state都是Box类型，这意味着agent的action是组合值，不可能通过Q-Learning来学习，因为Q-Learning的是Value-Based的方法，即是approximate value function，并输入state到approximated function中来获得Observation估值，并基于策略来选定action。简言之，Q-Learning/DQN只能处理动作是离散的情况，而该环境的动作是连续值。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Morvan Code: https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/5-1-policy-gradient-softmax1/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-59d708cfb23d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# 获得行动后新的状态、回报、游戏是否结束等信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# 通过将新的状态向量输入到网络中获得新状态对应的Q值。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_KNEE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_HIP\u001b[0m     \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxMotorTorque\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMOTORS_TORQUE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_KNEE\u001b[0m    \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "\n",
    "# 实现基于神经网络的Q-Learning\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# BipedalWalker环境\n",
    "env=gym.make('BipedalWalker-v2')\n",
    "\n",
    "N_S=env.observation_space.shape[0]\n",
    "\n",
    "# 该环境的action不是Discrete的行动，而是Box，即组合行动\n",
    "N_A=env.action_space.shape[0]\n",
    "\n",
    "# Q网络方法\n",
    "\n",
    "\n",
    "# 初始化tf计算图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 下面的几行代码建立了网络的前馈部分，它将用于选择行动\n",
    "inputs1 = tf.placeholder(shape=[1,N_S],dtype=tf.float32) # 定义输入向量；shape=[1,16]表示一个1*16的列向量，对应着冰湖环境中的16个方格/状态\n",
    "W = tf.Variable(tf.random_uniform([N_S,N_A],0,0.01)) # 定义权重矩阵（此处输入向量是不断要输入新的值的，因此一般用placeholder来承载；而需要训练的参数如w,b都应该用Variable类来定义，因为Variable对象菜会在tf后续的训练中被优化）\n",
    "Qout = tf.matmul(inputs1,W) # 输入向量*输入层到输出层的权重矩阵\n",
    "predict = tf.argmax(Qout,1) # 预测将要选择的行动\n",
    "\n",
    "# 下面的几行代码可以获得预测Q值与目标Q值间差值的平方和加总的损失。\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,N_A],dtype=tf.float32) # 定义更新后的Q值向量\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout)) #计算差值向量的总和（损失函数的定义）\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1) # 用梯度下降来训练，学习率设置为0.1\n",
    "updateModel = trainer.minimize(loss) # 训练模型（trianer最小化损失函数），minimize会自动更新Trainable的Variable类型的参数以最小化损失函数\n",
    "\n",
    "# 训练网络\n",
    "init = tf.initializers.global_variables() # 初始化全局参数\n",
    "\n",
    "# 设置学习参数\n",
    "y = .99  # 折现率\n",
    "e = 0.1  # 随机行动的概率\n",
    "num_episodes = 2000\n",
    "\n",
    "# 创建列表以包含每个episode对应的总回报与总步数。\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "# 启动session；session是运行operation和对Tensor进行估值的必须环境\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # 实际执行初始化\n",
    "    for i in range(num_episodes):\n",
    "        # 初始化环境并获得初始状态\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        \n",
    "        # Q网络\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            \n",
    "            # 基于Q网络的输出结果，贪婪地选择一个行动（有一定的概率选择随机行动）\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.reshape(s,[1,N_S])})\n",
    "            \n",
    "            # 基于随机数决定是否随机行动\n",
    "            if np.random.rand(1) < e:\n",
    "                a = env.action_space.sample()\n",
    "            \n",
    "            \n",
    "            print(a)\n",
    "            # 获得行动后新的状态、回报、游戏是否结束等信息\n",
    "            s1,r,d,_ = env.step(a)\n",
    "\n",
    "            # 通过将新的状态向量输入到网络中获得新状态对应的Q值。\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:np.reshape(s1,[1,N_S])})\n",
    "\n",
    "            # 获得最大的Q值\n",
    "            # Recall: Q(s,a) = Q(s,a) + lr*(r + y*max_a(Q(s1,a)) - Q(s,a))\n",
    "#             maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            \n",
    "\n",
    "            # 用目标和预测的Q值训练网络\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.reshape(s,[1,N_S]),nextQ:targetQ})\n",
    "            \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                # 随着训练的进行，逐渐减少选择随机行为的概率\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "print(\"成功得分的局数占比: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testAgent(QTable=None,test_episodes=30):\n",
    "    jList = []\n",
    "    rList = []\n",
    "    \n",
    "    trained=True\n",
    "    # random agent\n",
    "    if QTable is None:\n",
    "        trained=False\n",
    "        \n",
    "    for i in range(test_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        \n",
    "        while j<99:\n",
    "\n",
    "            j+=1\n",
    "            \n",
    "            if trained:\n",
    "                # 基于Q表贪婪地选择一个最优行动（这里去掉了之前训练过程中加入的噪音干扰）\n",
    "                a = np.argmax(Q[s,:])\n",
    "            else:\n",
    "                a=env.action_space.sample()\n",
    "            \n",
    "            s,r,d,_=env.step(a)\n",
    "            \n",
    "            rAll+=r;\n",
    "            \n",
    "            # 判断游戏是否已经结束\n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    return jList,rList\n",
    "print('基于学习到的Q值表，测试agent的性能：')\n",
    "print()\n",
    "\n",
    "print('随机行动/未经训练的agent得分率：')\n",
    "randomAgentResult=testAgent()\n",
    "print(sum(randomAgentResult[1])/len(randomAgentResult[1]))\n",
    "\n",
    "\n",
    "print('Q-Learning agent得分率：')\n",
    "QLearningAgentResult=testAgent(Q)\n",
    "print(sum(QLearningAgentResult[1])/len(QLearningAgentResult[1]))\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\n",
      "Save Model ./data/DDPG.ckpt-0\n",
      "\n",
      "Episode: 0 | ----- | Running_r: -171 | Epi_r: -171.56 | Exploration: 3.000 | Pos: 1 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 1 | ----- | Running_r: -170 | Epi_r: -153.37 | Exploration: 3.000 | Pos: 5 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 2 | ----- | Running_r: -163 | Epi_r: -26.06 | Exploration: 3.000 | Pos: 4 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 3 | ----- | Running_r: -163 | Epi_r: -161.95 | Exploration: 3.000 | Pos: 3 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 4 | ----- | Running_r: -156 | Epi_r: -22.42 | Exploration: 3.000 | Pos: 3 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 5 | ----- | Running_r: -148 | Epi_r: -5.18 | Exploration: 3.000 | Pos: 6 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 6 | ----- | Running_r: -141 | Epi_r: -11.57 | Exploration: 3.000 | Pos: 4 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 7 | ----- | Running_r: -141 | Epi_r: -142.47 | Exploration: 3.000 | Pos: 7 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 8 | ----- | Running_r: -135 | Epi_r: -22.04 | Exploration: 3.000 | Pos: 3 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 9 | ----- | Running_r: -130 | Epi_r: -26.08 | Exploration: 3.000 | Pos: 3 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 10 | ----- | Running_r: -131 | Epi_r: -147.43 | Exploration: 3.000 | Pos: 6 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 11 | ----- | Running_r: -125 | Epi_r: -18.00 | Exploration: 3.000 | Pos: 4 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 12 | ----- | Running_r: -120 | Epi_r: -20.91 | Exploration: 3.000 | Pos: 4 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 13 | ----- | Running_r: -115 | Epi_r: -26.33 | Exploration: 3.000 | Pos: 4 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 14 | ----- | Running_r: -110 | Epi_r: -16.50 | Exploration: 3.000 | Pos: 4 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 15 | ----- | Running_r: -105 | Epi_r: -9.36 | Exploration: 3.000 | Pos: 5 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 16 | ----- | Running_r: -103 | Epi_r: -68.41 | Exploration: 3.000 | Pos: 6 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 17 | ----- | Running_r: -99 | Epi_r: -23.99 | Exploration: 3.000 | Pos: 4 | LR_A: 0.000500 | LR_C: 0.000500\n",
      "Episode: 18 | ----- | Running_r: -101 | Epi_r: -124.39 | Exploration: 2.641 | Pos: 12 | LR_A: 0.000485 | LR_C: 0.000485\n",
      "Episode: 19 | ----- | Running_r: -96 | Epi_r: -11.14 | Exploration: 2.625 | Pos: 5 | LR_A: 0.000485 | LR_C: 0.000485\n",
      "Episode: 20 | ----- | Running_r: -92 | Epi_r: -6.79 | Exploration: 2.609 | Pos: 5 | LR_A: 0.000485 | LR_C: 0.000485\n",
      "Episode: 21 | ----- | Running_r: -87 | Epi_r: -7.21 | Exploration: 2.582 | Pos: 6 | LR_A: 0.000485 | LR_C: 0.000485\n",
      "Episode: 22 | ----- | Running_r: -89 | Epi_r: -127.38 | Exploration: 2.200 | Pos: 9 | LR_A: 0.000485 | LR_C: 0.000485\n",
      "Episode: 23 | ----- | Running_r: -92 | Epi_r: -151.86 | Exploration: 1.875 | Pos: 3 | LR_A: 0.000470 | LR_C: 0.000470\n",
      "Episode: 24 | ----- | Running_r: -89 | Epi_r: -31.42 | Exploration: 1.851 | Pos: 1 | LR_A: 0.000470 | LR_C: 0.000470\n",
      "Episode: 25 | ----- | Running_r: -85 | Epi_r: -3.65 | Exploration: 1.835 | Pos: 6 | LR_A: 0.000470 | LR_C: 0.000470\n",
      "Episode: 26 | ----- | Running_r: -82 | Epi_r: -19.57 | Exploration: 1.818 | Pos: 3 | LR_A: 0.000470 | LR_C: 0.000470\n",
      "Episode: 27 | ----- | Running_r: -83 | Epi_r: -110.60 | Exploration: 1.550 | Pos: 11 | LR_A: 0.000470 | LR_C: 0.000470\n",
      "Episode: 28 | ----- | Running_r: -79 | Epi_r: -6.11 | Exploration: 1.535 | Pos: 6 | LR_A: 0.000456 | LR_C: 0.000456\n",
      "Episode: 29 | ----- | Running_r: -76 | Epi_r: -6.14 | Exploration: 1.525 | Pos: 5 | LR_A: 0.000456 | LR_C: 0.000456\n",
      "Episode: 30 | ----- | Running_r: -72 | Epi_r: -6.19 | Exploration: 1.515 | Pos: 5 | LR_A: 0.000456 | LR_C: 0.000456\n",
      "Episode: 31 | ----- | Running_r: -74 | Epi_r: -113.06 | Exploration: 1.291 | Pos: 9 | LR_A: 0.000456 | LR_C: 0.000456\n",
      "Episode: 32 | ----- | Running_r: -75 | Epi_r: -84.55 | Exploration: 1.100 | Pos: 14 | LR_A: 0.000443 | LR_C: 0.000443\n",
      "Episode: 33 | ----- | Running_r: -71 | Epi_r: -8.08 | Exploration: 1.087 | Pos: 5 | LR_A: 0.000443 | LR_C: 0.000443\n",
      "Episode: 34 | ----- | Running_r: -69 | Epi_r: -31.56 | Exploration: 1.072 | Pos: 1 | LR_A: 0.000443 | LR_C: 0.000443\n",
      "Episode: 35 | ----- | Running_r: -66 | Epi_r: -3.43 | Exploration: 1.065 | Pos: 6 | LR_A: 0.000443 | LR_C: 0.000443\n",
      "Episode: 36 | ----- | Running_r: -63 | Epi_r: -2.30 | Exploration: 1.059 | Pos: 6 | LR_A: 0.000443 | LR_C: 0.000443\n",
      "Episode: 37 | ----- | Running_r: -63 | Epi_r: -75.22 | Exploration: 0.902 | Pos: 14 | LR_A: 0.000443 | LR_C: 0.000443\n",
      "Episode: 38 | ----- | Running_r: -63 | Epi_r: -63.31 | Exploration: 0.769 | Pos: 15 | LR_A: 0.000429 | LR_C: 0.000429\n",
      "Episode: 39 | ----- | Running_r: -61 | Epi_r: -13.41 | Exploration: 0.765 | Pos: 3 | LR_A: 0.000429 | LR_C: 0.000429\n",
      "Episode: 40 | ----- | Running_r: -61 | Epi_r: -66.29 | Exploration: 0.652 | Pos: 13 | LR_A: 0.000429 | LR_C: 0.000429\n",
      "Episode: 41 | ----- | Running_r: -61 | Epi_r: -61.84 | Exploration: 0.555 | Pos: 13 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 42 | ----- | Running_r: -59 | Epi_r: -13.22 | Exploration: 0.552 | Pos: 3 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 43 | ----- | Running_r: -56 | Epi_r: -13.98 | Exploration: 0.548 | Pos: 3 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 44 | ----- | Running_r: -56 | Epi_r: -45.44 | Exploration: 0.467 | Pos: 15 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 45 | ----- | Running_r: -53 | Epi_r: 0.08 | Exploration: 0.463 | Pos: 7 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 46 | ----- | Running_r: -51 | Epi_r: -6.12 | Exploration: 0.460 | Pos: 5 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 47 | ----- | Running_r: -48 | Epi_r: -5.16 | Exploration: 0.457 | Pos: 5 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 48 | ----- | Running_r: -46 | Epi_r: -2.18 | Exploration: 0.453 | Pos: 6 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 49 | ----- | Running_r: -44 | Epi_r: -3.77 | Exploration: 0.449 | Pos: 5 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 50 | ----- | Running_r: -42 | Epi_r: -11.21 | Exploration: 0.446 | Pos: 4 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 51 | ----- | Running_r: -41 | Epi_r: -15.56 | Exploration: 0.443 | Pos: 3 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 52 | ----- | Running_r: -39 | Epi_r: 2.12 | Exploration: 0.439 | Pos: 7 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 53 | ----- | Running_r: -38 | Epi_r: -26.95 | Exploration: 0.430 | Pos: 3 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 54 | ----- | Running_r: -37 | Epi_r: -15.94 | Exploration: 0.425 | Pos: 3 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 55 | ----- | Running_r: -36 | Epi_r: -15.02 | Exploration: 0.420 | Pos: 4 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 56 | ----- | Running_r: -34 | Epi_r: -3.61 | Exploration: 0.417 | Pos: 5 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 57 | ----- | Running_r: -33 | Epi_r: -6.00 | Exploration: 0.414 | Pos: 5 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 58 | ----- | Running_r: -31 | Epi_r: -1.62 | Exploration: 0.412 | Pos: 5 | LR_A: 0.000416 | LR_C: 0.000416\n",
      "Episode: 59 | ----- | Running_r: -30 | Epi_r: -12.67 | Exploration: 0.395 | Pos: 8 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 60 | ----- | Running_r: -30 | Epi_r: -27.88 | Exploration: 0.388 | Pos: 1 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 61 | ----- | Running_r: -30 | Epi_r: -31.52 | Exploration: 0.379 | Pos: 1 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 62 | ----- | Running_r: -30 | Epi_r: -25.62 | Exploration: 0.366 | Pos: 5 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 63 | ----- | Running_r: -28 | Epi_r: 1.65 | Exploration: 0.356 | Pos: 9 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 64 | ----- | Running_r: -27 | Epi_r: -5.56 | Exploration: 0.354 | Pos: 5 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 65 | ----- | Running_r: -26 | Epi_r: -7.67 | Exploration: 0.352 | Pos: 5 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 66 | ----- | Running_r: -26 | Epi_r: -18.80 | Exploration: 0.334 | Pos: 8 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 67 | ----- | Running_r: -25 | Epi_r: -5.97 | Exploration: 0.331 | Pos: 5 | LR_A: 0.000404 | LR_C: 0.000404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 68 | ----- | Running_r: -24 | Epi_r: -2.30 | Exploration: 0.329 | Pos: 6 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 69 | ----- | Running_r: -22 | Epi_r: -2.38 | Exploration: 0.325 | Pos: 6 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 70 | ----- | Running_r: -22 | Epi_r: -21.27 | Exploration: 0.306 | Pos: 9 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 71 | ----- | Running_r: -22 | Epi_r: -7.19 | Exploration: 0.305 | Pos: 4 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 72 | ----- | Running_r: -21 | Epi_r: -15.95 | Exploration: 0.301 | Pos: 3 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 73 | ----- | Running_r: -20 | Epi_r: -4.64 | Exploration: 0.300 | Pos: 5 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 74 | ----- | Running_r: -19 | Epi_r: 0.38 | Exploration: 0.297 | Pos: 7 | LR_A: 0.000404 | LR_C: 0.000404\n",
      "Episode: 75 | ----- | Running_r: -20 | Epi_r: -41.09 | Exploration: 0.276 | Pos: 6 | LR_A: 0.000392 | LR_C: 0.000392\n",
      "Episode: 76 | ----- | Running_r: -20 | Epi_r: -18.22 | Exploration: 0.274 | Pos: 3 | LR_A: 0.000392 | LR_C: 0.000392\n",
      "Episode: 77 | ----- | Running_r: -19 | Epi_r: -3.23 | Exploration: 0.272 | Pos: 6 | LR_A: 0.000392 | LR_C: 0.000392\n",
      "Episode: 78 | ----- | Running_r: -20 | Epi_r: -25.09 | Exploration: 0.258 | Pos: 7 | LR_A: 0.000392 | LR_C: 0.000392\n",
      "Episode: 79 | ----- | Running_r: -19 | Epi_r: -3.82 | Exploration: 0.256 | Pos: 5 | LR_A: 0.000392 | LR_C: 0.000392\n",
      "Episode: 80 | ----- | Running_r: -18 | Epi_r: -9.06 | Exploration: 0.254 | Pos: 5 | LR_A: 0.000392 | LR_C: 0.000392\n",
      "Episode: 81 | ----- | Running_r: -18 | Epi_r: -2.89 | Exploration: 0.252 | Pos: 6 | LR_A: 0.000392 | LR_C: 0.000392\n",
      "Episode: 82 | ----- | Running_r: -17 | Epi_r: -3.07 | Exploration: 0.250 | Pos: 6 | LR_A: 0.000392 | LR_C: 0.000392\n",
      "Episode: 83 | ----- | Running_r: -17 | Epi_r: -16.81 | Exploration: 0.240 | Pos: 7 | LR_A: 0.000392 | LR_C: 0.000392\n",
      "Episode: 84 | ----- | Running_r: -16 | Epi_r: -6.66 | Exploration: 0.237 | Pos: 6 | LR_A: 0.000392 | LR_C: 0.000392\n",
      "Episode: 85 | ----- | Running_r: -19 | Epi_r: -76.09 | Exploration: 0.202 | Pos: 5 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 86 | ----- | Running_r: -19 | Epi_r: -22.95 | Exploration: 0.191 | Pos: 6 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 87 | ----- | Running_r: -19 | Epi_r: -3.90 | Exploration: 0.190 | Pos: 5 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 88 | ----- | Running_r: -18 | Epi_r: -2.25 | Exploration: 0.188 | Pos: 6 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 89 | ----- | Running_r: -17 | Epi_r: -4.18 | Exploration: 0.186 | Pos: 6 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 90 | ----- | Running_r: -17 | Epi_r: -18.71 | Exploration: 0.180 | Pos: 5 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 91 | ----- | Running_r: -17 | Epi_r: -9.24 | Exploration: 0.179 | Pos: 5 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 92 | ----- | Running_r: -17 | Epi_r: -16.55 | Exploration: 0.174 | Pos: 5 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 93 | ----- | Running_r: -17 | Epi_r: -18.37 | Exploration: 0.169 | Pos: 4 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 94 | ----- | Running_r: -16 | Epi_r: -12.78 | Exploration: 0.167 | Pos: 5 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 95 | ----- | Running_r: -16 | Epi_r: -5.40 | Exploration: 0.166 | Pos: 5 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 96 | ----- | Running_r: -15 | Epi_r: 0.74 | Exploration: 0.164 | Pos: 7 | LR_A: 0.000380 | LR_C: 0.000380\n",
      "Episode: 97 | ----- | Running_r: -16 | Epi_r: -25.60 | Exploration: 0.156 | Pos: 6 | LR_A: 0.000380 | LR_C: 0.000380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-77af612b0612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# update priority\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_td\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mGLOBAL_STEP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mSAVE_MODEL_ITER\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DDPG.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-77af612b0612>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, idx, error)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_priority\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_priority\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-77af612b0612>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, tree_idx, p)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtree_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_propagate_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_propagate_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-77af612b0612>\u001b[0m in \u001b[0;36m_propagate_change\u001b[0;34m(self, tree_idx, change)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparent_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent_idx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_propagate_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_leaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-77af612b0612>\u001b[0m in \u001b[0;36m_propagate_change\u001b[0;34m(self, tree_idx, change)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparent_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent_idx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_propagate_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_leaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-77af612b0612>\u001b[0m in \u001b[0;36m_propagate_change\u001b[0;34m(self, tree_idx, change)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;34m\"\"\"change the sum of priority value in all parent nodes\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mparent_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtree_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparent_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent_idx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_propagate_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "MAX_EPISODES = 2000\n",
    "LR_A = 0.0005  # learning rate for actor\n",
    "LR_C = 0.0005  # learning rate for critic\n",
    "GAMMA = 0.999  # reward discount\n",
    "REPLACE_ITER_A = 1700\n",
    "REPLACE_ITER_C = 1500\n",
    "MEMORY_CAPACITY = 200000\n",
    "BATCH_SIZE = 32\n",
    "DISPLAY_THRESHOLD = 100  # display until the running reward > 100\n",
    "DATA_PATH = './data'\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL_ITER = 100000\n",
    "RENDER = False\n",
    "OUTPUT_GRAPH = False\n",
    "ENV_NAME = 'BipedalWalker-v2'\n",
    "\n",
    "GLOBAL_STEP = tf.Variable(0, trainable=False)\n",
    "INCREASE_GS = GLOBAL_STEP.assign(tf.add(GLOBAL_STEP, 1))\n",
    "LR_A = tf.train.exponential_decay(LR_A, GLOBAL_STEP, 10000, .97, staircase=True)\n",
    "LR_C = tf.train.exponential_decay(LR_C, GLOBAL_STEP, 10000, .97, staircase=True)\n",
    "END_POINT = (200 - 10) * (14/30)    # from game\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env.seed(1)\n",
    "\n",
    "STATE_DIM = env.observation_space.shape[0]  # 24\n",
    "ACTION_DIM = env.action_space.shape[0]  # 4\n",
    "ACTION_BOUND = env.action_space.high    # [1, 1, 1, 1]\n",
    "\n",
    "# all placeholder for tf\n",
    "with tf.name_scope('S'):\n",
    "    S = tf.placeholder(tf.float32, shape=[None, STATE_DIM], name='s')\n",
    "with tf.name_scope('R'):\n",
    "    R = tf.placeholder(tf.float32, [None, 1], name='r')\n",
    "with tf.name_scope('S_'):\n",
    "    S_ = tf.placeholder(tf.float32, shape=[None, STATE_DIM], name='s_')\n",
    "\n",
    "###############################  Actor  ####################################\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, action_dim, action_bound, learning_rate, t_replace_iter):\n",
    "        self.sess = sess\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.lr = learning_rate\n",
    "        self.t_replace_iter = t_replace_iter\n",
    "        self.t_replace_counter = 0\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            # input s, output a\n",
    "            self.a = self._build_net(S, scope='eval_net', trainable=True)\n",
    "\n",
    "            # input s_, output a, get a_ for critic\n",
    "            self.a_ = self._build_net(S_, scope='target_net', trainable=False)\n",
    "\n",
    "        self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval_net')\n",
    "        self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target_net')\n",
    "\n",
    "    def _build_net(self, s, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.01)\n",
    "            init_b = tf.constant_initializer(0.01)\n",
    "            net = tf.layers.dense(s, 500, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l1', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 200, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l2', trainable=trainable)\n",
    "\n",
    "            with tf.variable_scope('a'):\n",
    "                actions = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, kernel_initializer=init_w,\n",
    "                                          bias_initializer=init_b, name='a', trainable=trainable)\n",
    "                scaled_a = tf.multiply(actions, self.action_bound, name='scaled_a')  # Scale output to -action_bound to action_bound\n",
    "        return scaled_a\n",
    "\n",
    "    def learn(self, s):  # batch update\n",
    "        self.sess.run(self.train_op, feed_dict={S: s})\n",
    "        if self.t_replace_counter % self.t_replace_iter == 0:\n",
    "            self.sess.run([tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)])\n",
    "        self.t_replace_counter += 1\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]    # single state\n",
    "        return self.sess.run(self.a, feed_dict={S: s})[0]  # single action\n",
    "\n",
    "    def add_grad_to_graph(self, a_grads):\n",
    "        with tf.variable_scope('policy_grads'):\n",
    "            # ys = policy;\n",
    "            # xs = policy's parameters;\n",
    "            # self.a_grads = the gradients of the policy to get more Q\n",
    "            # tf.gradients will calculate dys/dxs with a initial gradients for ys, so this is dq/da * da/dparams\n",
    "            self.policy_grads_and_vars = tf.gradients(ys=self.a, xs=self.e_params, grad_ys=a_grads)\n",
    "\n",
    "        with tf.variable_scope('A_train'):\n",
    "            opt = tf.train.RMSPropOptimizer(-self.lr)  # (- learning rate) for ascent policy\n",
    "            self.train_op = opt.apply_gradients(zip(self.policy_grads_and_vars, self.e_params), global_step=GLOBAL_STEP)\n",
    "\n",
    "\n",
    "###############################  Critic  ####################################\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, gamma, t_replace_iter, a, a_):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.t_replace_iter = t_replace_iter\n",
    "        self.t_replace_counter = 0\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # Input (s, a), output q\n",
    "            self.a = a\n",
    "            self.q = self._build_net(S, self.a, 'eval_net', trainable=True)\n",
    "\n",
    "            # Input (s_, a_), output q_ for q_target\n",
    "            self.q_ = self._build_net(S_, a_, 'target_net', trainable=False)    # target_q is based on a_ from Actor's target_net\n",
    "\n",
    "            self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval_net')\n",
    "            self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target_net')\n",
    "\n",
    "        with tf.variable_scope('target_q'):\n",
    "            self.target_q = R + self.gamma * self.q_\n",
    "\n",
    "        with tf.variable_scope('abs_TD'):\n",
    "            self.abs_td = tf.abs(self.target_q - self.q)\n",
    "        self.ISWeights = tf.placeholder(tf.float32, [None, 1], name='IS_weights')\n",
    "        with tf.variable_scope('TD_error'):\n",
    "            self.loss = tf.reduce_mean(self.ISWeights * tf.squared_difference(self.target_q, self.q))\n",
    "\n",
    "        with tf.variable_scope('C_train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=GLOBAL_STEP)\n",
    "\n",
    "        with tf.variable_scope('a_grad'):\n",
    "            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)\n",
    "\n",
    "    def _build_net(self, s, a, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.01)\n",
    "            init_b = tf.constant_initializer(0.01)\n",
    "\n",
    "            with tf.variable_scope('l1'):\n",
    "                n_l1 = 700\n",
    "                # combine the action and states together in this way\n",
    "                w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=init_b, trainable=trainable)\n",
    "                net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "            with tf.variable_scope('l2'):\n",
    "                net = tf.layers.dense(net, 20, activation=tf.nn.relu, kernel_initializer=init_w,\n",
    "                                      bias_initializer=init_b, name='l2', trainable=trainable)\n",
    "            with tf.variable_scope('q'):\n",
    "                q = tf.layers.dense(net, 1, kernel_initializer=init_w, bias_initializer=init_b, trainable=trainable)   # Q(s,a)\n",
    "        return q\n",
    "\n",
    "    def learn(self, s, a, r, s_, ISW):\n",
    "        _, abs_td = self.sess.run([self.train_op, self.abs_td], feed_dict={S: s, self.a: a, R: r, S_: s_, self.ISWeights: ISW})\n",
    "        if self.t_replace_counter % self.t_replace_iter == 0:\n",
    "            self.sess.run([tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)])\n",
    "        self.t_replace_counter += 1\n",
    "        return abs_td\n",
    "\n",
    "\n",
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "    Story the data with it priority in tree and data frameworks.\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.tree = np.zeros(2 * capacity - 1)+1e-5\n",
    "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
    "        #             size: capacity - 1                       size: capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
    "        # [--------------data frame-------------]\n",
    "        #             size: capacity\n",
    "\n",
    "    def add_new_priority(self, p, data):\n",
    "        leaf_idx = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(leaf_idx, p)  # update tree_frame\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "\n",
    "        self.tree[tree_idx] = p\n",
    "        self._propagate_change(tree_idx, change)\n",
    "\n",
    "    def _propagate_change(self, tree_idx, change):\n",
    "        \"\"\"change the sum of priority value in all parent nodes\"\"\"\n",
    "        parent_idx = (tree_idx - 1) // 2\n",
    "        self.tree[parent_idx] += change\n",
    "        if parent_idx != 0:\n",
    "            self._propagate_change(parent_idx, change)\n",
    "\n",
    "    def get_leaf(self, lower_bound):\n",
    "        leaf_idx = self._retrieve(lower_bound)  # search the max leaf priority based on the lower_bound\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return [leaf_idx, self.tree[leaf_idx], self.data[data_idx]]\n",
    "\n",
    "    def _retrieve(self, lower_bound, parent_idx=0):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        left_child_idx = 2 * parent_idx + 1\n",
    "        right_child_idx = left_child_idx + 1\n",
    "\n",
    "        if left_child_idx >= len(self.tree):  # end search when no more child\n",
    "            return parent_idx\n",
    "\n",
    "        if self.tree[left_child_idx] == self.tree[right_child_idx]:\n",
    "            return self._retrieve(lower_bound, np.random.choice([left_child_idx, right_child_idx]))\n",
    "        if lower_bound <= self.tree[left_child_idx]:  # downward search, always search for a higher priority node\n",
    "            return self._retrieve(lower_bound, left_child_idx)\n",
    "        else:\n",
    "            return self._retrieve(lower_bound - self.tree[left_child_idx], right_child_idx)\n",
    "\n",
    "    @property\n",
    "    def root_priority(self):\n",
    "        return self.tree[0]  # the root\n",
    "\n",
    "\n",
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    epsilon = 0.001  # small amount to avoid zero priority\n",
    "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 1e-5  # annealing the bias\n",
    "    abs_err_upper = 1   # for stability refer to paper\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def store(self, error, transition):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add_new_priority(p, transition)\n",
    "\n",
    "    def prio_sample(self, n):\n",
    "        batch_idx, batch_memory, ISWeights = [], [], []\n",
    "        segment = self.tree.root_priority / n\n",
    "        self.beta = np.min([1, self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "\n",
    "        min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.root_priority\n",
    "        maxiwi = np.power(self.tree.capacity * min_prob, -self.beta)  # for later normalizing ISWeights\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            lower_bound = np.random.uniform(a, b)\n",
    "            while True:\n",
    "                idx, p, data = self.tree.get_leaf(lower_bound)\n",
    "                if type(data) is int:\n",
    "                    i -= 1\n",
    "                    lower_bound = np.random.uniform(segment * i, segment * (i+1))\n",
    "                else:\n",
    "                    break\n",
    "            prob = p / self.tree.root_priority\n",
    "            ISWeights.append(self.tree.capacity * prob)\n",
    "            batch_idx.append(idx)\n",
    "            batch_memory.append(data)\n",
    "\n",
    "        ISWeights = np.vstack(ISWeights)\n",
    "        ISWeights = np.power(ISWeights, -self.beta) / maxiwi  # normalize\n",
    "        return batch_idx, np.vstack(batch_memory), ISWeights\n",
    "\n",
    "    def random_sample(self, n):\n",
    "        idx = np.random.randint(0, self.tree.capacity, size=n, dtype=np.int)\n",
    "        return np.vstack(self.tree.data[idx])\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        error += self.epsilon   # avoid 0\n",
    "        clipped_error = np.clip(error, 0, self.abs_err_upper)\n",
    "        return np.power(clipped_error, self.alpha)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Create actor and critic.\n",
    "actor = Actor(sess, ACTION_DIM, ACTION_BOUND, LR_A, REPLACE_ITER_A)\n",
    "critic = Critic(sess, STATE_DIM, ACTION_DIM, LR_C, GAMMA, REPLACE_ITER_C, actor.a, actor.a_)\n",
    "actor.add_grad_to_graph(critic.a_grads)\n",
    "\n",
    "M = Memory(MEMORY_CAPACITY)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    all_ckpt = tf.train.get_checkpoint_state('./data', 'checkpoint').all_model_checkpoint_paths\n",
    "    saver.restore(sess, all_ckpt[-1])\n",
    "else:\n",
    "    if os.path.isdir(DATA_PATH): shutil.rmtree(DATA_PATH)\n",
    "    os.mkdir(DATA_PATH)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter('logs', graph=sess.graph)\n",
    "\n",
    "var = 3  # control exploration\n",
    "var_min = 0.01\n",
    "\n",
    "for i_episode in range(MAX_EPISODES):\n",
    "    # s = (hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements.)\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        a = actor.choose_action(s)\n",
    "        a = np.clip(np.random.normal(a, var), -1, 1)    # add randomness to action selection for exploration\n",
    "        s_, r, done, _ = env.step(a)    # r = total 300+ points up to the far end. If the robot falls, it gets -100.\n",
    "\n",
    "        if r == -100: r = -2\n",
    "        ep_r += r\n",
    "\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        max_p = np.max(M.tree.tree[-M.tree.capacity:])\n",
    "        M.store(max_p, transition)\n",
    "\n",
    "        if GLOBAL_STEP.eval(sess) > MEMORY_CAPACITY/20:\n",
    "            var = max([var*0.9999, var_min])  # decay the action randomness\n",
    "            tree_idx, b_M, ISWeights = M.prio_sample(BATCH_SIZE)    # for critic update\n",
    "            b_s = b_M[:, :STATE_DIM]\n",
    "            b_a = b_M[:, STATE_DIM: STATE_DIM + ACTION_DIM]\n",
    "            b_r = b_M[:, -STATE_DIM - 1: -STATE_DIM]\n",
    "            b_s_ = b_M[:, -STATE_DIM:]\n",
    "\n",
    "            abs_td = critic.learn(b_s, b_a, b_r, b_s_, ISWeights)\n",
    "            actor.learn(b_s)\n",
    "            for i in range(len(tree_idx)):  # update priority\n",
    "                idx = tree_idx[i]\n",
    "                M.update(idx, abs_td[i])\n",
    "        if GLOBAL_STEP.eval(sess) % SAVE_MODEL_ITER == 0:\n",
    "            ckpt_path = os.path.join(DATA_PATH, 'DDPG.ckpt')\n",
    "            save_path = saver.save(sess, ckpt_path, global_step=GLOBAL_STEP, write_meta_graph=False)\n",
    "            print(\"\\nSave Model %s\\n\" % save_path)\n",
    "\n",
    "        if done:\n",
    "            if \"running_r\" not in globals():\n",
    "                running_r = ep_r\n",
    "            else:\n",
    "                running_r = 0.95*running_r + 0.05*ep_r\n",
    "            if running_r > DISPLAY_THRESHOLD: RENDER = True\n",
    "            else: RENDER = False\n",
    "\n",
    "            done = '| Achieve ' if env.unwrapped.hull.position[0] >= END_POINT else '| -----'\n",
    "            print('Episode:', i_episode,\n",
    "                  done,\n",
    "                  '| Running_r: %i' % int(running_r),\n",
    "                  '| Epi_r: %.2f' % ep_r,\n",
    "                  '| Exploration: %.3f' % var,\n",
    "                  '| Pos: %.i' % int(env.unwrapped.hull.position[0]),\n",
    "                  '| LR_A: %.6f' % sess.run(LR_A),\n",
    "                  '| LR_C: %.6f' % sess.run(LR_C),\n",
    "                  )\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "        sess.run(INCREASE_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
