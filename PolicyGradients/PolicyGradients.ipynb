{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 知识参考\n",
    "\n",
    "[1] [强化学习之三：双臂赌博机（Two-armed Bandit）](https://blog.csdn.net/qq_32690999/article/details/78996390)\n",
    "\n",
    "[2] [强化学习之四：基于策略的Agents (Policy-based Agents)](https://blog.csdn.net/qq_32690999/article/details/78996416)\n",
    "\n",
    "[3] [Policy Gradients 思维决策 (Tensorflow)](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/5-2-policy-gradient-softmax2/)\n",
    "\n",
    "# Policy Based Agent (Policy Gradients Agent)\n",
    "\n",
    "## 策略梯度（Policy Gradient）\n",
    "\n",
    "- 必读的英文文章\n",
    "\n",
    "http://karpathy.github.io/2016/05/31/rl/\n",
    "\n",
    "[RL — Policy Gradient Explained](https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146)\n",
    "\n",
    "[An introduction to Policy Gradients with Cartpole and Doom](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)\n",
    "\n",
    "Policy Gradients的nn的损失函数定义如下：\n",
    "\n",
    "$$Loss=−log(π)∗A$$\n",
    "\n",
    "A是优越度，也是所有强化学习算法的一个重要部分。直觉上，它描述了一个行动比某个基准线好多少。在未来的算法中，我们将遇到更复杂的用于比较回报的基准线，而现在我们就假设基准线为0，于是此处我们可以简单地把它想成我们采取每个行动对应的回报。\n",
    "\n",
    "π是策略。在这个例子中，它和所选行动的权重相关。\n",
    "\n",
    "我们将基于损失函数来对nn的参数进行更新：\n",
    "\n",
    "  $$\\theta=\\theta+\\bigtriangledown_\\theta log(\\pi(s,a))*A$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from RL_brain import PolicyGradient\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RENDER = False  # 在屏幕上显示模拟窗口会拖慢运行速度, 我们等计算机学得差不多了再显示模拟\n",
    "DISPLAY_REWARD_THRESHOLD = 400  # 当 回合总 reward 大于 400 时显示模拟窗口\n",
    "\n",
    "env = gym.make('CartPole-v0')   # CartPole 这个模拟\n",
    "env = env.unwrapped     # 取消限制\n",
    "env.seed(1)     # 普通的 Policy gradient 方法, 使得回合的 variance 比较大, 所以我们选了一个好点的随机种子\n",
    "\n",
    "print(env.action_space)     # 显示可用 action\n",
    "print(env.observation_space)    # 显示可用 state 的 observation\n",
    "print(env.observation_space.high)   # 显示 observation 最高值\n",
    "print(env.observation_space.low)    # 显示 observation 最低值\n",
    "\n",
    "# 定义\n",
    "RL = PolicyGradient(\n",
    "    n_actions=env.action_space.n,\n",
    "    n_features=env.observation_space.shape[0],\n",
    "    learning_rate=0.02,\n",
    "    reward_decay=0.99,   # gamma\n",
    "    # output_graph=True,    # 输出 tensorboard 文件\n",
    ")\n",
    "\n",
    "\n",
    "for i_episode in range(3000):\n",
    "\n",
    "    observation = env.reset()\n",
    "\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        action = RL.choose_action(observation)\n",
    "\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "\n",
    "        RL.store_transition(observation, action, reward)    # 存储这一回合的 transition\n",
    "\n",
    "        if done:\n",
    "            ep_rs_sum = sum(RL.ep_rs)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True     # 判断是否显示模拟\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "\n",
    "            vt = RL.learn() # 学习, 输出 vt, 我们下节课讲这个 vt 的作用\n",
    "\n",
    "            if i_episode == 0:\n",
    "                plt.plot(vt)    # plot 这个回合的 vt\n",
    "                plt.xlabel('episode steps')\n",
    "                plt.ylabel('normalized state-action value')\n",
    "                plt.show()\n",
    "            break\n",
    "\n",
    "        observation = observation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Box(4,)\n",
      "Box(24,)\n",
      "WARNING:tensorflow:From <ipython-input-1-da8d113ecb41>:88: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "episode: 0   reward: -121\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12182db38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1   reward: -121\n",
      "None\n",
      "episode: 2   reward: -120\n",
      "None\n",
      "episode: 3   reward: -120\n",
      "None\n",
      "episode: 4   reward: -120\n",
      "None\n",
      "episode: 5   reward: -120\n",
      "None\n",
      "episode: 6   reward: -120\n",
      "None\n",
      "episode: 7   reward: -120\n",
      "None\n",
      "episode: 8   reward: -120\n",
      "None\n",
      "episode: 9   reward: -121\n",
      "None\n",
      "episode: 10   reward: -121\n",
      "None\n",
      "episode: 11   reward: -121\n",
      "None\n",
      "episode: 12   reward: -121\n",
      "None\n",
      "episode: 13   reward: -121\n",
      "None\n",
      "episode: 14   reward: -121\n",
      "None\n",
      "episode: 15   reward: -121\n",
      "None\n",
      "episode: 16   reward: -121\n",
      "None\n",
      "episode: 17   reward: -121\n",
      "None\n",
      "episode: 18   reward: -121\n",
      "None\n",
      "episode: 19   reward: -122\n",
      "None\n",
      "episode: 20   reward: -122\n",
      "None\n",
      "episode: 21   reward: -122\n",
      "None\n",
      "episode: 22   reward: -122\n",
      "None\n",
      "episode: 23   reward: -122\n",
      "None\n",
      "episode: 24   reward: -122\n",
      "None\n",
      "episode: 25   reward: -122\n",
      "None\n",
      "episode: 26   reward: -122\n",
      "None\n",
      "episode: 27   reward: -122\n",
      "None\n",
      "episode: 28   reward: -122\n",
      "None\n",
      "episode: 29   reward: -122\n",
      "None\n",
      "episode: 30   reward: -122\n",
      "None\n",
      "episode: 31   reward: -123\n",
      "None\n",
      "episode: 32   reward: -123\n",
      "None\n",
      "episode: 33   reward: -123\n",
      "None\n",
      "episode: 34   reward: -123\n",
      "None\n",
      "episode: 35   reward: -123\n",
      "None\n",
      "episode: 36   reward: -123\n",
      "None\n",
      "episode: 37   reward: -123\n",
      "None\n",
      "episode: 38   reward: -123\n",
      "None\n",
      "episode: 39   reward: -123\n",
      "None\n",
      "episode: 40   reward: -123\n",
      "None\n",
      "episode: 41   reward: -123\n",
      "None\n",
      "episode: 42   reward: -123\n",
      "None\n",
      "episode: 43   reward: -123\n",
      "None\n",
      "episode: 44   reward: -124\n",
      "None\n",
      "episode: 45   reward: -124\n",
      "None\n",
      "episode: 46   reward: -124\n",
      "None\n",
      "episode: 47   reward: -124\n",
      "None\n",
      "episode: 48   reward: -124\n",
      "None\n",
      "episode: 49   reward: -124\n",
      "None\n",
      "episode: 50   reward: -124\n",
      "None\n",
      "episode: 51   reward: -124\n",
      "None\n",
      "episode: 52   reward: -124\n",
      "None\n",
      "episode: 53   reward: -124\n",
      "None\n",
      "episode: 54   reward: -124\n",
      "None\n",
      "episode: 55   reward: -124\n",
      "None\n",
      "episode: 56   reward: -124\n",
      "None\n",
      "episode: 57   reward: -124\n",
      "None\n",
      "episode: 58   reward: -124\n",
      "None\n",
      "episode: 59   reward: -124\n",
      "None\n",
      "episode: 60   reward: -124\n",
      "None\n",
      "episode: 61   reward: -124\n",
      "None\n",
      "episode: 62   reward: -124\n",
      "None\n",
      "episode: 63   reward: -125\n",
      "None\n",
      "episode: 64   reward: -125\n",
      "None\n",
      "episode: 65   reward: -125\n",
      "None\n",
      "episode: 66   reward: -125\n",
      "None\n",
      "episode: 67   reward: -125\n",
      "None\n",
      "episode: 68   reward: -125\n",
      "None\n",
      "episode: 69   reward: -125\n",
      "None\n",
      "episode: 70   reward: -125\n",
      "None\n",
      "episode: 71   reward: -125\n",
      "None\n",
      "episode: 72   reward: -125\n",
      "None\n",
      "episode: 73   reward: -125\n",
      "None\n",
      "episode: 74   reward: -125\n",
      "None\n",
      "episode: 75   reward: -125\n",
      "None\n",
      "episode: 76   reward: -125\n",
      "None\n",
      "episode: 77   reward: -126\n",
      "None\n",
      "episode: 78   reward: -126\n",
      "None\n",
      "episode: 79   reward: -126\n",
      "None\n",
      "episode: 80   reward: -125\n",
      "None\n",
      "episode: 81   reward: -125\n",
      "None\n",
      "episode: 82   reward: -125\n",
      "None\n",
      "episode: 83   reward: -125\n",
      "None\n",
      "episode: 84   reward: -125\n",
      "None\n",
      "episode: 85   reward: -125\n",
      "None\n",
      "episode: 86   reward: -125\n",
      "None\n",
      "episode: 87   reward: -125\n",
      "None\n",
      "episode: 88   reward: -125\n",
      "None\n",
      "episode: 89   reward: -125\n",
      "None\n",
      "episode: 90   reward: -125\n",
      "None\n",
      "episode: 91   reward: -125\n",
      "None\n",
      "episode: 92   reward: -125\n",
      "None\n",
      "episode: 93   reward: -125\n",
      "None\n",
      "episode: 94   reward: -125\n",
      "None\n",
      "episode: 95   reward: -125\n",
      "None\n",
      "episode: 96   reward: -126\n",
      "None\n",
      "episode: 97   reward: -126\n",
      "None\n",
      "episode: 98   reward: -126\n",
      "None\n",
      "episode: 99   reward: -126\n",
      "None\n",
      "episode: 100   reward: -126\n",
      "None\n",
      "episode: 101   reward: -126\n",
      "None\n",
      "episode: 102   reward: -126\n",
      "None\n",
      "episode: 103   reward: -126\n",
      "None\n",
      "episode: 104   reward: -126\n",
      "None\n",
      "episode: 105   reward: -126\n",
      "None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-da8d113ecb41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRENDER\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-da8d113ecb41>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m#         prob_weights = self.sess.run(self.all_act_prob, feed_dict={self.tf_obs: observation[np.newaxis, :]})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m#         action = np.random.choice(range(prob_weights.shape[1]), p=prob_weights.ravel())  # select action w.r.t the actions prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_obs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# shape: 1*4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# shape: 1*4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Policy Gradient, Reinforcement Learning.\n",
    "The cart pole example\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "Tensorflow: 1.0\n",
    "gym: 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This part of code is the reinforcement learning brain, which is a brain of the agent.\n",
    "All decisions are made in here.\n",
    "Policy Gradient, Reinforcement Learning.\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "Tensorflow: 1.0\n",
    "gym: 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "# from gym import spaces\n",
    "\n",
    "# reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "tf.reset_default_graph() \n",
    "\n",
    "class PolicyGradient:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.95,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []\n",
    "\n",
    "        self._build_net()\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            # $ tensorboard --logdir=logs\n",
    "            # http://0.0.0.0:6006/\n",
    "            # tf.train.SummaryWriter soon be deprecated, use following\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.tf_obs = tf.placeholder(tf.float32, [None, self.n_features], name=\"observations\")\n",
    "            self.tf_acts = tf.placeholder(tf.int32, [None, self.n_actions], name=\"actions_num\")\n",
    "            self.tf_vt = tf.placeholder(tf.float32, [None, ], name=\"actions_value\")\n",
    "        # fc1\n",
    "        with tf.name_scope('nn'):\n",
    "            layer = tf.layers.dense(\n",
    "                inputs=self.tf_obs,\n",
    "                units=20,\n",
    "                activation=tf.nn.tanh,  # tanh activation\n",
    "                kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "                bias_initializer=tf.constant_initializer(0.1),\n",
    "                name='fc1',\n",
    "            )\n",
    "            # fc2\n",
    "            all_act = tf.layers.dense(\n",
    "                inputs=layer,\n",
    "                units=self.n_actions,\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "                bias_initializer=tf.constant_initializer(0.1),\n",
    "                name='fc2',\n",
    "            )\n",
    "\n",
    "#         self.all_act_prob = tf.nn.softmax(all_act, name='act_prob')  # use softmax to convert to probability\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            # to maximize total reward (log_p * R) is to minimize -(log_p * R), and the tf only have minimize(loss)\n",
    "#             neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_act, labels=self.tf_acts)   # this is negative log of chosen action\n",
    "            neg_log_prob = tf.nn.softmax_cross_entropy_with_logits(logits=all_act, labels=self.tf_acts)   \n",
    "    \n",
    "            # or in this way:\n",
    "            # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1)\n",
    "            loss = tf.reduce_mean(neg_log_prob * self.tf_vt)  # reward guided loss\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "            self.mu=all_act\n",
    "            self.sigma=tf.constant(1.0, shape=[1,self.n_actions])\n",
    "\n",
    "    def choose_action(self,observation):\n",
    "#         prob_weights = self.sess.run(self.all_act_prob, feed_dict={self.tf_obs: observation[np.newaxis, :]})\n",
    "#         action = np.random.choice(range(prob_weights.shape[1]), p=prob_weights.ravel())  # select action w.r.t the actions prob\n",
    "        mu=self.sess.run(self.mu,feed_dict={self.tf_obs: observation[np.newaxis, :]})[0] # shape: 1*4\n",
    "        sigma=self.sess.run(self.sigma)[0] # shape: 1*4\n",
    "        action=np.random.normal(mu, sigma)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r):\n",
    "        self.ep_obs.append(s)\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "\n",
    "    def learn(self):\n",
    "        # discount and normalize episode reward\n",
    "        discounted_ep_rs_norm = self._discount_and_norm_rewards()\n",
    "\n",
    "        # train on episode\n",
    "        self.sess.run(self.train_op, feed_dict={\n",
    "             self.tf_obs: np.vstack(self.ep_obs),  # shape=[None, n_obs]\n",
    "             self.tf_acts: np.array(self.ep_as),  # shape=[None, ]\n",
    "             self.tf_vt: discounted_ep_rs_norm,  # shape=[None, ]\n",
    "        })\n",
    "        \n",
    "        print(\"learn:\")\n",
    "        print('self.tf_acts:')_\n",
    "        print(np.array(self.ep_as))\n",
    "        print('self.tf_obs:')\n",
    "        print(np.vstack(self.ep_obs))\n",
    "        \n",
    "\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []    # empty episode data\n",
    "        return discounted_ep_rs_norm\n",
    "\n",
    "    def _discount_and_norm_rewards(self):\n",
    "        # discount episode rewards\n",
    "        discounted_ep_rs = np.zeros_like(self.ep_rs)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(self.ep_rs))):\n",
    "            running_add = running_add * self.gamma + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "\n",
    "        # normalize episode rewards\n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)\n",
    "        discounted_ep_rs /= np.std(discounted_ep_rs)\n",
    "        return discounted_ep_rs\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "MAX_EPISODES=3000\n",
    "DISPLAY_REWARD_THRESHOLD = -110  # renders environment if total episode reward is greater then this threshold\n",
    "RENDER = False  # rendering wastes time\n",
    "\n",
    "# env_name='CartPole-v0'\n",
    "env_name='BipedalWalker-v2'\n",
    "env = gym.make(env_name)\n",
    "env.seed(1)     # reproducible, general Policy gradient has high variance\n",
    "env = env.unwrapped\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "RL = PolicyGradient(\n",
    "    n_actions=env.action_space.shape[0],\n",
    "    n_features=env.observation_space.shape[0],\n",
    "    learning_rate=0.02,\n",
    "    reward_decay=0.99,\n",
    "    # output_graph=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for i_episode in range(MAX_EPISODES):\n",
    "\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        action = RL.choose_action(observation)\n",
    "        \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        \n",
    "        RL.store_transition(observation, action, reward)\n",
    "\n",
    "\n",
    "        if done:\n",
    "            ep_rs_sum = sum(RL.ep_rs)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True     # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "\n",
    "            vt = RL.learn()\n",
    "\n",
    "            if i_episode == 0:\n",
    "                plt.plot(vt)    # plot the episode vt\n",
    "                plt.xlabel('episode steps')\n",
    "                plt.ylabel('normalized state-action value')\n",
    "                plt.show()\n",
    "            break\n",
    "\n",
    "        observation = observation_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
