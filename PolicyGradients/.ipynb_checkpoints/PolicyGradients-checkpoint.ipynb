{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 知识参考\n",
    "\n",
    "[1] [强化学习之三：双臂赌博机（Two-armed Bandit）](https://blog.csdn.net/qq_32690999/article/details/78996390)\n",
    "\n",
    "[2] [强化学习之四：基于策略的Agents (Policy-based Agents)](https://blog.csdn.net/qq_32690999/article/details/78996416)\n",
    "\n",
    "[3] [Policy Gradients 思维决策 (Tensorflow)](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/5-2-policy-gradient-softmax2/)\n",
    "\n",
    "# Policy Based Agent (Policy Gradients Agent)\n",
    "\n",
    "## 策略梯度（Policy Gradient）\n",
    "\n",
    "之前的Q-Learning或者DQN在基于NN的实现中，都是输出对行动的估值，然后我们再根据估值来选行动，并且它们能估计的只能是离散的行动，比如说移动到哪个方格，前进后退，上下左右等等。但如果是需要控制一些连续变量，比如速度、角度时，之前的方法可能就有点无能为力了。\n",
    "\n",
    "而Policy Gradients就可以解决这个问题，**它对输出的设置就是应该采取的行动**，即我们现在可以直接让nn告诉我们：当前状态下，应该采取什么行动。\n",
    "\n",
    "为了训练我们的nn，我们将简单地基于**e-贪婪策略（e-greedy policy）**。这意味着大多数时间里，我们的agent将会选择有着预期最大回报值的\n",
    "行动，但偶尔，它也会随机行动。通过这种方式，agent可能尝试到每一个不同的行动并持续地学习到更多知识。\n",
    "\n",
    "Policy Gradients的nn的损失函数定义如下：\n",
    "\n",
    "$$Loss=−log(π)∗A$$\n",
    "\n",
    "A是优越度，也是所有强化学习算法的一个重要部分。直觉上，它描述了一个行动比某个基准线好多少。在未来的算法中，我们将遇到更复杂的用于比较回报的基准线，而现在我们就假设基准线为0，于是此处我们可以简单地把它想成我们采取每个行动对应的回报。\n",
    "\n",
    "π是策略。在这个例子中，它和所选行动的权重相关。\n",
    "\n",
    "我们将基于损失函数来对nn的参数进行更新：\n",
    "\n",
    "  $$\\theta=\\theta+\\bigtriangledown_\\theta log(\\pi(s,a))*A$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# 折现率\n",
    "gamma = 0.99\n",
    "\n",
    "num_episodes=100\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    s=env.reset()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n16.0\\n21.47\\n25.57\\n38.03\\n43.59\\n53.05\\n67.38\\n90.44\\n120.19\\n131.75\\n162.65\\n156.48\\n168.18\\n181.43\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '''\n",
    "# Simple Reinforcement Learning in Tensorflow Part 2-b:\n",
    "# Vanilla Policy Gradient Agent\n",
    "# This tutorial contains a simple example of how to build a policy-gradient based agent that can solve the CartPole problem. For more information, see this Medium post. This implementation is generalizable to more than two actions.\n",
    "\n",
    "# For more Reinforcement Learning algorithms, including DQN and Model-based learning in Tensorflow, see my Github repo, DeepRL-Agents.\n",
    "# '''\n",
    "\n",
    "# # 此实例中，我们的agent已经模块化了\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.contrib.slim as slim\n",
    "# import numpy as np\n",
    "# import gym\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# # 尽量使用xrange；xrange循环的性能一般比range好一些，因为xrange返回的是生成器，而不是像range直接生成一个序列\n",
    "# try:\n",
    "#     xrange = xrange\n",
    "# except:\n",
    "#     xrange = range\n",
    "\n",
    "    \n",
    "\n",
    "# env = gym.make('CartPole-v0')\n",
    "\n",
    "# # 折现率\n",
    "# gamma = 0.99\n",
    "\n",
    "# # 折现回报\n",
    "# def discount_rewards(r):\n",
    "#     # 给定一维的reward数组，返回折现后的reward数组\n",
    "#     discounted_r = np.zeros_like(r)\n",
    "#     running_add = 0\n",
    "#     for t in reversed(xrange(0, r.size)):\n",
    "#         running_add = running_add * gamma + r[t]\n",
    "#         discounted_r[t] = running_add\n",
    "#     return discounted_r\n",
    "\n",
    "\n",
    "\n",
    "# class agent():\n",
    "#     def __init__(self, lr, s_size,a_size,h_size):\n",
    "#         # 下面几行代码建立了网络的feed-foward部分，即agent接收当前状态输入，产生一个行动模块\n",
    "#         self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "#         hidden = slim.fully_connected(self.state_in,h_size,biases_initializer=None,activation_fn=tf.nn.relu) # 隐藏层为全连接层\n",
    "#         self.output = slim.fully_connected(hidden,a_size,activation_fn=tf.nn.softmax,biases_initializer=None) # 输出层为全连接层\n",
    "#         self.chosen_action = tf.argmax(self.output,1) # 选择Q值最大的对应行动\n",
    "\n",
    "#         # 下面六行代码描述了训练过程。我们将回报与所选择的行为喂给网络。\n",
    "#         # 计算损失，并用损失来更新网络\n",
    "#         self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "#         self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "#         self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "#         self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes) # \n",
    "\n",
    "#         self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder) # 损失函数：Loss=-log(π)*reward\n",
    "        \n",
    "#         tvars = tf.trainable_variables()\n",
    "#         self.gradient_holders = []\n",
    "        \n",
    "#         for idx,var in enumerate(tvars):\n",
    "#             placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "#             self.gradient_holders.append(placeholder)\n",
    "        \n",
    "#         self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=lr) # 选择Adam优化器\n",
    "#         self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))\n",
    "        \n",
    "        \n",
    "# #  训练agent\n",
    "\n",
    "# tf.reset_default_graph() # 清除默认计算图元素\n",
    "\n",
    "# myAgent = agent(lr=1e-2,s_size=4,a_size=2,h_size=8) # 实例化一个agent\n",
    "\n",
    "# total_episodes = 100 # 训练周期数\n",
    "# max_ep = 999\n",
    "# update_frequency = 5\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# # 启动计算图\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     i = 0\n",
    "#     total_reward = []\n",
    "#     total_lenght = []\n",
    "        \n",
    "#     gradBuffer = sess.run(tf.trainable_variables())\n",
    "#     for ix,grad in enumerate(gradBuffer):\n",
    "#         gradBuffer[ix] = grad * 0\n",
    "        \n",
    "#     while i < total_episodes:\n",
    "#         s = env.reset()\n",
    "#         running_reward = 0\n",
    "#         ep_history = []\n",
    "#         for j in range(max_ep):\n",
    "#             # 按照网络输出，依照概率选择一个行动\n",
    "#             a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]})\n",
    "#             a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "#             a = np.argmax(a_dist == a)\n",
    "\n",
    "#             s1,r,d,_ = env.step(a) # Get our reward for taking an action given a bandit.\n",
    "#             ep_history.append([s,a,r,s1])\n",
    "#             s = s1\n",
    "#             running_reward += r\n",
    "#             if d == True:\n",
    "#                 # 更新网络\n",
    "#                 ep_history = np.array(ep_history)\n",
    "#                 ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "#                 feed_dict={myAgent.reward_holder:ep_history[:,2],\n",
    "#                         myAgent.action_holder:ep_history[:,1],myAgent.state_in:np.vstack(ep_history[:,0])}\n",
    "#                 grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
    "#                 for idx,grad in enumerate(grads):\n",
    "#                     gradBuffer[idx] += grad\n",
    "\n",
    "#                 if i % update_frequency == 0 and i != 0:\n",
    "#                     feed_dict= dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "#                     _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
    "#                     for ix,grad in enumerate(gradBuffer):\n",
    "#                         gradBuffer[ix] = grad * 0\n",
    "                \n",
    "#                 total_reward.append(running_reward)\n",
    "#                 total_lenght.append(j)\n",
    "#                 break\n",
    "\n",
    "        \n",
    "#             #Update our running tally of scores.\n",
    "#         if i % 100 == 0:\n",
    "#             print(np.mean(total_reward[-100:]))\n",
    "#         i += 1\n",
    "\n",
    "# '''\n",
    "# 16.0\n",
    "# 21.47\n",
    "# 25.57\n",
    "# 38.03\n",
    "# 43.59\n",
    "# 53.05\n",
    "# 67.38\n",
    "# 90.44\n",
    "# 120.19\n",
    "# 131.75\n",
    "# 162.65\n",
    "# 156.48\n",
    "# 168.18\n",
    "# 181.43\n",
    "# '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  3  6  9 12 15 18 21 24 27]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
