{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG - BipedalWalker-v2\n",
    "\n",
    "- Xinyao Qian\n",
    "- Tianhao Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get familiar with the BipedalWalker-v2 environment first\n",
    "\n",
    "Find that BipedalWalker behaves embarrasingly bad if taking random walking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Load Environment\n",
    "ENV_NAME = 'BipedalWalker-v2'\n",
    "env = gym.make(ENV_NAME)\n",
    "# Repeoducible environment parameters\n",
    "env.seed(1)\n",
    "\n",
    "s=env.reset()\n",
    "episode=100\n",
    "steps=5000\n",
    "while i in range(episode):\n",
    "    for j in range(steps):\n",
    "        env.render()\n",
    "        a=env.action_space.sample()\n",
    "        s_,r,d,_=env.step(a)\n",
    "\n",
    "        if d:\n",
    "            s=env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Since the action space of BipedalWalker is consecutive, that means value based models such as Q-Learning or DQN, are not applicable**, because value based models generally try to fit a better value function that tells us how good it is to be at a certain state s (V(s)) or to take action a at the state s (Q(s,a)), and then we still need to choose specific action based on our exploring strategy (e.g. $\\epsilon$-greedy). Obviously, it can't work when our actions are consecutive/countless.\n",
    "\n",
    "So then we consider using **policy based models**, for example, REINFORCE. However, there is another problem that REINFORCE can only update parameters/learn everytime an episode ends, which slowed the convergence process. \n",
    "\n",
    "Then we get to know that there is another series of models that called **Actor Critic which combines the advantages of both the value based model and the policy based model and make it possible for policy based models to update itself at every step**. \n",
    "\n",
    "Specifically, we simultaneously train a policy gradients network and a Q-Learning network. The policy network behaves as the actor which takes in observations and outputs best actions to be taken, while the value network will behave as a critic to take in observations and tell the actor how 'good' to be at the current state, so that the actor can know how good its last action that brought it here was, and update its parameters according to this feedback, while the critic can also update its own parameters in the way Q-Learning does. **In a sense, actor and critic are supervising each other to become better and better**.\n",
    "\n",
    "<center>\n",
    "![](https://morvanzhou.github.io/static/results/ML-intro/AC3.png)\n",
    "</center>\n",
    "\n",
    "\n",
    "> https://morvanzhou.github.io/static/results/ML-intro/AC3.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment preparation & Definition of Classes: Actor, Critic, Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "# Load Environment\n",
    "ENV_NAME = 'BipedalWalker-v2'\n",
    "env = gym.make(ENV_NAME)\n",
    "# Repeoducible environment parameters\n",
    "env.seed(1)\n",
    "\n",
    "\n",
    "STATE_DIM = env.observation_space.shape[0]  # 24 environment variables\n",
    "ACTION_DIM = env.action_space.shape[0]  # 4 consecutive actions\n",
    "ACTION_BOUND = env.action_space.high    # [1, 1, 1, 1]\n",
    "\n",
    "# all placeholder for tf\n",
    "with tf.name_scope('S'):\n",
    "    S = tf.placeholder(tf.float32, shape=[None, STATE_DIM], name='s')\n",
    "with tf.name_scope('R'):\n",
    "    R = tf.placeholder(tf.float32, [None, 1], name='r')\n",
    "with tf.name_scope('S_'):\n",
    "    S_ = tf.placeholder(tf.float32, shape=[None, STATE_DIM], name='s_')\n",
    "\n",
    "###############################  Actor  ####################################\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, action_dim, action_bound, learning_rate, t_replace_iter):\n",
    "        self.sess = sess\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.lr = learning_rate\n",
    "        self.t_replace_iter = t_replace_iter\n",
    "        self.t_replace_counter = 0\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            # input s, output a\n",
    "            self.a = self._build_net(S, scope='eval_net', trainable=True)\n",
    "\n",
    "            # input s_, output a, get a_ for critic\n",
    "            self.a_ = self._build_net(S_, scope='target_net', trainable=False)\n",
    "\n",
    "        self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval_net')\n",
    "        self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target_net')\n",
    "\n",
    "    def _build_net(self, s, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.01)\n",
    "            init_b = tf.constant_initializer(0.01)\n",
    "            net = tf.layers.dense(s, 500, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l1', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 200, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l2', trainable=trainable)\n",
    "\n",
    "            with tf.variable_scope('a'):\n",
    "                actions = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, kernel_initializer=init_w,\n",
    "                                          bias_initializer=init_b, name='a', trainable=trainable)\n",
    "                scaled_a = tf.multiply(actions, self.action_bound, name='scaled_a')  # Scale output to -action_bound to action_bound\n",
    "        return scaled_a\n",
    "\n",
    "    def learn(self, s):  # batch update\n",
    "        self.sess.run(self.train_op, feed_dict={S: s})\n",
    "        if self.t_replace_counter % self.t_replace_iter == 0:\n",
    "            self.sess.run([tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)])\n",
    "        self.t_replace_counter += 1\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]    # single state\n",
    "        return self.sess.run(self.a, feed_dict={S: s})[0]  # single action\n",
    "\n",
    "    def add_grad_to_graph(self, a_grads):\n",
    "        with tf.variable_scope('policy_grads'):\n",
    "            # ys = policy;\n",
    "            # xs = policy's parameters;\n",
    "            # self.a_grads = the gradients of the policy to get more Q\n",
    "            # tf.gradients will calculate dys/dxs with a initial gradients for ys, so this is dq/da * da/dparams\n",
    "            self.policy_grads_and_vars = tf.gradients(ys=self.a, xs=self.e_params, grad_ys=a_grads)\n",
    "\n",
    "        with tf.variable_scope('A_train'):\n",
    "            opt = tf.train.RMSPropOptimizer(-self.lr)  # (- learning rate) for ascent policy\n",
    "            self.train_op = opt.apply_gradients(zip(self.policy_grads_and_vars, self.e_params), global_step=GLOBAL_STEP)\n",
    "\n",
    "\n",
    "########################################  Critic  #########################################\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, gamma, t_replace_iter, a, a_):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.t_replace_iter = t_replace_iter\n",
    "        self.t_replace_counter = 0\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # Input (s, a), output q\n",
    "            self.a = a\n",
    "            self.q = self._build_net(S, self.a, 'eval_net', trainable=True)\n",
    "\n",
    "            # Input (s_, a_), output q_ for q_target\n",
    "            self.q_ = self._build_net(S_, a_, 'target_net', trainable=False)    # target_q is based on a_ from Actor's target_net\n",
    "\n",
    "            self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval_net')\n",
    "            self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target_net')\n",
    "\n",
    "        with tf.variable_scope('target_q'):\n",
    "            self.target_q = R + self.gamma * self.q_\n",
    "\n",
    "        with tf.variable_scope('abs_TD'):\n",
    "            self.abs_td = tf.abs(self.target_q - self.q)\n",
    "        self.ISWeights = tf.placeholder(tf.float32, [None, 1], name='IS_weights')\n",
    "        with tf.variable_scope('TD_error'):\n",
    "            self.loss = tf.reduce_mean(self.ISWeights * tf.squared_difference(self.target_q, self.q))\n",
    "\n",
    "        with tf.variable_scope('C_train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=GLOBAL_STEP)\n",
    "\n",
    "        with tf.variable_scope('a_grad'):\n",
    "            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)\n",
    "\n",
    "    def _build_net(self, s, a, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.01)\n",
    "            init_b = tf.constant_initializer(0.01)\n",
    "\n",
    "            with tf.variable_scope('l1'):\n",
    "                n_l1 = 700\n",
    "                # combine the action and states together in this way\n",
    "                w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=init_b, trainable=trainable)\n",
    "                net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "            with tf.variable_scope('l2'):\n",
    "                net = tf.layers.dense(net, 20, activation=tf.nn.relu, kernel_initializer=init_w,\n",
    "                                      bias_initializer=init_b, name='l2', trainable=trainable)\n",
    "            with tf.variable_scope('q'):\n",
    "                q = tf.layers.dense(net, 1, kernel_initializer=init_w, bias_initializer=init_b, trainable=trainable)   # Q(s,a)\n",
    "        return q\n",
    "\n",
    "    def learn(self, s, a, r, s_, ISW):\n",
    "        _, abs_td = self.sess.run([self.train_op, self.abs_td], feed_dict={S: s, self.a: a, R: r, S_: s_, self.ISWeights: ISW})\n",
    "        if self.t_replace_counter % self.t_replace_iter == 0:\n",
    "            self.sess.run([tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)])\n",
    "        self.t_replace_counter += 1\n",
    "        return abs_td\n",
    "\n",
    "########################################  Assistanting Class: SumTree and Memory  #########################################\n",
    "\n",
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "    Story the data with it priority in tree and data frameworks.\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.tree = np.zeros(2 * capacity - 1)+1e-5\n",
    "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
    "        #             size: capacity - 1                       size: capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
    "        # [--------------data frame-------------]\n",
    "        #             size: capacity\n",
    "\n",
    "    def add_new_priority(self, p, data):\n",
    "        leaf_idx = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(leaf_idx, p)  # update tree_frame\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "\n",
    "        self.tree[tree_idx] = p\n",
    "        self._propagate_change(tree_idx, change)\n",
    "\n",
    "    def _propagate_change(self, tree_idx, change):\n",
    "        \"\"\"change the sum of priority value in all parent nodes\"\"\"\n",
    "        parent_idx = (tree_idx - 1) // 2\n",
    "        self.tree[parent_idx] += change\n",
    "        if parent_idx != 0:\n",
    "            self._propagate_change(parent_idx, change)\n",
    "\n",
    "    def get_leaf(self, lower_bound):\n",
    "        leaf_idx = self._retrieve(lower_bound)  # search the max leaf priority based on the lower_bound\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return [leaf_idx, self.tree[leaf_idx], self.data[data_idx]]\n",
    "\n",
    "    def _retrieve(self, lower_bound, parent_idx=0):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        left_child_idx = 2 * parent_idx + 1\n",
    "        right_child_idx = left_child_idx + 1\n",
    "\n",
    "        if left_child_idx >= len(self.tree):  # end search when no more child\n",
    "            return parent_idx\n",
    "\n",
    "        if self.tree[left_child_idx] == self.tree[right_child_idx]:\n",
    "            return self._retrieve(lower_bound, np.random.choice([left_child_idx, right_child_idx]))\n",
    "        if lower_bound <= self.tree[left_child_idx]:  # downward search, always search for a higher priority node\n",
    "            return self._retrieve(lower_bound, left_child_idx)\n",
    "        else:\n",
    "            return self._retrieve(lower_bound - self.tree[left_child_idx], right_child_idx)\n",
    "\n",
    "    @property\n",
    "    def root_priority(self):\n",
    "        return self.tree[0]  # the root\n",
    "\n",
    "\n",
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    epsilon = 0.001  # small amount to avoid zero priority\n",
    "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 1e-5  # annealing the bias\n",
    "    abs_err_upper = 1   # for stability refer to paper\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def store(self, error, transition):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add_new_priority(p, transition)\n",
    "\n",
    "    def prio_sample(self, n):\n",
    "        batch_idx, batch_memory, ISWeights = [], [], []\n",
    "        segment = self.tree.root_priority / n\n",
    "        self.beta = np.min([1, self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "\n",
    "        min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.root_priority\n",
    "        maxiwi = np.power(self.tree.capacity * min_prob, -self.beta)  # for later normalizing ISWeights\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            lower_bound = np.random.uniform(a, b)\n",
    "            while True:\n",
    "                idx, p, data = self.tree.get_leaf(lower_bound)\n",
    "                if type(data) is int:\n",
    "                    i -= 1\n",
    "                    lower_bound = np.random.uniform(segment * i, segment * (i+1))\n",
    "                else:\n",
    "                    break\n",
    "            prob = p / self.tree.root_priority\n",
    "            ISWeights.append(self.tree.capacity * prob)\n",
    "            batch_idx.append(idx)\n",
    "            batch_memory.append(data)\n",
    "\n",
    "        ISWeights = np.vstack(ISWeights)\n",
    "        ISWeights = np.power(ISWeights, -self.beta) / maxiwi  # normalize\n",
    "        return batch_idx, np.vstack(batch_memory), ISWeights\n",
    "\n",
    "    def random_sample(self, n):\n",
    "        idx = np.random.randint(0, self.tree.capacity, size=n, dtype=np.int)\n",
    "        return np.vstack(self.tree.data[idx])\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        error += self.epsilon   # avoid 0\n",
    "        clipped_error = np.clip(error, 0, self.abs_err_upper)\n",
    "        return np.power(clipped_error, self.alpha)\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop for trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data/DDPG.ckpt-1200000\n",
      "Episode: 0 | Achieve  | Running_r: 273 | Epi_r: 273.78 | Exploration: 0.090 | Pos: 88 | LR_A: 0.000013 | LR_C: 0.000013\n",
      "Episode: 1 | Achieve  | Running_r: 273 | Epi_r: 275.58 | Exploration: 0.080 | Pos: 88 | LR_A: 0.000013 | LR_C: 0.000013\n",
      "Episode: 2 | ----- | Running_r: 256 | Epi_r: -64.07 | Exploration: 0.069 | Pos: 15 | LR_A: 0.000013 | LR_C: 0.000013\n",
      "Episode: 3 | Achieve  | Running_r: 257 | Epi_r: 274.45 | Exploration: 0.062 | Pos: 88 | LR_A: 0.000013 | LR_C: 0.000013\n",
      "Episode: 4 | Achieve  | Running_r: 258 | Epi_r: 277.56 | Exploration: 0.055 | Pos: 88 | LR_A: 0.000013 | LR_C: 0.000013\n",
      "Episode: 5 | Achieve  | Running_r: 259 | Epi_r: 276.76 | Exploration: 0.049 | Pos: 88 | LR_A: 0.000012 | LR_C: 0.000012\n",
      "Episode: 6 | ----- | Running_r: 249 | Epi_r: 57.10 | Exploration: 0.048 | Pos: 24 | LR_A: 0.000012 | LR_C: 0.000012\n",
      "Episode: 7 | ----- | Running_r: 238 | Epi_r: 33.57 | Exploration: 0.047 | Pos: 17 | LR_A: 0.000012 | LR_C: 0.000012\n",
      "Episode: 8 | Achieve  | Running_r: 240 | Epi_r: 278.81 | Exploration: 0.042 | Pos: 88 | LR_A: 0.000012 | LR_C: 0.000012\n",
      "Episode: 9 | Achieve  | Running_r: 242 | Epi_r: 274.64 | Exploration: 0.037 | Pos: 88 | LR_A: 0.000012 | LR_C: 0.000012\n"
     ]
    }
   ],
   "source": [
    "########################################  Hyperparameters  ########################################\n",
    "\n",
    "MAX_EPISODES = 500\n",
    "LR_A = 0.0005  # learning rate for actor\n",
    "LR_C = 0.0005  # learning rate for mcritic\n",
    "GAMMA = 0.999  # reward discount\n",
    "REPLACE_ITER_A = 1700\n",
    "REPLACE_ITER_C = 1500\n",
    "MEMORY_CAPACITY = 200000\n",
    "BATCH_SIZE = 32\n",
    "DISPLAY_THRESHOLD = 100  # display until the running reward > 100\n",
    "DATA_PATH = './data'\n",
    "SAVE_MODEL_ITER = 100000\n",
    "RENDER = False\n",
    "OUTPUT_GRAPH = False\n",
    "\n",
    "GLOBAL_STEP = tf.Variable(0, trainable=False)\n",
    "INCREASE_GS = GLOBAL_STEP.assign(tf.add(GLOBAL_STEP, 1))\n",
    "LR_A = tf.train.exponential_decay(LR_A, GLOBAL_STEP, 10000, .97, staircase=True)\n",
    "LR_C = tf.train.exponential_decay(LR_C, GLOBAL_STEP, 10000, .97, staircase=True)\n",
    "END_POINT = (200 - 10) * (14/30)    # from game\n",
    "\n",
    "##################################################\n",
    "LOAD_MODEL = True # Whether to load trained model#\n",
    "##################################################\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Create actor and critic.\n",
    "actor = Actor(sess, ACTION_DIM, ACTION_BOUND, LR_A, REPLACE_ITER_A)\n",
    "critic = Critic(sess, STATE_DIM, ACTION_DIM, LR_C, GAMMA, REPLACE_ITER_C, actor.a, actor.a_)\n",
    "actor.add_grad_to_graph(critic.a_grads)\n",
    "\n",
    "M = Memory(MEMORY_CAPACITY)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)  # Maximum number of recent checkpoints to keep. Defaults to 5.\n",
    "\n",
    "\n",
    "################################# Determine whether it's a new training or going-on training ###############3\n",
    "if LOAD_MODEL: # Returns CheckpointState proto from the \"checkpoint\" file.\n",
    "    all_ckpt = tf.train.get_checkpoint_state('./data', 'checkpoint').all_model_checkpoint_paths\n",
    "    saver.restore(sess, all_ckpt[-1]) # reload trained parameters into the tf session\n",
    "else:\n",
    "    if os.path.isdir(DATA_PATH): shutil.rmtree(DATA_PATH) # recursively remove all files under directory\n",
    "    os.mkdir(DATA_PATH)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter('logs', graph=sess.graph)\n",
    "\n",
    "var = 0.1  # control exploration\n",
    "var_min = 0.01\n",
    "\n",
    "\n",
    "#################################  Main loop for training #################################\n",
    "for i_episode in range(MAX_EPISODES):\n",
    "    # s = (hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements.)\n",
    "    s = env.reset()\n",
    "    ep_r = 0 # episode reward\n",
    "    \n",
    "    while True:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "            \n",
    "        a = actor.choose_action(s)\n",
    "        a = np.clip(np.random.normal(a, var), -1, 1)    # add randomness to action selection for exploration\n",
    "        s_, r, done, _ = env.step(a)    # r = total 300+ points up to the far end. If the robot falls, it gets -100.\n",
    "        \n",
    "        # when r=-100, that means BipedalWalker has falled to the groud\n",
    "        if r == -100: r = -2\n",
    "        ep_r += r\n",
    "\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        max_p = np.max(M.tree.tree[-M.tree.capacity:])\n",
    "        M.store(max_p, transition)\n",
    "        \n",
    "        \n",
    "        # when the training reaches certain stage, we lessen the probability of exploration\n",
    "        if GLOBAL_STEP.eval(sess) > MEMORY_CAPACITY/20:\n",
    "            var = max([var*0.9999, var_min])  # decay the action randomness\n",
    "            tree_idx, b_M, ISWeights = M.prio_sample(BATCH_SIZE)    # for critic update\n",
    "            b_s = b_M[:, :STATE_DIM]\n",
    "            b_a = b_M[:, STATE_DIM: STATE_DIM + ACTION_DIM]\n",
    "            b_r = b_M[:, -STATE_DIM - 1: -STATE_DIM]\n",
    "            b_s_ = b_M[:, -STATE_DIM:]\n",
    "            \n",
    "            # Critic updates its parameters\n",
    "            abs_td = critic.learn(b_s, b_a, b_r, b_s_, ISWeights)\n",
    "            \n",
    "            # Actor updates its parameters\n",
    "            actor.learn(b_s)\n",
    "            \n",
    "            for i in range(len(tree_idx)):  # update priority\n",
    "                idx = tree_idx[i]\n",
    "                M.update(idx, abs_td[i])\n",
    "                \n",
    "        if GLOBAL_STEP.eval(sess) % SAVE_MODEL_ITER == 0:\n",
    "            ckpt_path = os.path.join(DATA_PATH, 'DDPG.ckpt')\n",
    "            save_path = saver.save(sess, ckpt_path, global_step=GLOBAL_STEP, write_meta_graph=False)\n",
    "            print(\"\\nSave Model %s\\n\" % save_path)\n",
    "\n",
    "        if done:\n",
    "            if \"running_r\" not in globals():\n",
    "                running_r = ep_r\n",
    "            else:\n",
    "                running_r = 0.95*running_r + 0.05*ep_r\n",
    "            if running_r > DISPLAY_THRESHOLD: RENDER = True\n",
    "            else: RENDER = False\n",
    "\n",
    "            done = '| Achieve ' if env.unwrapped.hull.position[0] >= END_POINT else '| -----'\n",
    "            print('Episode:', i_episode,\n",
    "                  done,\n",
    "                  '| Running_r: %i' % int(running_r),\n",
    "                  '| Epi_r: %.2f' % ep_r,\n",
    "                  '| Exploration: %.3f' % var,\n",
    "                  '| Pos: %.i' % int(env.unwrapped.hull.position[0]),\n",
    "                  '| LR_A: %.6f' % sess.run(LR_A),\n",
    "                  '| LR_C: %.6f' % sess.run(LR_C),\n",
    "                  )\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "        sess.run(INCREASE_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
