{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG - BipedalWalker-v2\n",
    "\n",
    "- Xinyao Qian\n",
    "- Tianhao Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get familiar with the BipedalWalker-v2 environment first\n",
    "\n",
    "Find that BipedalWalker behaves embarrasingly bad if taking random walking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Load Environment\n",
    "ENV_NAME = 'BipedalWalker-v2'\n",
    "env = gym.make(ENV_NAME)\n",
    "# Repeoducible environment parameters\n",
    "env.seed(1)\n",
    "\n",
    "s=env.reset()\n",
    "episode=100\n",
    "steps=5000\n",
    "while i in range(episode):\n",
    "    for j in range(steps):\n",
    "        env.render()\n",
    "        a=env.action_space.sample()\n",
    "        s_,r,d,_=env.step(a)\n",
    "\n",
    "        if d:\n",
    "            s=env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Since the action space of BipedalWalker is consecutive, that means value based models such as Q-Learning or DQN, are not applicable**, because value based models generally try to fit a better value function that tells us how good it is to be at a certain state s (V(s)) or to take action a at the state s (Q(s,a)), and then we still need to choose specific action based on our exploring strategy (e.g. $\\epsilon$-greedy). Obviously, it can't work when our actions are consecutive/countless.\n",
    "\n",
    "So then we consider using **policy based models**, for example, REINFORCE. However, there is another problem that REINFORCE can only update parameters/learn everytime an episode ends, which slowed the convergence process. \n",
    "\n",
    "Then we get to know that there is another series of models that called **Actor Critic which combines the advantages of both the value based model and the policy based model and make it possible for policy based models to update itself at every step**. \n",
    "\n",
    "Specifically, we simultaneously train a policy gradients network and a Q-Learning network. The policy network behaves as the actor which takes in observations and outputs best actions to be taken, while the value network will behave as a critic to take in observations and tell the actor how 'good' to be at the current state, so that the actor can know how good its last action that brought it here was, and update its parameters according to this feedback, while the critic can also update its own parameters in the way Q-Learning does. **In a sense, actor and critic are supervising each other to become better and better**.\n",
    "\n",
    "<center>\n",
    "![](https://morvanzhou.github.io/static/results/ML-intro/AC3.png)\n",
    "</center>\n",
    "\n",
    "\n",
    "> https://morvanzhou.github.io/static/results/ML-intro/AC3.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment preparation & Definition of Classes: Actor, Critic, Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "class Actor:\n",
    "    def __init__(self, sess, learning_rate,action_dim,action_bound):\n",
    "        self.sess = sess\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # input current state, output action to be taken\n",
    "        self.a = self.build_neural_network(S, scope='eval_nn', trainable=True)\n",
    "\n",
    "        self.a_ = self.build_neural_network(S_, scope='target_nn', trainable=False)\n",
    "\n",
    "        self.eval_parameters = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval_nn')\n",
    "        self.target_parameters = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target_nn')\n",
    "\n",
    "\n",
    "    def act(self, s):\n",
    "        s = s[np.newaxis, :] \n",
    "        return self.sess.run(self.a, feed_dict={s: state})[0] \n",
    "\n",
    "    def learn(self, state):  # update parameters\n",
    "        self.sess.run(self.train_op, feed_dict={s: state})\n",
    "        if self.t_replace_counter % self.t_replace_iter == 0:\n",
    "            self.sess.run([tf.assign(t, e) for t, e in zip(self.target_parameters, self.eval_parameters)])\n",
    "        self.t_replace_counter += 1\n",
    "    \n",
    "    def build_neural_network(self, s, scope, trainable):\n",
    "        \n",
    "        init_weights = tf.random_normal_initializer(0., 0.1)\n",
    "        init_bias = tf.constant_initializer(0.01)\n",
    "        # three dense layer networks\n",
    "        nn = tf.layers.dense(s, 500, activation=tf.nn.relu,\n",
    "                              kernel_initializer=init_weights, bias_initializer=init_bias, name='l1', trainable=trainable)\n",
    "        nn = tf.layers.dense(nn, 200, activation=tf.nn.relu,\n",
    "                              kernel_initializer=init_weights, bias_initializer=init_bias, name='l2', trainable=trainable)\n",
    "        actions = tf.layers.dense(nn, self.action_dim, activation=tf.nn.tanh, kernel_initializer=init_weights,\n",
    "                                  bias_initializer=init_bias, name='a', trainable=trainable)\n",
    "        scaled_actions = tf.multiply(actions, self.action_bound, name='scaled_actions')  \n",
    "\n",
    "        return scaled_actions\n",
    "\n",
    "    def add_gradient(self, a_gradients):\n",
    "        self.policy_gradients_and_vars = tf.gradients(ys=self.a, xs=self.eval_parameters, grad_ys=a_gradients)\n",
    "        opt = tf.train.RMSPropOptimizer(-self.learning_rate) # gradient ascent\n",
    "        self.train_op = opt.apply_gradients(zip(self.policy_gradients_and_vars, self.eval_parameters), global_step=GLOBAL_STEP)\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, discount_factor, action, next_action):\n",
    "        self.sess = sess\n",
    "        self.state_dimension = state_dim\n",
    "        self.action_dimension = action_dim\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        # Input (state, action) pair, output q values\n",
    "        self.action = action\n",
    "        self.q = self.build_nn(S, self.action, 'evaluation_nn', trainable=True)\n",
    "\n",
    "        self.next_q = self.build_nn(NextState, next_action, 'target_nn', trainable=False)    \n",
    "\n",
    "        self.evaluation_parameters = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        self.target_parameters = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "        \n",
    "        self.target_q = R + self.discount_factor * self.q_\n",
    "\n",
    "\n",
    "        # temporal difference\n",
    "        self.td = tf.abs(self.target_q - self.q)\n",
    "        self.weights = tf.placeholder(tf.float32, [None, 1], name='weights')\n",
    "        \n",
    "        self.loss = tf.reduce_mean(self.weights * tf.squared_difference(self.target_q, self.q))\n",
    "        self.train_optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=GLOBAL_STEP) # add global_step parameters to ensure increment of global_step\n",
    "        self.action_gradients = tf.gradients(self.q, a)[0]   \n",
    "\n",
    "\n",
    "    def learn(self, state, action, reward, nextState, weights):\n",
    "        _, td = self.sess.run([self.train_op, self.td], feed_dict={State: state, self.action: action, R: reward, NextState: nextState, self.weights: weights})\n",
    "        \n",
    "        self.sess.run([tf.assign(t, e) for t, e in zip(self.target_parameters, self.evaluation_parameters)])\n",
    "        return td\n",
    "\n",
    "    def build_nn(self, s, a, scope, trainable):\n",
    "        \n",
    "        init_weights = tf.random_normal_initializer(0., 0.01)\n",
    "        init_bias = tf.constant_initializer(0.01)\n",
    "\n",
    "        w1_state = tf.get_variable('w1_state', [self.state_dimension, 500], initializer=init_weights, trainable=trainable)\n",
    "        w1_action= tf.get_variable('w1_action', [self.state_dimension, 500], initializer=init_weights, trainable=trainable)\n",
    "        b1 = tf.get_variable('b1', [1, 500], initializer=init_bias, trainable=trainable)\n",
    "        nn = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "        \n",
    "        nn = tf.layers.dense(nn, 20, activation=tf.nn.relu, kernel_initializer=init_weights,\n",
    "                                  bias_initializer=init_bias, name='l2', trainable=trainable)\n",
    "        \n",
    "        q = tf.layers.dense(nn, 1, kernel_initializer=init_weights, bias_initializer=init_bias, trainable=trainable)   # Q(s,a)\n",
    "        return q\n",
    "\n",
    "\n",
    "# https://github.com/jaromiru/AI-blog/blob/master/SumTree.py\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "\n",
    "    \n",
    "# https://github.com/jaromiru/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "\n",
    "class Memory:   # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample) \n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append( (idx, data) )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop for trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data/DDPG.ckpt-1200000\n",
      "Episode: 0 | Achieve  | Running_r: 271 | Epi_r: 271.74 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n",
      "Episode: 1 | Achieve  | Running_r: 271 | Epi_r: 269.24 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n",
      "Episode: 2 | Achieve  | Running_r: 271 | Epi_r: 273.15 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n",
      "Episode: 3 | Achieve  | Running_r: 271 | Epi_r: 271.24 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n",
      "Episode: 4 | Achieve  | Running_r: 271 | Epi_r: 269.90 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n",
      "Episode: 5 | Achieve  | Running_r: 271 | Epi_r: 268.49 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n",
      "Episode: 6 | Achieve  | Running_r: 271 | Epi_r: 271.28 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n",
      "Episode: 7 | Achieve  | Running_r: 271 | Epi_r: 269.52 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n",
      "Episode: 8 | Achieve  | Running_r: 271 | Epi_r: 270.98 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n",
      "Episode: 9 | Achieve  | Running_r: 271 | Epi_r: 270.82 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n",
      "Episode: 10 | Achieve  | Running_r: 271 | Epi_r: 268.31 | Exploration: 0.000 | Pos: 88 | LR_A: 0.000000 | LR_C: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# reproducible results\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "# Load Environment\n",
    "ENV_NAME = 'BipedalWalker-v2'\n",
    "env = gym.make(ENV_NAME)\n",
    "# Reproducible environment parameters\n",
    "env.seed(1)\n",
    "\n",
    "\n",
    "STATE_DIMENSION = env.observation_space.shape[0] \n",
    "ACTION_DIMENSION = env.action_space.shape[0] \n",
    "ACTION_BOUND = env.action_space.high \n",
    "\n",
    "########################################  Hyperparameters  ########################################\n",
    "\n",
    "# number of episodes to be trained\n",
    "TRAIN_EPI_NUM=500\n",
    "# Learning rate for actor and critic\n",
    "ACTOR_LR=0.05\n",
    "CRITIC_LR=0.05\n",
    "R_DISCOUNT=0.9 # reward discount\n",
    "\n",
    "MEMORY_CAPACITY=1000000\n",
    "\n",
    "ACTOR_REP_ITE=1700 # after such many iterations, update ACTOR\n",
    "CRITIC_REP_ITE=1500\n",
    "\n",
    "BATCH=40 # size of batch used to learn\n",
    "\n",
    "# Path used to store training result (parameters)\n",
    "TRAIN_DATA_PATH='./train'\n",
    "\n",
    "\n",
    "GLOBAL_STEP = tf.Variable(0, trainable=False) # record how many steps we have gone through\n",
    "INCREASE_GLOBAL_STEP = GLOBAL_STEP.assign(tf.add(GLOBAL_STEP, 1))\n",
    "\n",
    "\n",
    "# set automatically decaying learning rate to ensure convergence\n",
    "ACTOR_LR = tf.train.exponential_decay(LR_A, GLOBAL_STEP, 10000, .95, staircase=True)\n",
    "CRITIC_LR = tf.train.exponential_decay(LR_C, GLOBAL_STEP, 10000, .90, staircase=True)\n",
    "\n",
    "\n",
    "END_POINT = (200 - 10) * (14/30)    # The end point of the game\n",
    "\n",
    "\n",
    "##################################################\n",
    "LOAD_MODEL = True # Whether to load trained model#\n",
    "##################################################\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Create actor and critic.\n",
    "    actor = Actor(sess, ACTION_DIMENSION, ACTION_BOUND, ACTOR_LR, REPLACE_ITER_A)\n",
    "    critic = Critic(sess, STATE_DIMENSION, ACTION_DIMENSION, CRITIC_LR, R_DISCOUNT, REPLACE_ITER_C, actor.a, actor.a_)\n",
    "\n",
    "    actor.add_grad_to_graph(critic.a_grads)\n",
    "\n",
    "    # Memory class implementation from: https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    memory = Memory(MEMORY_CAPACITY)\n",
    "\n",
    "    # saver is used to store or restore trained parameters\n",
    "    saver = tf.train.Saver(max_to_keep=100)  # Maximum number of recent checkpoints to keep. Defaults to 5.\n",
    "\n",
    "\n",
    "    ################################# Determine whether it's a new training or going-on training ###############3\n",
    "    if LOAD_MODEL: # Returns CheckpointState proto from the \"checkpoint\" file.\n",
    "        checkpoints = tf.train.get_checkpoint_state(TRAIN_DATA_PATH, 'checkpoint').all_model_checkpoint_paths\n",
    "        saver.restore(sess, checkpoints[-1]) # reload trained parameters into the tf session\n",
    "    else:\n",
    "        if os.path.isdir(TRAIN_DATA_PATH): \n",
    "          shutil.rmtree(TRAIN_DATA_PATH) # recursively remove all files under directory\n",
    "        os.mkdir(TRAIN_DATA_PATH)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    explore_degree=0.1\n",
    "    explore_degree_minimum=0.0001\n",
    "    explore_decay_factor=0.99\n",
    "\n",
    "    #################################  Main loop for training #################################\n",
    "    for i_episode in range(MAX_EPISODES):\n",
    "        \n",
    "        state = env.reset()\n",
    "        episode_reward = 0 # the episode reward\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            action = actor.act(s)\n",
    "\n",
    "            action = np.clip(np.random.normal(action, explore_degree), -ACTION_BOUND, ACTION_BOUND)   # explore using randomness\n",
    "            next_state, reward, done, _ = env.step(a) \n",
    "\n",
    "            trainsition = np.hstack((s, a, [r], s_))\n",
    "            probability = np.max(memory.tree.tree[-memory.tree.capacity:])\n",
    "            memory.store(probability, transition)  # stored for later learning\n",
    "\n",
    "            # when r=-100, that means BipedalWalker has falled to the groud\n",
    "            episode_reward += reward\n",
    "\n",
    "\n",
    "            # when the training reaches stable stage, we lessen the probability of exploration\n",
    "            if GLOBAL_STEP.eval(sess) > MEMORY_CAPACITY/20:\n",
    "                explore_degree = max([explore_decay_factor*explore_degree, explore_degree_minimum])  # decay the action randomness\n",
    "                tree_index, b_memory, weights = memory.prio_sample(BATCH)    # for critic update\n",
    "\n",
    "                b_state = b_memory[:, :STATE_DIMENSION]\n",
    "                b_action = b_memory[:, STATE_DIMENSION: STATE_DIMENSION + ACTION_DIMENSION]\n",
    "                b_reward = b_memory[:, -STATE_DIMENSION - 1: -STATE_DIMENSION]\n",
    "                b_next_state = b_memory[:, -STATE_DIMENSION:]\n",
    "                \n",
    "                td = critic.learn(b_state, b_action, b_reward, b_next_state, weights)\n",
    "                actor.learn(b_state)\n",
    "                \n",
    "                for i in range(len(tree_index)):  # update priority\n",
    "                    index = tree_idx[i]\n",
    "                    memory.update(index, td[i])\n",
    "\n",
    "\n",
    "            # if GLOBAL_STEP.eval(sess) % SAVE_MODEL_ITER == 0:\n",
    "            #     ckpt_path = os.path.join(TRAIN_DATA_PATH, 'DDPG.ckpt')\n",
    "            #     save_path = saver.save(sess, ckpt_path, global_step=GLOBAL_STEP, write_meta_graph=False)\n",
    "            #     print(\"\\nSave Model %s\\n\" % save_path)\n",
    "\n",
    "            if done:\n",
    "                if \"running_reward\" not in globals():\n",
    "                    running_reward = episode_reward\n",
    "                else:\n",
    "                    running_reward = 0.95*running_r + 0.05*ep_r\n",
    "                \n",
    "                print('Episode:',i_episode,'running reward: ',running_reward,', episode reward: ',episode_reward)\n",
    "                break # start new episode\n",
    "\n",
    "            state = nextState\n",
    "            sess.run(INCREASE_GLOBAL_STEP)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
