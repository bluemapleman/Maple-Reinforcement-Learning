{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tutorial\n",
    "\n",
    "[An intro to Advantage Actor Critic methods: let’s play Sonic the Hedgehog!\n",
    "](https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d)\n",
    "\n",
    "# Introducing Actor Critic\n",
    "\n",
    "The Actor Critic model is a better score function. Instead of waiting until the end of the episode as we do in Monte Carlo REINFORCE, we make an update at each step (TD Learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AC Algorithm for continuous action_space\n",
    "# Source: https://github.com/allanbreyes/bipedal-walker/blob/master/writeup.md\n",
    "\n",
    "# randomly initialize critic network Q and actor μ\n",
    "# initialize target network Q' and μ'\n",
    "# initialize replay buffer R\n",
    "# for episode = 1, M do:\n",
    "#   initialize a random process N for action exploration\n",
    "#   receive initial observation state s1\n",
    "#   for t = 1, T do:\n",
    "#     select action a_t = μ + N_t according to current policy\n",
    "#     execute action a_t and observe reward r_t and new state s_t+1\n",
    "#     store experience in replay buffer\n",
    "#     sample a random minibatch of N transitions from R\n",
    "#     update target values according to discount, γ\n",
    "#     update the actor policy using the sampled policy gradient\n",
    "#     update the target networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   reward: -105\n",
      "episode: 1   reward: -110\n",
      "episode: 2   reward: -114\n",
      "episode: 3   reward: -120\n",
      "episode: 4   reward: -122\n",
      "episode: 5   reward: -128\n",
      "episode: 6   reward: -131\n",
      "episode: 7   reward: -135\n",
      "episode: 8   reward: -135\n",
      "episode: 9   reward: -138\n",
      "episode: 10   reward: -139\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actor-Critic with continuous action using TD-error as the Advantage, Reinforcement Learning.\n",
    "The Pendulum example (based on https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb)\n",
    "Cannot converge!!! oscillate!!!\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "tensorflow r1.3\n",
    "gym 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, action_bound, lr=0.0001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.float32, None, name=\"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, name=\"td_error\")  # TD_error\n",
    "\n",
    "        l1 = tf.layers.dense(\n",
    "            inputs=self.s,\n",
    "            units=30,  # number of hidden units\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='l1'\n",
    "        )\n",
    "        \n",
    "        # average value for action\n",
    "        mu = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=1,  # number of hidden units\n",
    "            activation=tf.nn.tanh, # use tanh to scale output to [-1,1]\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='mu'\n",
    "        )\n",
    "        \n",
    "        # deviation value for action\n",
    "        sigma = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=1,  # output units\n",
    "            activation=tf.nn.softplus,  # get action probabilities\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(1.),  # biases\n",
    "            name='sigma'\n",
    "        )\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        # self.e = epsilon = tf.train.exponential_decay(2., global_step, 1000, 0.9)\n",
    "        self.mu, self.sigma = tf.squeeze(mu*2), tf.squeeze(sigma+0.1)\n",
    "        self.normal_dist = tf.distributions.Normal(self.mu, self.sigma)\n",
    "        \n",
    "        # key: sample from the normal districution (mu,sigma) as the action to be taken\n",
    "        self.action = tf.clip_by_value(self.normal_dist.sample(1), action_bound[0], action_bound[1]) # ensure no action value is invalid\n",
    "\n",
    "        with tf.name_scope('exp_v'):\n",
    "            log_prob = self.normal_dist.log_prob(self.a)  # loss without advantage\n",
    "            self.exp_v = log_prob * self.td_error  # advantage (TD_error) guided loss\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.exp_v += 0.01*self.normal_dist.entropy()\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)    # min(v) = max(-v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.action, {self.s: s})  # get probabilities for all actions\n",
    "\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "            self.v_ = tf.placeholder(tf.float32, [1, 1], name=\"v_next\")\n",
    "            self.r = tf.placeholder(tf.float32, name='r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=30,  # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = tf.reduce_mean(self.r + GAMMA * self.v_ - self.v)\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 1000\n",
    "MAX_EP_STEPS = 200\n",
    "DISPLAY_REWARD_THRESHOLD = 100  # renders environment if total episode reward is greater then this threshold\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = env.action_space.high\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_S, lr=LR_A, action_bound=[-A_BOUND, A_BOUND])\n",
    "critic = Critic(sess, n_features=N_S, lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    ep_rs = []\n",
    "    while True:\n",
    "        # if RENDER:\n",
    "        env.render()\n",
    "        a = actor.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "        r /= 10\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "        ep_rs.append(r)\n",
    "        if t > MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(ep_rs)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   reward: -105\n",
      "episode: 1   reward: -110\n",
      "episode: 2   reward: -114\n",
      "episode: 3   reward: -120\n",
      "episode: 4   reward: -122\n",
      "episode: 5   reward: -128\n",
      "episode: 6   reward: -131\n",
      "episode: 7   reward: -135\n",
      "episode: 8   reward: -135\n",
      "episode: 9   reward: -138\n",
      "episode: 10   reward: -139\n",
      "episode: 11   reward: -142\n",
      "episode: 12   reward: -145\n",
      "episode: 13   reward: -146\n",
      "episode: 14   reward: -146\n",
      "episode: 15   reward: -148\n",
      "episode: 16   reward: -149\n",
      "episode: 17   reward: -148\n",
      "episode: 18   reward: -147\n",
      "episode: 19   reward: -147\n",
      "episode: 20   reward: -148\n",
      "episode: 21   reward: -147\n",
      "episode: 22   reward: -143\n",
      "episode: 23   reward: -144\n",
      "episode: 24   reward: -144\n",
      "episode: 25   reward: -144\n",
      "episode: 26   reward: -143\n",
      "episode: 27   reward: -143\n",
      "episode: 28   reward: -141\n",
      "episode: 29   reward: -139\n",
      "episode: 30   reward: -135\n",
      "episode: 31   reward: -135\n",
      "episode: 32   reward: -132\n",
      "episode: 33   reward: -132\n",
      "episode: 34   reward: -131\n",
      "episode: 35   reward: -133\n",
      "episode: 36   reward: -130\n",
      "episode: 37   reward: -132\n",
      "episode: 38   reward: -132\n",
      "episode: 39   reward: -133\n",
      "episode: 40   reward: -133\n",
      "episode: 41   reward: -135\n",
      "episode: 42   reward: -135\n",
      "episode: 43   reward: -136\n",
      "episode: 44   reward: -135\n",
      "episode: 45   reward: -136\n",
      "episode: 46   reward: -133\n",
      "episode: 47   reward: -133\n",
      "episode: 48   reward: -131\n",
      "episode: 49   reward: -131\n",
      "episode: 50   reward: -129\n",
      "episode: 51   reward: -124\n",
      "episode: 52   reward: -126\n",
      "episode: 53   reward: -129\n",
      "episode: 54   reward: -131\n",
      "episode: 55   reward: -132\n",
      "episode: 56   reward: -132\n",
      "episode: 57   reward: -127\n",
      "episode: 58   reward: -124\n",
      "episode: 59   reward: -125\n",
      "episode: 60   reward: -128\n",
      "episode: 61   reward: -129\n",
      "episode: 62   reward: -131\n",
      "episode: 63   reward: -131\n",
      "episode: 64   reward: -133\n",
      "episode: 65   reward: -134\n",
      "episode: 66   reward: -134\n",
      "episode: 67   reward: -136\n",
      "episode: 68   reward: -137\n",
      "episode: 69   reward: -139\n",
      "episode: 70   reward: -134\n",
      "episode: 71   reward: -136\n",
      "episode: 72   reward: -137\n",
      "episode: 73   reward: -138\n",
      "episode: 74   reward: -133\n",
      "episode: 75   reward: -135\n",
      "episode: 76   reward: -136\n",
      "episode: 77   reward: -135\n",
      "episode: 78   reward: -137\n",
      "episode: 79   reward: -138\n",
      "episode: 80   reward: -139\n",
      "episode: 81   reward: -138\n",
      "episode: 82   reward: -138\n",
      "episode: 83   reward: -140\n",
      "episode: 84   reward: -139\n",
      "episode: 85   reward: -139\n",
      "episode: 86   reward: -137\n",
      "episode: 87   reward: -135\n",
      "episode: 88   reward: -134\n",
      "episode: 89   reward: -131\n",
      "episode: 90   reward: -133\n",
      "episode: 91   reward: -131\n",
      "episode: 92   reward: -130\n",
      "episode: 93   reward: -126\n",
      "episode: 94   reward: -123\n",
      "episode: 95   reward: -122\n",
      "episode: 96   reward: -117\n",
      "episode: 97   reward: -114\n",
      "episode: 98   reward: -113\n",
      "episode: 99   reward: -116\n",
      "episode: 100   reward: -117\n",
      "episode: 101   reward: -114\n",
      "episode: 102   reward: -117\n",
      "episode: 103   reward: -115\n",
      "episode: 104   reward: -112\n",
      "episode: 105   reward: -107\n",
      "episode: 106   reward: -104\n",
      "episode: 107   reward: -98\n",
      "episode: 108   reward: -100\n",
      "episode: 109   reward: -101\n",
      "episode: 110   reward: -98\n",
      "episode: 111   reward: -101\n",
      "episode: 112   reward: -102\n",
      "episode: 113   reward: -102\n",
      "episode: 114   reward: -98\n",
      "episode: 115   reward: -101\n",
      "episode: 116   reward: -100\n",
      "episode: 117   reward: -103\n",
      "episode: 118   reward: -96\n",
      "episode: 119   reward: -97\n",
      "episode: 120   reward: -95\n",
      "episode: 121   reward: -92\n",
      "episode: 122   reward: -93\n",
      "episode: 123   reward: -89\n",
      "episode: 124   reward: -92\n",
      "episode: 125   reward: -90\n",
      "episode: 126   reward: -91\n",
      "episode: 127   reward: -94\n",
      "episode: 128   reward: -93\n",
      "episode: 129   reward: -93\n",
      "episode: 130   reward: -87\n",
      "episode: 131   reward: -91\n",
      "episode: 132   reward: -86\n",
      "episode: 133   reward: -80\n",
      "episode: 134   reward: -85\n",
      "episode: 135   reward: -81\n",
      "episode: 136   reward: -85\n",
      "episode: 137   reward: -87\n",
      "episode: 138   reward: -85\n",
      "episode: 139   reward: -87\n",
      "episode: 140   reward: -87\n",
      "episode: 141   reward: -88\n",
      "episode: 142   reward: -90\n",
      "episode: 143   reward: -95\n",
      "episode: 144   reward: -100\n",
      "episode: 145   reward: -103\n",
      "episode: 146   reward: -104\n",
      "episode: 147   reward: -98\n",
      "episode: 148   reward: -102\n",
      "episode: 149   reward: -99\n",
      "episode: 150   reward: -99\n",
      "episode: 151   reward: -103\n",
      "episode: 152   reward: -106\n",
      "episode: 153   reward: -107\n",
      "episode: 154   reward: -108\n",
      "episode: 155   reward: -108\n",
      "episode: 156   reward: -107\n",
      "episode: 157   reward: -106\n",
      "episode: 158   reward: -106\n",
      "episode: 159   reward: -106\n",
      "episode: 160   reward: -99\n",
      "episode: 161   reward: -97\n",
      "episode: 162   reward: -94\n",
      "episode: 163   reward: -84\n",
      "episode: 164   reward: -80\n",
      "episode: 165   reward: -78\n",
      "episode: 166   reward: -78\n",
      "episode: 167   reward: -79\n",
      "episode: 168   reward: -79\n",
      "episode: 169   reward: -76\n",
      "episode: 170   reward: -72\n",
      "episode: 171   reward: -77\n",
      "episode: 172   reward: -79\n",
      "episode: 173   reward: -73\n",
      "episode: 174   reward: -67\n",
      "episode: 175   reward: -72\n",
      "episode: 176   reward: -74\n",
      "episode: 177   reward: -76\n",
      "episode: 178   reward: -75\n",
      "episode: 179   reward: -76\n",
      "episode: 180   reward: -71\n",
      "episode: 181   reward: -74\n",
      "episode: 182   reward: -70\n",
      "episode: 183   reward: -66\n",
      "episode: 184   reward: -66\n",
      "episode: 185   reward: -67\n",
      "episode: 186   reward: -64\n",
      "episode: 187   reward: -59\n",
      "episode: 188   reward: -57\n",
      "episode: 189   reward: -56\n",
      "episode: 190   reward: -59\n",
      "episode: 191   reward: -65\n",
      "episode: 192   reward: -65\n",
      "episode: 193   reward: -68\n",
      "episode: 194   reward: -71\n",
      "episode: 195   reward: -69\n",
      "episode: 196   reward: -72\n",
      "episode: 197   reward: -65\n",
      "episode: 198   reward: -67\n",
      "episode: 199   reward: -69\n",
      "episode: 200   reward: -69\n",
      "episode: 201   reward: -62\n",
      "episode: 202   reward: -61\n",
      "episode: 203   reward: -57\n",
      "episode: 204   reward: -54\n",
      "episode: 205   reward: -52\n",
      "episode: 206   reward: -51\n",
      "episode: 207   reward: -48\n",
      "episode: 208   reward: -46\n",
      "episode: 209   reward: -41\n",
      "episode: 210   reward: -40\n",
      "episode: 211   reward: -36\n",
      "episode: 212   reward: -42\n",
      "episode: 213   reward: -39\n",
      "episode: 214   reward: -47\n",
      "episode: 215   reward: -44\n",
      "episode: 216   reward: -43\n",
      "episode: 217   reward: -41\n",
      "episode: 218   reward: -45\n",
      "episode: 219   reward: -48\n",
      "episode: 220   reward: -49\n",
      "episode: 221   reward: -52\n",
      "episode: 222   reward: -57\n",
      "episode: 223   reward: -52\n",
      "episode: 224   reward: -52\n",
      "episode: 225   reward: -48\n",
      "episode: 226   reward: -44\n",
      "episode: 227   reward: -46\n",
      "episode: 228   reward: -44\n",
      "episode: 229   reward: -42\n",
      "episode: 230   reward: -38\n",
      "episode: 231   reward: -46\n",
      "episode: 232   reward: -48\n",
      "episode: 233   reward: -44\n",
      "episode: 234   reward: -49\n",
      "episode: 235   reward: -52\n",
      "episode: 236   reward: -57\n",
      "episode: 237   reward: -54\n",
      "episode: 238   reward: -60\n",
      "episode: 239   reward: -57\n",
      "episode: 240   reward: -57\n",
      "episode: 241   reward: -61\n",
      "episode: 242   reward: -60\n",
      "episode: 243   reward: -54\n",
      "episode: 244   reward: -49\n",
      "episode: 245   reward: -45\n",
      "episode: 246   reward: -41\n",
      "episode: 247   reward: -44\n",
      "episode: 248   reward: -40\n",
      "episode: 249   reward: -40\n",
      "episode: 250   reward: -44\n",
      "episode: 251   reward: -51\n",
      "episode: 252   reward: -51\n",
      "episode: 253   reward: -57\n",
      "episode: 254   reward: -63\n",
      "episode: 255   reward: -68\n",
      "episode: 256   reward: -68\n",
      "episode: 257   reward: -71\n",
      "episode: 258   reward: -75\n",
      "episode: 259   reward: -78\n",
      "episode: 260   reward: -78\n",
      "episode: 261   reward: -79\n",
      "episode: 262   reward: -83\n",
      "episode: 263   reward: -84\n",
      "episode: 264   reward: -82\n",
      "episode: 265   reward: -74\n",
      "episode: 266   reward: -71\n",
      "episode: 267   reward: -72\n",
      "episode: 268   reward: -74\n",
      "episode: 269   reward: -75\n",
      "episode: 270   reward: -75\n",
      "episode: 271   reward: -79\n",
      "episode: 272   reward: -77\n",
      "episode: 273   reward: -76\n",
      "episode: 274   reward: -74\n",
      "episode: 275   reward: -73\n",
      "episode: 276   reward: -72\n",
      "episode: 277   reward: -74\n",
      "episode: 278   reward: -72\n",
      "episode: 279   reward: -73\n",
      "episode: 280   reward: -70\n",
      "episode: 281   reward: -69\n",
      "episode: 282   reward: -69\n",
      "episode: 283   reward: -72\n",
      "episode: 284   reward: -75\n",
      "episode: 285   reward: -76\n",
      "episode: 286   reward: -77\n",
      "episode: 287   reward: -84\n",
      "episode: 288   reward: -87\n",
      "episode: 289   reward: -88\n",
      "episode: 290   reward: -91\n",
      "episode: 291   reward: -93\n",
      "episode: 292   reward: -97\n",
      "episode: 293   reward: -102\n",
      "episode: 294   reward: -101\n",
      "episode: 295   reward: -107\n",
      "episode: 296   reward: -109\n",
      "episode: 297   reward: -109\n",
      "episode: 298   reward: -107\n",
      "episode: 299   reward: -110\n",
      "episode: 300   reward: -111\n",
      "episode: 301   reward: -109\n",
      "episode: 302   reward: -111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 303   reward: -116\n",
      "episode: 304   reward: -119\n",
      "episode: 305   reward: -118\n",
      "episode: 306   reward: -114\n",
      "episode: 307   reward: -112\n",
      "episode: 308   reward: -112\n",
      "episode: 309   reward: -107\n",
      "episode: 310   reward: -107\n",
      "episode: 311   reward: -100\n",
      "episode: 312   reward: -102\n",
      "episode: 313   reward: -102\n",
      "episode: 314   reward: -98\n",
      "episode: 315   reward: -100\n",
      "episode: 316   reward: -94\n",
      "episode: 317   reward: -90\n",
      "episode: 318   reward: -89\n",
      "episode: 319   reward: -81\n",
      "episode: 320   reward: -74\n",
      "episode: 321   reward: -74\n",
      "episode: 322   reward: -72\n",
      "episode: 323   reward: -74\n",
      "episode: 324   reward: -77\n",
      "episode: 325   reward: -78\n",
      "episode: 326   reward: -73\n",
      "episode: 327   reward: -66\n",
      "episode: 328   reward: -60\n",
      "episode: 329   reward: -56\n",
      "episode: 330   reward: -59\n",
      "episode: 331   reward: -61\n",
      "episode: 332   reward: -62\n",
      "episode: 333   reward: -60\n",
      "episode: 334   reward: -63\n",
      "episode: 335   reward: -70\n",
      "episode: 336   reward: -68\n",
      "episode: 337   reward: -70\n",
      "episode: 338   reward: -77\n",
      "episode: 339   reward: -79\n",
      "episode: 340   reward: -83\n",
      "episode: 341   reward: -87\n",
      "episode: 342   reward: -90\n",
      "episode: 343   reward: -93\n",
      "episode: 344   reward: -93\n",
      "episode: 345   reward: -96\n",
      "episode: 346   reward: -97\n",
      "episode: 347   reward: -93\n",
      "episode: 348   reward: -89\n",
      "episode: 349   reward: -89\n",
      "episode: 350   reward: -85\n",
      "episode: 351   reward: -86\n",
      "episode: 352   reward: -85\n",
      "episode: 353   reward: -86\n",
      "episode: 354   reward: -85\n",
      "episode: 355   reward: -85\n",
      "episode: 356   reward: -83\n",
      "episode: 357   reward: -82\n",
      "episode: 358   reward: -82\n",
      "episode: 359   reward: -83\n",
      "episode: 360   reward: -88\n",
      "episode: 361   reward: -86\n",
      "episode: 362   reward: -89\n",
      "episode: 363   reward: -89\n",
      "episode: 364   reward: -92\n",
      "episode: 365   reward: -89\n",
      "episode: 366   reward: -87\n",
      "episode: 367   reward: -84\n",
      "episode: 368   reward: -82\n",
      "episode: 369   reward: -80\n",
      "episode: 370   reward: -73\n",
      "episode: 371   reward: -70\n",
      "episode: 372   reward: -75\n",
      "episode: 373   reward: -72\n",
      "episode: 374   reward: -71\n",
      "episode: 375   reward: -69\n",
      "episode: 376   reward: -64\n",
      "episode: 377   reward: -61\n",
      "episode: 378   reward: -59\n",
      "episode: 379   reward: -58\n",
      "episode: 380   reward: -59\n",
      "episode: 381   reward: -59\n",
      "episode: 382   reward: -59\n",
      "episode: 383   reward: -58\n",
      "episode: 384   reward: -54\n",
      "episode: 385   reward: -56\n",
      "episode: 386   reward: -52\n",
      "episode: 387   reward: -53\n",
      "episode: 388   reward: -55\n",
      "episode: 389   reward: -57\n",
      "episode: 390   reward: -53\n",
      "episode: 391   reward: -51\n",
      "episode: 392   reward: -57\n",
      "episode: 393   reward: -63\n",
      "episode: 394   reward: -69\n",
      "episode: 395   reward: -74\n",
      "episode: 396   reward: -80\n",
      "episode: 397   reward: -85\n",
      "episode: 398   reward: -89\n",
      "episode: 399   reward: -93\n",
      "episode: 400   reward: -97\n",
      "episode: 401   reward: -101\n",
      "episode: 402   reward: -103\n",
      "episode: 403   reward: -105\n",
      "episode: 404   reward: -107\n",
      "episode: 405   reward: -108\n",
      "episode: 406   reward: -110\n",
      "episode: 407   reward: -113\n",
      "episode: 408   reward: -115\n",
      "episode: 409   reward: -115\n",
      "episode: 410   reward: -117\n",
      "episode: 411   reward: -119\n",
      "episode: 412   reward: -120\n",
      "episode: 413   reward: -120\n",
      "episode: 414   reward: -114\n",
      "episode: 415   reward: -113\n",
      "episode: 416   reward: -104\n",
      "episode: 417   reward: -105\n",
      "episode: 418   reward: -97\n",
      "episode: 419   reward: -94\n",
      "episode: 420   reward: -97\n",
      "episode: 421   reward: -98\n",
      "episode: 422   reward: -94\n",
      "episode: 423   reward: -96\n",
      "episode: 424   reward: -93\n",
      "episode: 425   reward: -91\n",
      "episode: 426   reward: -91\n",
      "episode: 427   reward: -89\n",
      "episode: 428   reward: -82\n",
      "episode: 429   reward: -76\n",
      "episode: 430   reward: -68\n",
      "episode: 431   reward: -63\n",
      "episode: 432   reward: -62\n",
      "episode: 433   reward: -59\n",
      "episode: 434   reward: -55\n",
      "episode: 435   reward: -54\n",
      "episode: 436   reward: -50\n",
      "episode: 437   reward: -46\n",
      "episode: 438   reward: -52\n",
      "episode: 439   reward: -58\n",
      "episode: 440   reward: -64\n",
      "episode: 441   reward: -70\n",
      "episode: 442   reward: -76\n",
      "episode: 443   reward: -81\n",
      "episode: 444   reward: -73\n",
      "episode: 445   reward: -79\n",
      "episode: 446   reward: -82\n",
      "episode: 447   reward: -87\n",
      "episode: 448   reward: -88\n",
      "episode: 449   reward: -92\n",
      "episode: 450   reward: -86\n",
      "episode: 451   reward: -88\n",
      "episode: 452   reward: -92\n",
      "episode: 453   reward: -86\n",
      "episode: 454   reward: -91\n",
      "episode: 455   reward: -95\n",
      "episode: 456   reward: -85\n",
      "episode: 457   reward: -80\n",
      "episode: 458   reward: -84\n",
      "episode: 459   reward: -90\n",
      "episode: 460   reward: -93\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec52dc7f6a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# gradient = grad[r + gamma * V(s_) - V(s)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# true_gradient = grad[logPi(s,a) * td_error]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ec52dc7f6a24>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, s, r, s_)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mv_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         td_error, _ = self.sess.run([self.td_error, self.train_op],\n\u001b[0;32m--> 122\u001b[0;31m                                           {self.s: s, self.v_: v_, self.r: r})\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actor-Critic with continuous action using TD-error as the Advantage, Reinforcement Learning.\n",
    "The Pendulum example (based on https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb)\n",
    "Cannot converge!!! oscillate!!!\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "tensorflow r1.3\n",
    "gym 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, action_bound, lr=0.0001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.float32, None, name=\"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, name=\"td_error\")  # TD_error\n",
    "\n",
    "        l1 = tf.layers.dense(\n",
    "            inputs=self.s,\n",
    "            units=30,  # number of hidden units\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='l1'\n",
    "        )\n",
    "        \n",
    "        # average value for action\n",
    "        mu = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=1,  # number of hidden units\n",
    "            activation=tf.nn.tanh, # use tanh to scale output to [-1,1]\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='mu'\n",
    "        )\n",
    "        \n",
    "        # deviation value for action\n",
    "        sigma = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=1,  # output units\n",
    "            activation=tf.nn.softplus,  # get action probabilities\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(1.),  # biases\n",
    "            name='sigma'\n",
    "        )\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        # self.e = epsilon = tf.train.exponential_decay(2., global_step, 1000, 0.9)\n",
    "        self.mu, self.sigma = tf.squeeze(mu*2), tf.squeeze(sigma+0.1)\n",
    "        self.normal_dist = tf.distributions.Normal(self.mu, self.sigma)\n",
    "        \n",
    "        # key: sample from the normal districution (mu,sigma) as the action to be taken\n",
    "        self.action = tf.clip_by_value(self.normal_dist.sample(1), action_bound[0], action_bound[1]) # ensure no action value is invalid\n",
    "\n",
    "        with tf.name_scope('exp_v'):\n",
    "            log_prob = self.normal_dist.log_prob(self.a)  # loss without advantage\n",
    "            self.exp_v = log_prob * self.td_error  # advantage (TD_error) guided loss\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.exp_v += 0.01*self.normal_dist.entropy()\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)    # min(v) = max(-v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.action, {self.s: s})  # get probabilities for all actions\n",
    "\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "            self.v_ = tf.placeholder(tf.float32, [1, 1], name=\"v_next\")\n",
    "            self.r = tf.placeholder(tf.float32, name='r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=30,  # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = tf.reduce_mean(self.r + GAMMA * self.v_ - self.v)\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 1000\n",
    "MAX_EP_STEPS = 200\n",
    "DISPLAY_REWARD_THRESHOLD = 100  # renders environment if total episode reward is greater then this threshold\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = env.action_space.high\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_S, lr=LR_A, action_bound=[-A_BOUND, A_BOUND])\n",
    "critic = Critic(sess, n_features=N_S, lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    ep_rs = []\n",
    "    while True:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        a = actor.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "        r /= 10\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "        ep_rs.append(r)\n",
    "        if t > MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(ep_rs)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0   reward: -2388\n",
      "episode: 1   reward: -4088\n",
      "episode: 2   reward: -5615\n",
      "episode: 3   reward: -7007\n",
      "episode: 4   reward: -8263\n",
      "episode: 5   reward: -7945\n",
      "episode: 6   reward: -9108\n",
      "episode: 7   reward: -10111\n",
      "episode: 8   reward: -9102\n",
      "episode: 9   reward: -10125\n",
      "episode: 10   reward: -9115\n",
      "episode: 11   reward: -8206\n",
      "episode: 12   reward: -9339\n",
      "episode: 13   reward: -10362\n",
      "episode: 14   reward: -9328\n",
      "episode: 15   reward: -9442\n",
      "episode: 16   reward: -10420\n",
      "episode: 17   reward: -11325\n",
      "episode: 18   reward: -12144\n",
      "episode: 19   reward: -12884\n",
      "episode: 20   reward: -13537\n",
      "episode: 21   reward: -13160\n",
      "episode: 22   reward: -13787\n",
      "episode: 23   reward: -14355\n",
      "episode: 24   reward: -14867\n",
      "episode: 25   reward: -15299\n",
      "episode: 26   reward: -15674\n",
      "episode: 27   reward: -16017\n",
      "episode: 28   reward: -14417\n",
      "episode: 29   reward: -14927\n",
      "episode: 30   reward: -13436\n",
      "episode: 31   reward: -13979\n",
      "episode: 32   reward: -14498\n",
      "episode: 33   reward: -13049\n",
      "episode: 34   reward: -13631\n",
      "episode: 35   reward: -14212\n",
      "episode: 36   reward: -14716\n",
      "episode: 37   reward: -14898\n",
      "episode: 38   reward: -15086\n",
      "episode: 39   reward: -15510\n",
      "episode: 40   reward: -15892\n",
      "episode: 41   reward: -16253\n",
      "episode: 42   reward: -16551\n",
      "episode: 43   reward: -16815\n",
      "episode: 44   reward: -16882\n",
      "episode: 45   reward: -16947\n",
      "episode: 46   reward: -17191\n",
      "episode: 47   reward: -17309\n",
      "episode: 48   reward: -17431\n",
      "episode: 49   reward: -17611\n",
      "episode: 50   reward: -17759\n",
      "episode: 51   reward: -15984\n",
      "episode: 52   reward: -14387\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a33249476243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# gradient = grad[r + gamma * V(s_) - V(s)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# true_gradient = grad[logPi(s,a) * td_error]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a33249476243>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, s, r, s_)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mv_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         td_error, _ = self.sess.run([self.td_error, self.train_op],\n\u001b[1;32m    123\u001b[0m                                           {self.s: s, self.v_: v_, self.r: r})\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actor-Critic with continuous action using TD-error as the Advantage, Reinforcement Learning.\n",
    "The Pendulum example (based on https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb)\n",
    "Cannot converge!!! oscillate!!!\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "tensorflow r1.3\n",
    "gym 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "tf.reset_default_graph() # reset\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, action_bound, lr=0.0001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.float32, None, name=\"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, name=\"td_error\")  # TD_error\n",
    "\n",
    "        l1 = tf.layers.dense(\n",
    "            inputs=self.s,\n",
    "            units=30,  # number of hidden units\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='l1'\n",
    "        )\n",
    "        \n",
    "        # average value for action\n",
    "        mu = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=N_A,  # number of hidden units\n",
    "            activation=tf.nn.tanh, # use tanh to scale output to [-1,1]\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='mu'\n",
    "        )\n",
    "        \n",
    "        # deviation value for action\n",
    "        sigma = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=N_A,  # output units\n",
    "            activation=tf.nn.softplus,  # get action probabilities\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(1.),  # biases\n",
    "            name='sigma'\n",
    "        )\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        # self.e = epsilon = tf.train.exponential_decay(2., global_step, 1000, 0.9)\n",
    "        self.mu, self.sigma = tf.squeeze(mu*2), tf.squeeze(sigma+0.1)\n",
    "        self.normal_dist = tf.distributions.Normal(self.mu, self.sigma)\n",
    "        \n",
    "        # key: sample from the normal districution (mu,sigma) as the action to be taken\n",
    "        self.action = tf.clip_by_value(self.normal_dist.sample(1), np.reshape(action_bound[0],(1,4)), np.reshape(action_bound[1],(1,4))) # ensure no action value is invalid\n",
    "\n",
    "        with tf.name_scope('exp_v'):\n",
    "            log_prob = self.normal_dist.log_prob(self.a)  # loss without advantage\n",
    "            self.exp_v = log_prob * self.td_error  # advantage (TD_error) guided loss\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.exp_v += 0.01*self.normal_dist.entropy()\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)    # min(v) = max(-v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.action, {self.s: s})  # get probabilities for all actions\n",
    "\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "            self.v_ = tf.placeholder(tf.float32, [1, 1], name=\"v_next\")\n",
    "            self.r = tf.placeholder(tf.float32, name='r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=30,  # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = tf.reduce_mean(self.r + GAMMA * self.v_ - self.v)\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 1000\n",
    "MAX_EP_STEPS = 200\n",
    "DISPLAY_REWARD_THRESHOLD = 100  # renders environment if total episode reward is greater then this threshold\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A=env.action_space.shape[0]\n",
    "A_BOUND = env.action_space.high\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_S, lr=LR_A, action_bound=[-A_BOUND, A_BOUND])\n",
    "critic = Critic(sess, n_features=N_S, lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    ep_rs = []\n",
    "    while True:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        a = actor.choose_action(s)\n",
    "        a=a[0]\n",
    "        \n",
    "        s_, r, done, info = env.step(a)\n",
    "        r /= 10\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "        ep_rs.append(r)\n",
    "        if t > MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(ep_rs)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4]\n"
     ]
    }
   ],
   "source": [
    "a=[[1,2,3],[2,3,4],[3,4,5]]\n",
    "a=np.array(a)\n",
    "print(a[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size,a_size,h_size):\n",
    "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "        hidden = slim.fully_connected(self.state_in,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden,a_size,activation_fn=tf.nn.softmax,biases_initializer=None)\n",
    "        self.chosen_action = tf.argmax(self.output,1)\n",
    "\n",
    "        #The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "        #to compute the loss, and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx,var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  1.47445947e-02,  -4.52420302e-03,   1.28968270e-03,\n",
      "         -4.42169607e-03,  -1.67561751e-02,   2.91425060e-03,\n",
      "         -1.94829277e-04,  -1.38024706e-03],\n",
      "       [  5.01107387e-02,  -2.87446320e-01,   2.72752834e-03,\n",
      "          2.07848474e-01,  -1.00883707e-01,   1.85157761e-01,\n",
      "          9.15826485e-03,  -8.31003767e-03],\n",
      "       [ -7.28943944e-03,   2.20602304e-02,  -6.00731524e-04,\n",
      "         -1.13972705e-02,   6.99976319e-03,  -1.42100379e-02,\n",
      "         -5.02189621e-04,   5.76587569e-04],\n",
      "       [ -8.23226497e-02,   4.17846978e-01,  -4.49230010e-03,\n",
      "         -2.94986606e-01,   1.53697044e-01,  -2.69154996e-01,\n",
      "         -1.29977651e-02,   1.26604009e-02]], dtype=float32), array([[-0.05834411,  0.05834411],\n",
      "       [-0.17819208,  0.17819202],\n",
      "       [-0.0459293 ,  0.04592931],\n",
      "       [-0.09173708,  0.09173704],\n",
      "       [-0.01741329,  0.0174133 ],\n",
      "       [-0.37915605,  0.37915593],\n",
      "       [-0.09761829,  0.09761826],\n",
      "       [-0.07454276,  0.07454277]], dtype=float32)]\n",
      "15.0\n",
      "[array([[-0.01700732, -0.0058874 , -0.00050264,  0.01480045,  0.02510748,\n",
      "         0.        ,  0.00024031,  0.00152661],\n",
      "       [-0.02511643, -0.0118261 , -0.00063591,  0.02266728,  0.0396336 ,\n",
      "         0.        ,  0.00028823,  0.00193138],\n",
      "       [ 0.02650543,  0.01050979,  0.00072631, -0.02421867, -0.03928589,\n",
      "         0.        , -0.00037043, -0.00220594],\n",
      "       [ 0.03216987, -0.00263826,  0.00173435, -0.00700415, -0.05405094,\n",
      "         0.        , -0.00028342, -0.00526757]], dtype=float32), array([[ 0.02525219, -0.02525218],\n",
      "       [ 0.00093457, -0.00093457],\n",
      "       [ 0.0238435 , -0.02384351],\n",
      "       [ 0.02982566, -0.02982565],\n",
      "       [-0.00057313,  0.00057312],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.00029334, -0.00029334],\n",
      "       [ 0.0203841 , -0.02038409]], dtype=float32)]\n",
      "[array([[  1.00911744e-02,   1.23976776e-02,  -3.08555900e-05,\n",
      "         -8.97052977e-03,  -2.12895069e-02,  -7.98592903e-03,\n",
      "         -3.95261886e-04,  -1.67731172e-03],\n",
      "       [  5.12468219e-02,  -2.10486561e-01,   2.74689216e-03,\n",
      "          1.52300790e-01,  -1.04367651e-01,   1.35584354e-01,\n",
      "          6.71071606e-03,  -8.51802155e-03],\n",
      "       [  7.28487689e-03,   5.61653115e-02,  -4.42803721e-05,\n",
      "         -4.06392738e-02,  -2.10930705e-02,  -3.61787342e-02,\n",
      "         -1.79065834e-03,  -1.21086091e-03],\n",
      "       [ -7.50868842e-02,   3.60964984e-01,  -3.91989248e-03,\n",
      "         -2.61181772e-01,   1.66749150e-01,  -2.32514620e-01,\n",
      "         -1.15082581e-02,   1.24806119e-02]], dtype=float32), array([[-0.05703365,  0.05703365],\n",
      "       [-0.11520656,  0.11520656],\n",
      "       [-0.03911433,  0.03911433],\n",
      "       [-0.07111596,  0.07111595],\n",
      "       [-0.01678544,  0.01678543],\n",
      "       [-0.31852227,  0.31852227],\n",
      "       [-0.05617269,  0.05617269],\n",
      "       [-0.07794564,  0.07794564]], dtype=float32)]\n",
      "[array([[ -1.60412155e-02,   0.00000000e+00,  -8.77885730e-04,\n",
      "          0.00000000e+00,   2.36115418e-02,   4.61617624e-03,\n",
      "          2.28476682e-04,   2.66629783e-03],\n",
      "       [ -2.48569489e-01,   0.00000000e+00,  -1.36034312e-02,\n",
      "          0.00000000e+00,   5.30648768e-01,  -1.53239276e-02,\n",
      "         -7.58454611e-04,   4.13160920e-02],\n",
      "       [ -2.12960579e-02,   0.00000000e+00,  -1.16547453e-03,\n",
      "          0.00000000e+00,   6.89554494e-03,   1.90168656e-02,\n",
      "          9.41235863e-04,   3.53974523e-03],\n",
      "       [  4.27326769e-01,   0.00000000e+00,   2.33862642e-02,\n",
      "          0.00000000e+00,  -8.27684700e-01,  -1.82382092e-02,\n",
      "         -9.02696396e-04,  -7.10283294e-02]], dtype=float32), array([[ 0.27284414, -0.27284414],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.24516243, -0.24516246],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.11536846, -0.11536845],\n",
      "       [ 0.01247062, -0.01247062],\n",
      "       [ 0.00939612, -0.00939612],\n",
      "       [ 0.44705373, -0.44705367]], dtype=float32)]\n",
      "[array([[  1.68122072e-03,   2.01380551e-02,   3.16561229e-04,\n",
      "         -5.36652096e-03,  -5.01215551e-03,  -9.55163129e-03,\n",
      "         -8.14077852e-04,  -8.13920808e-04],\n",
      "       [  1.52127743e-01,   1.91484183e-01,   9.03556962e-03,\n",
      "         -1.15199342e-01,  -2.99671322e-01,  -1.10503405e-01,\n",
      "         -7.76268542e-03,  -2.72926167e-02],\n",
      "       [ -4.49159741e-02,   5.62275052e-02,  -1.76694756e-03,\n",
      "         -1.32017760e-02,   1.00161515e-01,  -2.23035160e-02,\n",
      "         -7.67242163e-04,   5.29121934e-03],\n",
      "       [ -3.05812478e-01,  -1.68904439e-01,  -1.57635473e-02,\n",
      "          1.59732968e-01,   6.23806357e-01,   1.35647178e-01,\n",
      "          1.01883933e-02,   4.66353893e-02]], dtype=float32), array([[-0.15973872,  0.15973872],\n",
      "       [ 0.13657895, -0.13657895],\n",
      "       [-0.1808985 ,  0.18089849],\n",
      "       [ 0.06514499, -0.06514499],\n",
      "       [-0.05485224,  0.05485225],\n",
      "       [ 0.18849784, -0.18849784],\n",
      "       [ 0.08887538, -0.08887538],\n",
      "       [-0.2750839 ,  0.2750839 ]], dtype=float32)]\n",
      "[array([[  5.44066075e-03,   2.04947428e-04,   9.95079099e-05,\n",
      "         -3.18608410e-03,  -1.77609059e-03,   1.25646649e-04,\n",
      "         -1.20079487e-04,  -3.02224391e-04],\n",
      "       [  5.39164059e-02,   4.40012217e-01,   2.69230665e-03,\n",
      "         -3.30725968e-01,  -1.12518087e-01,  -2.88719654e-01,\n",
      "         -1.38556939e-02,  -8.17704573e-03],\n",
      "       [  7.74260936e-03,  -5.04991375e-02,   2.85026588e-04,\n",
      "          1.46825677e-02,  -7.08288997e-02,   1.74747929e-02,\n",
      "          1.99950626e-03,  -8.65679758e-04],\n",
      "       [ -5.41262627e-02,  -7.77914107e-01,  -3.28854634e-03,\n",
      "          5.24112999e-01,  -1.16148107e-02,   4.70247865e-01,\n",
      "          2.56035514e-02,   9.98793822e-03]], dtype=float32), array([[-0.04590971,  0.0459097 ],\n",
      "       [ 0.29996186, -0.29996186],\n",
      "       [-0.02733041,  0.0273304 ],\n",
      "       [ 0.18792662, -0.18792662],\n",
      "       [-0.0401915 ,  0.04019149],\n",
      "       [ 0.62324846, -0.6232484 ],\n",
      "       [ 0.11702767, -0.11702767],\n",
      "       [-0.06718878,  0.06718878]], dtype=float32)]\n",
      "[array([[ -1.45757161e-02,   1.03273734e-01,  -2.33749073e-04,\n",
      "         -5.70339337e-02,   2.70257555e-02,  -4.61098850e-02,\n",
      "         -5.91833319e-04,   5.08340774e-03],\n",
      "       [  1.90876760e-02,   5.32355607e-01,   8.73355020e-05,\n",
      "         -3.96611184e-01,  -1.08695805e-01,  -3.53062868e-01,\n",
      "         -5.25471708e-03,  -6.71195146e-03],\n",
      "       [  1.81283988e-02,  -1.59213409e-01,   2.00198294e-04,\n",
      "          8.22474062e-02,  -2.35215612e-02,   6.86712861e-02,\n",
      "          7.91126280e-04,  -6.71843812e-03],\n",
      "       [ -9.65655223e-03,  -1.06986010e+00,  -3.04924004e-04,\n",
      "          7.05501378e-01,   1.89521536e-01,   6.28699422e-01,\n",
      "          8.55332613e-03,   9.94662405e-04]], dtype=float32), array([[-0.00465049,  0.00465049],\n",
      "       [ 0.40659687, -0.40659687],\n",
      "       [-0.01062193,  0.01062193],\n",
      "       [ 0.20996933, -0.20996934],\n",
      "       [-0.03700145,  0.03700146],\n",
      "       [ 0.86806142, -0.86806154],\n",
      "       [ 0.11610846, -0.11610847],\n",
      "       [-0.0416952 ,  0.0416952 ]], dtype=float32)]\n",
      "[array([[  2.52096285e-03,   0.00000000e+00,   5.36645639e-05,\n",
      "         -6.28347229e-03,  -5.35606733e-03,   0.00000000e+00,\n",
      "          0.00000000e+00,  -5.23022609e-04],\n",
      "       [ -2.06114233e-01,   0.00000000e+00,  -4.38779453e-03,\n",
      "          1.11934571e-02,   4.37912911e-01,   0.00000000e+00,\n",
      "          0.00000000e+00,   4.27625999e-02],\n",
      "       [  4.07016315e-02,   0.00000000e+00,   8.66460323e-04,\n",
      "         -6.54619979e-03,  -8.64752159e-02,   0.00000000e+00,\n",
      "          0.00000000e+00,  -8.44438095e-03],\n",
      "       [  3.29457134e-01,   0.00000000e+00,   7.01353978e-03,\n",
      "         -1.50389737e-02,  -6.99968934e-01,   0.00000000e+00,\n",
      "          0.00000000e+00,  -6.83526024e-02]], dtype=float32), array([[ 0.18098511, -0.18098512],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.19900163, -0.1990016 ],\n",
      "       [ 0.00081545, -0.00081545],\n",
      "       [ 0.10790954, -0.10790954],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.35058579, -0.35058573]], dtype=float32)]\n",
      "[array([[-0.00757326,  0.02654496,  0.        , -0.02799086,  0.02637393,\n",
      "        -0.01429669, -0.00045088,  0.00157123],\n",
      "       [ 0.02168107,  0.23705931,  0.        , -0.16196059, -0.04732399,\n",
      "        -0.14859092, -0.00280955, -0.00449818],\n",
      "       [ 0.01581502, -0.03645479,  0.        ,  0.04223237, -0.05277706,\n",
      "         0.01714347,  0.00071377, -0.00328115],\n",
      "       [-0.00726016, -0.3461628 ,  0.        ,  0.25842649, -0.01302741,\n",
      "         0.21297194,  0.00472479,  0.00150627]], dtype=float32), array([[-0.00927641,  0.00927641],\n",
      "       [ 0.15186442, -0.15186441],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.05585956, -0.05585955],\n",
      "       [-0.02859575,  0.02859575],\n",
      "       [ 0.31555939, -0.31555939],\n",
      "       [ 0.05280688, -0.05280688],\n",
      "       [-0.02401636,  0.02401636]], dtype=float32)]\n",
      "[array([[  5.81815187e-03,   5.16153406e-03,   1.86823076e-04,\n",
      "         -3.55436513e-03,  -1.23613216e-02,   0.00000000e+00,\n",
      "          2.72114739e-05,  -1.82075170e-03],\n",
      "       [ -4.50298898e-02,   1.22534009e-02,  -8.09115008e-04,\n",
      "         -8.43800604e-03,   9.56711024e-02,   0.00000000e+00,\n",
      "          2.00348208e-04,   7.88553897e-03],\n",
      "       [  1.84337758e-02,   1.85395535e-02,   6.18588063e-04,\n",
      "         -1.27668139e-02,  -3.91646363e-02,   0.00000000e+00,\n",
      "         -4.65552846e-04,  -6.02864753e-03],\n",
      "       [  1.11276686e-01,   1.04752611e-02,   2.49665137e-03,\n",
      "         -7.21353386e-03,  -2.36419916e-01,   0.00000000e+00,\n",
      "         -1.01726793e-03,  -2.43320093e-02]], dtype=float32), array([[ 0.04783386, -0.04783385],\n",
      "       [ 0.01315613, -0.01315613],\n",
      "       [ 0.09446361, -0.0944636 ],\n",
      "       [ 0.00128885, -0.00128885],\n",
      "       [ 0.0221765 , -0.02217649],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.01120508, -0.01120508],\n",
      "       [ 0.11398678, -0.11398672]], dtype=float32)]\n",
      "[array([[  4.61939648e-02,  -4.09728587e-02,   8.89175048e-04,\n",
      "          7.64460396e-03,  -9.48890969e-02,   2.03099493e-02,\n",
      "          3.38856771e-04,  -7.66268605e-03],\n",
      "       [  3.27518374e-01,   1.34002596e-01,   6.97701471e-03,\n",
      "         -9.35485885e-02,  -6.84307516e-01,  -8.16121548e-02,\n",
      "         -1.81685260e-03,  -6.78621233e-02],\n",
      "       [ -5.71765415e-02,   3.20960060e-02,  -9.74925933e-04,\n",
      "          4.32444736e-04,   1.22490451e-01,  -7.47503713e-03,\n",
      "         -2.63119378e-04,   9.77388490e-03],\n",
      "       [ -4.93522942e-01,  -1.99793041e-01,  -1.05069904e-02,\n",
      "          1.60774037e-01,   1.08007252e+00,   1.21995814e-01,\n",
      "          2.57974910e-03,   1.00890972e-01]], dtype=float32), array([[-0.3184793 ,  0.31847927],\n",
      "       [ 0.08747227, -0.08747227],\n",
      "       [-0.27321833,  0.27321833],\n",
      "       [ 0.06443289, -0.0644329 ],\n",
      "       [-0.13138887,  0.13138887],\n",
      "       [ 0.1524514 , -0.1524514 ],\n",
      "       [ 0.06290067, -0.06290068],\n",
      "       [-0.50577557,  0.50577557]], dtype=float32)]\n",
      "[array([[ -4.60595498e-03,   3.71548324e-03,  -2.38500197e-05,\n",
      "         -2.44559068e-03,   1.02431448e-02,   0.00000000e+00,\n",
      "          2.55657633e-05,   6.35361357e-04],\n",
      "       [ -2.18787178e-01,   2.05009989e-03,  -2.01362930e-03,\n",
      "         -1.34940865e-03,   4.86558944e-01,   0.00000000e+00,\n",
      "          1.41064738e-05,   5.36416285e-02],\n",
      "       [  4.52357754e-02,   2.01552287e-02,   5.20234229e-04,\n",
      "         -1.32664945e-02,  -1.00599475e-01,   0.00000000e+00,\n",
      "          1.38685529e-04,  -1.38585912e-02],\n",
      "       [  3.76295507e-01,   9.99498740e-03,   3.53153958e-03,\n",
      "         -6.57886127e-03,  -8.36840451e-01,   0.00000000e+00,\n",
      "          6.87742213e-05,  -9.40775946e-02]], dtype=float32), array([[ 0.20422114, -0.20422113],\n",
      "       [ 0.01076544, -0.01076544],\n",
      "       [ 0.23908588, -0.23908585],\n",
      "       [ 0.00656889, -0.00656889],\n",
      "       [ 0.11714308, -0.11714306],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.00573168, -0.00573168],\n",
      "       [ 0.41616967, -0.41616964]], dtype=float32)]\n",
      "[array([[  1.34646427e-02,   2.09826045e-02,   7.17797739e-05,\n",
      "         -1.38110872e-02,   0.00000000e+00,  -1.83658320e-02,\n",
      "          1.44378122e-04,  -3.31819244e-03],\n",
      "       [  2.54559331e-03,   3.64940278e-02,   1.39013473e-05,\n",
      "         -2.40209475e-02,   0.00000000e+00,  -2.24367511e-02,\n",
      "          2.51108431e-04,  -6.27329573e-04],\n",
      "       [  1.79603882e-02,  -2.18781214e-02,   8.49422795e-05,\n",
      "          1.44005297e-02,   0.00000000e+00,   3.38466140e-03,\n",
      "         -1.50540785e-04,  -4.42611240e-03],\n",
      "       [ -9.33539867e-03,  -7.22977072e-02,  -7.11882603e-05,\n",
      "          4.75874506e-02,   0.00000000e+00,   4.39931639e-02,\n",
      "         -4.97467525e-04,   2.30059167e-03]], dtype=float32), array([[-0.0221621 ,  0.0221621 ],\n",
      "       [ 0.02930653, -0.02930655],\n",
      "       [-0.00137009,  0.00137009],\n",
      "       [ 0.00592531, -0.00592532],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.0642923 , -0.06429231],\n",
      "       [-0.00033183,  0.00033182],\n",
      "       [-0.01129684,  0.01129684]], dtype=float32)]\n",
      "[array([[  1.90426987e-02,   5.73294312e-02,   1.18469761e-04,\n",
      "         -4.23134528e-02,  -6.15610555e-02,  -4.31395359e-02,\n",
      "          2.79348809e-04,  -4.74560820e-03],\n",
      "       [  1.34477049e-01,   4.42733556e-01,   8.94229917e-04,\n",
      "         -3.40499014e-01,  -2.88449645e-01,  -2.53700525e-01,\n",
      "          2.72934185e-03,  -2.95080617e-02],\n",
      "       [ -1.53079564e-02,  -9.90648568e-02,  -1.60561234e-04,\n",
      "          5.53985350e-02,   5.74210696e-02,   7.62147307e-02,\n",
      "         -5.14373649e-04,   5.16865309e-03],\n",
      "       [ -1.41524076e-01,  -7.54272580e-01,  -1.38204813e-03,\n",
      "          4.92871135e-01,   2.99039096e-01,   4.58078742e-01,\n",
      "         -4.65395255e-03,   3.66036892e-02]], dtype=float32), array([[-0.10738573,  0.10738572],\n",
      "       [ 0.30718493, -0.30718493],\n",
      "       [-0.08296569,  0.08296568],\n",
      "       [ 0.10469167, -0.10469165],\n",
      "       [-0.05943526,  0.05943524],\n",
      "       [ 0.66021967, -0.66021967],\n",
      "       [ 0.07738057, -0.07738055],\n",
      "       [-0.16105436,  0.16105431]], dtype=float32)]\n",
      "[array([[  2.66313343e-03,  -2.09546704e-02,   0.00000000e+00,\n",
      "          1.05808303e-02,  -5.92252053e-03,   1.22420564e-02,\n",
      "         -1.44186648e-04,   0.00000000e+00],\n",
      "       [  4.10428643e-03,  -6.45098925e-01,   0.00000000e+00,\n",
      "          4.19664562e-01,  -9.12748836e-03,   3.76877159e-01,\n",
      "         -4.43884963e-03,   0.00000000e+00],\n",
      "       [  2.84305960e-03,   7.10855201e-02,   0.00000000e+00,\n",
      "         -5.02185076e-02,  -6.32265676e-03,  -4.15293016e-02,\n",
      "          4.89130907e-04,   0.00000000e+00],\n",
      "       [  1.38426060e-03,   1.05100381e+00,   0.00000000e+00,\n",
      "         -6.93457067e-01,  -3.07844579e-03,  -6.14013255e-01,\n",
      "          7.23183295e-03,   0.00000000e+00]], dtype=float32), array([[-0.00391568,  0.00391568],\n",
      "       [-0.44100797,  0.44100791],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [-0.21340959,  0.21340957],\n",
      "       [-0.001414  ,  0.001414  ],\n",
      "       [-0.85308349,  0.85308343],\n",
      "       [-0.15045336,  0.15045333],\n",
      "       [ 0.        ,  0.        ]], dtype=float32)]\n",
      "[array([[  0.00000000e+00,   1.78022217e-02,   0.00000000e+00,\n",
      "         -1.17177088e-02,   0.00000000e+00,  -4.74731438e-03,\n",
      "          1.22494996e-04,   0.00000000e+00],\n",
      "       [  0.00000000e+00,  -1.81060389e-01,   0.00000000e+00,\n",
      "          1.19176880e-01,   0.00000000e+00,   1.07402116e-01,\n",
      "         -1.24585582e-03,   0.00000000e+00],\n",
      "       [  0.00000000e+00,  -4.12219577e-02,   0.00000000e+00,\n",
      "          2.71329563e-02,   0.00000000e+00,   1.28998561e-02,\n",
      "         -2.83643429e-04,   0.00000000e+00],\n",
      "       [  0.00000000e+00,   2.68255323e-01,   0.00000000e+00,\n",
      "         -1.76569998e-01,   0.00000000e+00,  -1.53113529e-01,\n",
      "          1.84583350e-03,   0.00000000e+00]], dtype=float32), array([[ 0.        ,  0.        ],\n",
      "       [-0.13745931,  0.13745931],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [-0.0859028 ,  0.08590278],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [-0.20378725,  0.20378721],\n",
      "       [-0.06255056,  0.06255054],\n",
      "       [ 0.        ,  0.        ]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.03512461,  0.07239474,  0.0010833 , -0.04675518, -0.05658679,\n",
      "        -0.03076056,  0.00094203, -0.00833462],\n",
      "       [ 0.129859  , -0.11247241,  0.00400507,  0.07263885, -0.2965177 ,\n",
      "         0.06178579, -0.00146354, -0.03081385],\n",
      "       [-0.06253977, -0.12197431, -0.00192883,  0.07877547,  0.09852508,\n",
      "         0.0497652 , -0.00158717,  0.01483988],\n",
      "       [-0.24783522, -0.05423759, -0.00764365,  0.03502857,  0.51698542,\n",
      "         0.01231128, -0.00070575,  0.05880807]], dtype=float32), array([[-0.14623745,  0.14623745],\n",
      "       [-0.03937835,  0.03937832],\n",
      "       [-0.15786871,  0.1578687 ],\n",
      "       [-0.01037876,  0.01037876],\n",
      "       [-0.05048144,  0.05048144],\n",
      "       [ 0.00263484, -0.0026349 ],\n",
      "       [-0.09595071,  0.09595069],\n",
      "       [-0.23470207,  0.23470207]], dtype=float32)]\n",
      "[array([[ -4.19094674e-02,   2.23931715e-01,  -5.61202352e-04,\n",
      "         -1.39108151e-01,   7.01168925e-02,  -1.40848875e-01,\n",
      "          3.14829755e-03,   6.79315766e-03],\n",
      "       [  2.97372401e-01,   1.10646033e+00,   9.34432261e-03,\n",
      "         -7.46967018e-01,  -6.84081852e-01,  -6.24548674e-01,\n",
      "          1.40928533e-02,  -7.28288665e-02],\n",
      "       [ -6.31233528e-02,  -3.33405323e-02,  -2.67988094e-03,\n",
      "         -6.51162351e-03,   1.86001509e-01,   3.46228480e-04,\n",
      "         -1.65659876e-04,   1.94048267e-02],\n",
      "       [ -5.42744935e-01,  -1.46322286e+00,  -1.71149056e-02,\n",
      "          9.24432516e-01,   1.20710731e+00,   8.15668643e-01,\n",
      "         -1.97985582e-02,   1.27659127e-01]], dtype=float32), array([[-0.25505549,  0.25505552],\n",
      "       [ 0.8097381 , -0.80973798],\n",
      "       [-0.35991156,  0.35991159],\n",
      "       [ 0.21956645, -0.21956643],\n",
      "       [-0.18877539,  0.18877541],\n",
      "       [ 1.26349962, -1.2634995 ],\n",
      "       [ 0.24141878, -0.24141875],\n",
      "       [-0.60330504,  0.6033051 ]], dtype=float32)]\n",
      "[array([[  1.80294104e-02,   2.84024570e-02,   3.26437905e-04,\n",
      "         -1.83433518e-02,  -4.20496203e-02,  -1.51441526e-02,\n",
      "          2.58838816e-04,  -3.36014456e-03],\n",
      "       [ -1.58432439e-01,   2.92619050e-01,  -4.94242832e-03,\n",
      "         -1.88984096e-01,   3.55345637e-01,  -1.57507330e-01,\n",
      "          3.67671950e-03,   3.88433449e-02],\n",
      "       [ -3.03807221e-02,  -4.21247110e-02,  -5.69073774e-04,\n",
      "          2.72056852e-02,   7.24503696e-02,   2.10624263e-02,\n",
      "         -3.46875167e-04,   5.68701932e-03],\n",
      "       [  2.14791521e-01,  -4.45784926e-01,   6.56955410e-03,\n",
      "          2.87904233e-01,  -4.89449441e-01,   2.59132862e-01,\n",
      "         -5.91701083e-03,  -5.05970009e-02]], dtype=float32), array([[ 0.1321483 , -0.13214827],\n",
      "       [ 0.19695449, -0.19695449],\n",
      "       [ 0.10082228, -0.10082226],\n",
      "       [ 0.06979656, -0.06979655],\n",
      "       [ 0.11546525, -0.11546524],\n",
      "       [ 0.36764568, -0.36764574],\n",
      "       [ 0.05849695, -0.05849696],\n",
      "       [ 0.28087831, -0.28087831]], dtype=float32)]\n",
      "[array([[  0.00000000e+00,  -3.81020494e-02,   0.00000000e+00,\n",
      "          2.46077050e-02,   1.69752929e-02,   2.17328779e-02,\n",
      "         -4.95798071e-04,   1.79433264e-03],\n",
      "       [  0.00000000e+00,  -2.14609027e-01,   0.00000000e+00,\n",
      "          1.38602376e-01,  -2.13569589e-02,   1.22409977e-01,\n",
      "         -2.79257237e-03,  -2.25748611e-03],\n",
      "       [  0.00000000e+00,   4.63359319e-02,   0.00000000e+00,\n",
      "         -2.99254470e-02,  -1.74253397e-02,  -2.64293626e-02,\n",
      "          6.02940330e-04,  -1.84190366e-03],\n",
      "       [  0.00000000e+00,   4.01018620e-01,   0.00000000e+00,\n",
      "         -2.58992523e-01,   9.24483465e-04,  -2.28735372e-01,\n",
      "          5.21820271e-03,   9.77203090e-05]], dtype=float32), array([[ 0.        ,  0.        ],\n",
      "       [-0.16746335,  0.16746332],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [-0.0757556 ,  0.07575557],\n",
      "       [-0.01417926,  0.01417926],\n",
      "       [-0.31836957,  0.31836951],\n",
      "       [-0.02761234,  0.02761232],\n",
      "       [-0.011233  ,  0.011233  ]], dtype=float32)]\n",
      "[array([[ -1.32778962e-03,   3.79506201e-02,  -6.24171633e-04,\n",
      "         -1.21523999e-02,   4.54311669e-02,  -3.31959464e-02,\n",
      "          7.51284184e-04,   4.80219116e-03],\n",
      "       [  6.72429278e-02,   2.36927226e-01,   2.53750640e-03,\n",
      "         -1.61541328e-01,  -1.84696466e-01,  -1.25855953e-01,\n",
      "          2.88362568e-03,  -1.95229035e-02],\n",
      "       [  6.73654489e-03,   2.72532962e-02,  -1.69206207e-04,\n",
      "         -1.40663758e-02,   1.23159401e-02,  -2.42467076e-02,\n",
      "          5.28497738e-04,   1.30182644e-03],\n",
      "       [ -7.40993395e-02,  -3.13915849e-01,  -2.83634872e-03,\n",
      "          2.20308334e-01,   2.06448227e-01,   1.76268756e-01,\n",
      "         -3.98359587e-03,   2.18221266e-02]], dtype=float32), array([[-0.05196278,  0.05196278],\n",
      "       [ 0.18716168, -0.18716168],\n",
      "       [-0.04465258,  0.04465258],\n",
      "       [ 0.07661319, -0.07661319],\n",
      "       [-0.06843257,  0.06843257],\n",
      "       [ 0.24635452, -0.24635452],\n",
      "       [ 0.05756652, -0.05756652],\n",
      "       [-0.12416936,  0.12416936]], dtype=float32)]\n",
      "[array([[  6.70942059e-03,   1.51106324e-02,   1.07128501e-04,\n",
      "         -9.79482289e-03,  -1.55467931e-02,  -8.30512308e-03,\n",
      "          3.61451675e-04,  -1.67145592e-03],\n",
      "       [  5.19229919e-02,  -1.05823509e-01,   8.86244292e-04,\n",
      "          6.78671300e-02,  -1.20313793e-01,   5.81628084e-02,\n",
      "         -2.53132870e-03,  -1.38274897e-02],\n",
      "       [ -1.47154089e-03,   1.69772245e-02,  -4.36397022e-06,\n",
      "         -9.17126611e-03,   3.40979383e-03,  -9.33103636e-03,\n",
      "          4.06100124e-04,   6.80881785e-05],\n",
      "       [ -6.61310777e-02,   2.20466986e-01,  -1.09703338e-03,\n",
      "         -1.37800351e-01,   1.53236181e-01,  -1.21173270e-01,\n",
      "          5.27363550e-03,   1.71162952e-02]], dtype=float32), array([[-0.04739855,  0.04739855],\n",
      "       [-0.07555114,  0.07555116],\n",
      "       [-0.02717713,  0.02717713],\n",
      "       [-0.05174162,  0.05174163],\n",
      "       [-0.0288091 ,  0.0288091 ],\n",
      "       [-0.15235642,  0.15235649],\n",
      "       [-0.01795212,  0.01795213],\n",
      "       [-0.08038362,  0.08038362]], dtype=float32)]\n",
      "[array([[  0.00000000e+00,  -2.66455580e-02,   0.00000000e+00,\n",
      "          1.66880414e-02,   1.93349719e-02,   1.46449562e-02,\n",
      "         -2.52233935e-04,   0.00000000e+00],\n",
      "       [  0.00000000e+00,   8.37783888e-03,   0.00000000e+00,\n",
      "         -5.24698105e-03,  -9.04859137e-03,  -4.60463390e-03,\n",
      "          2.01596413e-05,   0.00000000e+00],\n",
      "       [  0.00000000e+00,  -6.58990741e-02,   0.00000000e+00,\n",
      "          4.12724204e-02,   1.73267964e-02,   3.62195186e-02,\n",
      "         -1.23119261e-03,   0.00000000e+00],\n",
      "       [  0.00000000e+00,   4.24435735e-02,   0.00000000e+00,\n",
      "         -2.65823025e-02,  -2.49949396e-02,  -2.33279131e-02,\n",
      "          5.17385663e-04,   0.00000000e+00]], dtype=float32), array([[ 0.        ,  0.        ],\n",
      "       [-0.06055117,  0.06055117],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [-0.06124455,  0.06124456],\n",
      "       [-0.01027081,  0.01027082],\n",
      "       [ 0.00969583, -0.00969577],\n",
      "       [-0.0053813 ,  0.00538129],\n",
      "       [ 0.        ,  0.        ]], dtype=float32)]\n",
      "[array([[  7.06052221e-03,   0.00000000e+00,   1.76605317e-04,\n",
      "          1.06540527e-02,  -3.33270021e-02,   3.76342121e-03,\n",
      "         -6.88108266e-04,  -2.75546615e-03],\n",
      "       [  1.23723209e-01,   0.00000000e+00,   1.98400347e-03,\n",
      "         -9.89287393e-04,  -2.56180197e-01,  -4.81693679e-03,\n",
      "          1.53375659e-04,  -3.09544839e-02],\n",
      "       [ -3.78186256e-02,   0.00000000e+00,  -5.01544040e-04,\n",
      "         -1.19320620e-02,   7.98756257e-02,   8.15507118e-03,\n",
      "          1.36949937e-04,   7.82528054e-03],\n",
      "       [ -1.83057040e-01,   0.00000000e+00,  -3.01666511e-03,\n",
      "         -1.57920606e-02,   4.10709441e-01,   1.95393362e-03,\n",
      "          4.41811921e-04,   4.70661595e-02]], dtype=float32), array([[-0.10318898,  0.10318901],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [-0.10047005,  0.1004701 ],\n",
      "       [ 0.00069418, -0.00069418],\n",
      "       [-0.0539285 ,  0.05392852],\n",
      "       [ 0.01111816, -0.01111816],\n",
      "       [ 0.0113779 , -0.0113779 ],\n",
      "       [-0.20204237,  0.20204246]], dtype=float32)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3da434a76a69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0mgradBuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36marray_repr\u001b[0;34m(arr, max_line_width, precision, suppress_small)\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m         lst = array2string(arr, max_line_width, precision, suppress_small,\n\u001b[0;32m-> 1881\u001b[0;31m                            ', ', class_name + \"(\")\n\u001b[0m\u001b[1;32m   1882\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# show zero-length shape unless it is (0,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[], shape=%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mrepr_running\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mrepr_running\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         lst = _array2string(a, max_line_width, precision, suppress_small,\n\u001b[0;32m--> 523\u001b[0;31m                             separator, prefix, formatter=formatter)\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_array2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, formatter)\u001b[0m\n\u001b[1;32m    362\u001b[0m     lst = _formatArray(a, format_function, a.ndim, max_line_width,\n\u001b[1;32m    363\u001b[0m                        \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                        _summaryEdgeItems, summary_insert)[:-1]\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_formatArray\u001b[0;34m(a, format_function, rank, max_line_len, next_line_prefix, separator, edge_items, summary_insert)\u001b[0m\n\u001b[1;32m    591\u001b[0m             s += _formatArray(a[-i], format_function, rank-1, max_line_len,\n\u001b[1;32m    592\u001b[0m                               \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_items\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                               summary_insert)\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleading_items\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrailing_items\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_formatArray\u001b[0;34m(a, format_function, rank, max_line_len, next_line_prefix, separator, edge_items, summary_insert)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrailing_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extendLine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_line_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_line_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, strip_zeros)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_nc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_fmt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'+'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_nan_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
    "\n",
    "myAgent = agent(lr=1e-2,s_size=4,a_size=2,h_size=8) #Load the agent.\n",
    "\n",
    "total_episodes = 5000 #Set total number of episodes to train agent on.\n",
    "max_ep = 999\n",
    "update_frequency = 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    total_reward = []\n",
    "    total_lenght = []\n",
    "        \n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "        \n",
    "    while i < total_episodes:\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_ep):\n",
    "            #Probabilistically pick an action given our network outputs.\n",
    "            a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]})\n",
    "            \n",
    "            \n",
    "            a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "\n",
    "            a = np.argmax(a_dist == a)\n",
    "\n",
    "            s1,r,d,_ = env.step(a) #Get our reward for taking an action given a bandit.\n",
    "            ep_history.append([s,a,r,s1])\n",
    "            s = s1\n",
    "            running_reward += r\n",
    "            if d == True:\n",
    "                #Update the network.\n",
    "\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "                \n",
    "                    \n",
    "                feed_dict={myAgent.reward_holder:ep_history[:,2],\n",
    "                        myAgent.action_holder:ep_history[:,1],myAgent.state_in:np.vstack(ep_history[:,0])}\n",
    "                \n",
    "\n",
    "#                 print('tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1]:',(tf.range(0, tf.shape(myAgent.output)[0]) * tf.shape(myAgent.output)[1]).eval(feed_dict=feed_dict))\n",
    "#                 print('output.shape:',tf.shape(myAgent.output).eval(feed_dict=feed_dict))\n",
    "#                 print('action_holder:',ep_history[:,1])\n",
    "#                 eles=sess.run([myAgent.indexes,myAgent.responsible_outputs,myAgent.loss],feed_dict=feed_dict)\n",
    "#                 print('indexes:',eles[0])\n",
    "#                 print('reshape(output):',tf.reshape(myAgent.output, [-1]).eval(feed_dict=feed_dict))\n",
    "#                 print('responsible_outputs:',eles[1])\n",
    "#                 print('loss:',eles[2])\n",
    "#                 print('reward_holder:',ep_history[:,2])\n",
    "                \n",
    "                grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
    "        \n",
    "                for idx,grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    feed_dict= dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
    "                    for ix,grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                total_reward.append(running_reward)\n",
    "                total_lenght.append(j)\n",
    "                break\n",
    "\n",
    "        \n",
    "            #Update our running tally of scores.\n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(total_reward[-100:]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'range_1:0' shape=(8,) dtype=int32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(0,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
