{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tutorial\n",
    "\n",
    "[An intro to Advantage Actor Critic methods: let’s play Sonic the Hedgehog!\n",
    "](https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d)\n",
    "\n",
    "# Introducing Actor Critic\n",
    "\n",
    "The Actor Critic model is a better score function. Instead of waiting until the end of the episode as we do in Monte Carlo REINFORCE, we make an update at each step (TD Learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AC Algorithm for continuous action_space\n",
    "# Source: https://github.com/allanbreyes/bipedal-walker/blob/master/writeup.md\n",
    "\n",
    "# randomly initialize critic network Q and actor μ\n",
    "# initialize target network Q' and μ'\n",
    "# initialize replay buffer R\n",
    "# for episode = 1, M do:\n",
    "#   initialize a random process N for action exploration\n",
    "#   receive initial observation state s1\n",
    "#   for t = 1, T do:\n",
    "#     select action a_t = μ + N_t according to current policy\n",
    "#     execute action a_t and observe reward r_t and new state s_t+1\n",
    "#     store experience in replay buffer\n",
    "#     sample a random minibatch of N transitions from R\n",
    "#     update target values according to discount, γ\n",
    "#     update the actor policy using the sampled policy gradient\n",
    "#     update the target networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   reward: -105\n",
      "episode: 1   reward: -110\n",
      "episode: 2   reward: -114\n",
      "episode: 3   reward: -120\n",
      "episode: 4   reward: -122\n",
      "episode: 5   reward: -128\n",
      "episode: 6   reward: -131\n",
      "episode: 7   reward: -135\n",
      "episode: 8   reward: -135\n",
      "episode: 9   reward: -138\n",
      "episode: 10   reward: -139\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actor-Critic with continuous action using TD-error as the Advantage, Reinforcement Learning.\n",
    "The Pendulum example (based on https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb)\n",
    "Cannot converge!!! oscillate!!!\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "tensorflow r1.3\n",
    "gym 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, action_bound, lr=0.0001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.float32, None, name=\"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, name=\"td_error\")  # TD_error\n",
    "\n",
    "        l1 = tf.layers.dense(\n",
    "            inputs=self.s,\n",
    "            units=30,  # number of hidden units\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='l1'\n",
    "        )\n",
    "        \n",
    "        # average value for action\n",
    "        mu = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=1,  # number of hidden units\n",
    "            activation=tf.nn.tanh, # use tanh to scale output to [-1,1]\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='mu'\n",
    "        )\n",
    "        \n",
    "        # deviation value for action\n",
    "        sigma = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=1,  # output units\n",
    "            activation=tf.nn.softplus,  # get action probabilities\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(1.),  # biases\n",
    "            name='sigma'\n",
    "        )\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        # self.e = epsilon = tf.train.exponential_decay(2., global_step, 1000, 0.9)\n",
    "        self.mu, self.sigma = tf.squeeze(mu*2), tf.squeeze(sigma+0.1)\n",
    "        self.normal_dist = tf.distributions.Normal(self.mu, self.sigma)\n",
    "        \n",
    "        # key: sample from the normal districution (mu,sigma) as the action to be taken\n",
    "        self.action = tf.clip_by_value(self.normal_dist.sample(1), action_bound[0], action_bound[1]) # ensure no action value is invalid\n",
    "\n",
    "        with tf.name_scope('exp_v'):\n",
    "            log_prob = self.normal_dist.log_prob(self.a)  # loss without advantage\n",
    "            self.exp_v = log_prob * self.td_error  # advantage (TD_error) guided loss\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.exp_v += 0.01*self.normal_dist.entropy()\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)    # min(v) = max(-v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.action, {self.s: s})  # get probabilities for all actions\n",
    "\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "            self.v_ = tf.placeholder(tf.float32, [1, 1], name=\"v_next\")\n",
    "            self.r = tf.placeholder(tf.float32, name='r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=30,  # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = tf.reduce_mean(self.r + GAMMA * self.v_ - self.v)\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 1000\n",
    "MAX_EP_STEPS = 200\n",
    "DISPLAY_REWARD_THRESHOLD = 100  # renders environment if total episode reward is greater then this threshold\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = env.action_space.high\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_S, lr=LR_A, action_bound=[-A_BOUND, A_BOUND])\n",
    "critic = Critic(sess, n_features=N_S, lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    ep_rs = []\n",
    "    while True:\n",
    "        # if RENDER:\n",
    "        env.render()\n",
    "        a = actor.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "        r /= 10\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "        ep_rs.append(r)\n",
    "        if t > MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(ep_rs)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   reward: -105\n",
      "episode: 1   reward: -110\n",
      "episode: 2   reward: -114\n",
      "episode: 3   reward: -120\n",
      "episode: 4   reward: -122\n",
      "episode: 5   reward: -128\n",
      "episode: 6   reward: -131\n",
      "episode: 7   reward: -135\n",
      "episode: 8   reward: -135\n",
      "episode: 9   reward: -138\n",
      "episode: 10   reward: -139\n",
      "episode: 11   reward: -142\n",
      "episode: 12   reward: -145\n",
      "episode: 13   reward: -146\n",
      "episode: 14   reward: -146\n",
      "episode: 15   reward: -148\n",
      "episode: 16   reward: -149\n",
      "episode: 17   reward: -148\n",
      "episode: 18   reward: -147\n",
      "episode: 19   reward: -147\n",
      "episode: 20   reward: -148\n",
      "episode: 21   reward: -147\n",
      "episode: 22   reward: -143\n",
      "episode: 23   reward: -144\n",
      "episode: 24   reward: -144\n",
      "episode: 25   reward: -144\n",
      "episode: 26   reward: -143\n",
      "episode: 27   reward: -143\n",
      "episode: 28   reward: -141\n",
      "episode: 29   reward: -139\n",
      "episode: 30   reward: -135\n",
      "episode: 31   reward: -135\n",
      "episode: 32   reward: -132\n",
      "episode: 33   reward: -132\n",
      "episode: 34   reward: -131\n",
      "episode: 35   reward: -133\n",
      "episode: 36   reward: -130\n",
      "episode: 37   reward: -132\n",
      "episode: 38   reward: -132\n",
      "episode: 39   reward: -133\n",
      "episode: 40   reward: -133\n",
      "episode: 41   reward: -135\n",
      "episode: 42   reward: -135\n",
      "episode: 43   reward: -136\n",
      "episode: 44   reward: -135\n",
      "episode: 45   reward: -136\n",
      "episode: 46   reward: -133\n",
      "episode: 47   reward: -133\n",
      "episode: 48   reward: -131\n",
      "episode: 49   reward: -131\n",
      "episode: 50   reward: -129\n",
      "episode: 51   reward: -124\n",
      "episode: 52   reward: -126\n",
      "episode: 53   reward: -129\n",
      "episode: 54   reward: -131\n",
      "episode: 55   reward: -132\n",
      "episode: 56   reward: -132\n",
      "episode: 57   reward: -127\n",
      "episode: 58   reward: -124\n",
      "episode: 59   reward: -125\n",
      "episode: 60   reward: -128\n",
      "episode: 61   reward: -129\n",
      "episode: 62   reward: -131\n",
      "episode: 63   reward: -131\n",
      "episode: 64   reward: -133\n",
      "episode: 65   reward: -134\n",
      "episode: 66   reward: -134\n",
      "episode: 67   reward: -136\n",
      "episode: 68   reward: -137\n",
      "episode: 69   reward: -139\n",
      "episode: 70   reward: -134\n",
      "episode: 71   reward: -136\n",
      "episode: 72   reward: -137\n",
      "episode: 73   reward: -138\n",
      "episode: 74   reward: -133\n",
      "episode: 75   reward: -135\n",
      "episode: 76   reward: -136\n",
      "episode: 77   reward: -135\n",
      "episode: 78   reward: -137\n",
      "episode: 79   reward: -138\n",
      "episode: 80   reward: -139\n",
      "episode: 81   reward: -138\n",
      "episode: 82   reward: -138\n",
      "episode: 83   reward: -140\n",
      "episode: 84   reward: -139\n",
      "episode: 85   reward: -139\n",
      "episode: 86   reward: -137\n",
      "episode: 87   reward: -135\n",
      "episode: 88   reward: -134\n",
      "episode: 89   reward: -131\n",
      "episode: 90   reward: -133\n",
      "episode: 91   reward: -131\n",
      "episode: 92   reward: -130\n",
      "episode: 93   reward: -126\n",
      "episode: 94   reward: -123\n",
      "episode: 95   reward: -122\n",
      "episode: 96   reward: -117\n",
      "episode: 97   reward: -114\n",
      "episode: 98   reward: -113\n",
      "episode: 99   reward: -116\n",
      "episode: 100   reward: -117\n",
      "episode: 101   reward: -114\n",
      "episode: 102   reward: -117\n",
      "episode: 103   reward: -115\n",
      "episode: 104   reward: -112\n",
      "episode: 105   reward: -107\n",
      "episode: 106   reward: -104\n",
      "episode: 107   reward: -98\n",
      "episode: 108   reward: -100\n",
      "episode: 109   reward: -101\n",
      "episode: 110   reward: -98\n",
      "episode: 111   reward: -101\n",
      "episode: 112   reward: -102\n",
      "episode: 113   reward: -102\n",
      "episode: 114   reward: -98\n",
      "episode: 115   reward: -101\n",
      "episode: 116   reward: -100\n",
      "episode: 117   reward: -103\n",
      "episode: 118   reward: -96\n",
      "episode: 119   reward: -97\n",
      "episode: 120   reward: -95\n",
      "episode: 121   reward: -92\n",
      "episode: 122   reward: -93\n",
      "episode: 123   reward: -89\n",
      "episode: 124   reward: -92\n",
      "episode: 125   reward: -90\n",
      "episode: 126   reward: -91\n",
      "episode: 127   reward: -94\n",
      "episode: 128   reward: -93\n",
      "episode: 129   reward: -93\n",
      "episode: 130   reward: -87\n",
      "episode: 131   reward: -91\n",
      "episode: 132   reward: -86\n",
      "episode: 133   reward: -80\n",
      "episode: 134   reward: -85\n",
      "episode: 135   reward: -81\n",
      "episode: 136   reward: -85\n",
      "episode: 137   reward: -87\n",
      "episode: 138   reward: -85\n",
      "episode: 139   reward: -87\n",
      "episode: 140   reward: -87\n",
      "episode: 141   reward: -88\n",
      "episode: 142   reward: -90\n",
      "episode: 143   reward: -95\n",
      "episode: 144   reward: -100\n",
      "episode: 145   reward: -103\n",
      "episode: 146   reward: -104\n",
      "episode: 147   reward: -98\n",
      "episode: 148   reward: -102\n",
      "episode: 149   reward: -99\n",
      "episode: 150   reward: -99\n",
      "episode: 151   reward: -103\n",
      "episode: 152   reward: -106\n",
      "episode: 153   reward: -107\n",
      "episode: 154   reward: -108\n",
      "episode: 155   reward: -108\n",
      "episode: 156   reward: -107\n",
      "episode: 157   reward: -106\n",
      "episode: 158   reward: -106\n",
      "episode: 159   reward: -106\n",
      "episode: 160   reward: -99\n",
      "episode: 161   reward: -97\n",
      "episode: 162   reward: -94\n",
      "episode: 163   reward: -84\n",
      "episode: 164   reward: -80\n",
      "episode: 165   reward: -78\n",
      "episode: 166   reward: -78\n",
      "episode: 167   reward: -79\n",
      "episode: 168   reward: -79\n",
      "episode: 169   reward: -76\n",
      "episode: 170   reward: -72\n",
      "episode: 171   reward: -77\n",
      "episode: 172   reward: -79\n",
      "episode: 173   reward: -73\n",
      "episode: 174   reward: -67\n",
      "episode: 175   reward: -72\n",
      "episode: 176   reward: -74\n",
      "episode: 177   reward: -76\n",
      "episode: 178   reward: -75\n",
      "episode: 179   reward: -76\n",
      "episode: 180   reward: -71\n",
      "episode: 181   reward: -74\n",
      "episode: 182   reward: -70\n",
      "episode: 183   reward: -66\n",
      "episode: 184   reward: -66\n",
      "episode: 185   reward: -67\n",
      "episode: 186   reward: -64\n",
      "episode: 187   reward: -59\n",
      "episode: 188   reward: -57\n",
      "episode: 189   reward: -56\n",
      "episode: 190   reward: -59\n",
      "episode: 191   reward: -65\n",
      "episode: 192   reward: -65\n",
      "episode: 193   reward: -68\n",
      "episode: 194   reward: -71\n",
      "episode: 195   reward: -69\n",
      "episode: 196   reward: -72\n",
      "episode: 197   reward: -65\n",
      "episode: 198   reward: -67\n",
      "episode: 199   reward: -69\n",
      "episode: 200   reward: -69\n",
      "episode: 201   reward: -62\n",
      "episode: 202   reward: -61\n",
      "episode: 203   reward: -57\n",
      "episode: 204   reward: -54\n",
      "episode: 205   reward: -52\n",
      "episode: 206   reward: -51\n",
      "episode: 207   reward: -48\n",
      "episode: 208   reward: -46\n",
      "episode: 209   reward: -41\n",
      "episode: 210   reward: -40\n",
      "episode: 211   reward: -36\n",
      "episode: 212   reward: -42\n",
      "episode: 213   reward: -39\n",
      "episode: 214   reward: -47\n",
      "episode: 215   reward: -44\n",
      "episode: 216   reward: -43\n",
      "episode: 217   reward: -41\n",
      "episode: 218   reward: -45\n",
      "episode: 219   reward: -48\n",
      "episode: 220   reward: -49\n",
      "episode: 221   reward: -52\n",
      "episode: 222   reward: -57\n",
      "episode: 223   reward: -52\n",
      "episode: 224   reward: -52\n",
      "episode: 225   reward: -48\n",
      "episode: 226   reward: -44\n",
      "episode: 227   reward: -46\n",
      "episode: 228   reward: -44\n",
      "episode: 229   reward: -42\n",
      "episode: 230   reward: -38\n",
      "episode: 231   reward: -46\n",
      "episode: 232   reward: -48\n",
      "episode: 233   reward: -44\n",
      "episode: 234   reward: -49\n",
      "episode: 235   reward: -52\n",
      "episode: 236   reward: -57\n",
      "episode: 237   reward: -54\n",
      "episode: 238   reward: -60\n",
      "episode: 239   reward: -57\n",
      "episode: 240   reward: -57\n",
      "episode: 241   reward: -61\n",
      "episode: 242   reward: -60\n",
      "episode: 243   reward: -54\n",
      "episode: 244   reward: -49\n",
      "episode: 245   reward: -45\n",
      "episode: 246   reward: -41\n",
      "episode: 247   reward: -44\n",
      "episode: 248   reward: -40\n",
      "episode: 249   reward: -40\n",
      "episode: 250   reward: -44\n",
      "episode: 251   reward: -51\n",
      "episode: 252   reward: -51\n",
      "episode: 253   reward: -57\n",
      "episode: 254   reward: -63\n",
      "episode: 255   reward: -68\n",
      "episode: 256   reward: -68\n",
      "episode: 257   reward: -71\n",
      "episode: 258   reward: -75\n",
      "episode: 259   reward: -78\n",
      "episode: 260   reward: -78\n",
      "episode: 261   reward: -79\n",
      "episode: 262   reward: -83\n",
      "episode: 263   reward: -84\n",
      "episode: 264   reward: -82\n",
      "episode: 265   reward: -74\n",
      "episode: 266   reward: -71\n",
      "episode: 267   reward: -72\n",
      "episode: 268   reward: -74\n",
      "episode: 269   reward: -75\n",
      "episode: 270   reward: -75\n",
      "episode: 271   reward: -79\n",
      "episode: 272   reward: -77\n",
      "episode: 273   reward: -76\n",
      "episode: 274   reward: -74\n",
      "episode: 275   reward: -73\n",
      "episode: 276   reward: -72\n",
      "episode: 277   reward: -74\n",
      "episode: 278   reward: -72\n",
      "episode: 279   reward: -73\n",
      "episode: 280   reward: -70\n",
      "episode: 281   reward: -69\n",
      "episode: 282   reward: -69\n",
      "episode: 283   reward: -72\n",
      "episode: 284   reward: -75\n",
      "episode: 285   reward: -76\n",
      "episode: 286   reward: -77\n",
      "episode: 287   reward: -84\n",
      "episode: 288   reward: -87\n",
      "episode: 289   reward: -88\n",
      "episode: 290   reward: -91\n",
      "episode: 291   reward: -93\n",
      "episode: 292   reward: -97\n",
      "episode: 293   reward: -102\n",
      "episode: 294   reward: -101\n",
      "episode: 295   reward: -107\n",
      "episode: 296   reward: -109\n",
      "episode: 297   reward: -109\n",
      "episode: 298   reward: -107\n",
      "episode: 299   reward: -110\n",
      "episode: 300   reward: -111\n",
      "episode: 301   reward: -109\n",
      "episode: 302   reward: -111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 303   reward: -116\n",
      "episode: 304   reward: -119\n",
      "episode: 305   reward: -118\n",
      "episode: 306   reward: -114\n",
      "episode: 307   reward: -112\n",
      "episode: 308   reward: -112\n",
      "episode: 309   reward: -107\n",
      "episode: 310   reward: -107\n",
      "episode: 311   reward: -100\n",
      "episode: 312   reward: -102\n",
      "episode: 313   reward: -102\n",
      "episode: 314   reward: -98\n",
      "episode: 315   reward: -100\n",
      "episode: 316   reward: -94\n",
      "episode: 317   reward: -90\n",
      "episode: 318   reward: -89\n",
      "episode: 319   reward: -81\n",
      "episode: 320   reward: -74\n",
      "episode: 321   reward: -74\n",
      "episode: 322   reward: -72\n",
      "episode: 323   reward: -74\n",
      "episode: 324   reward: -77\n",
      "episode: 325   reward: -78\n",
      "episode: 326   reward: -73\n",
      "episode: 327   reward: -66\n",
      "episode: 328   reward: -60\n",
      "episode: 329   reward: -56\n",
      "episode: 330   reward: -59\n",
      "episode: 331   reward: -61\n",
      "episode: 332   reward: -62\n",
      "episode: 333   reward: -60\n",
      "episode: 334   reward: -63\n",
      "episode: 335   reward: -70\n",
      "episode: 336   reward: -68\n",
      "episode: 337   reward: -70\n",
      "episode: 338   reward: -77\n",
      "episode: 339   reward: -79\n",
      "episode: 340   reward: -83\n",
      "episode: 341   reward: -87\n",
      "episode: 342   reward: -90\n",
      "episode: 343   reward: -93\n",
      "episode: 344   reward: -93\n",
      "episode: 345   reward: -96\n",
      "episode: 346   reward: -97\n",
      "episode: 347   reward: -93\n",
      "episode: 348   reward: -89\n",
      "episode: 349   reward: -89\n",
      "episode: 350   reward: -85\n",
      "episode: 351   reward: -86\n",
      "episode: 352   reward: -85\n",
      "episode: 353   reward: -86\n",
      "episode: 354   reward: -85\n",
      "episode: 355   reward: -85\n",
      "episode: 356   reward: -83\n",
      "episode: 357   reward: -82\n",
      "episode: 358   reward: -82\n",
      "episode: 359   reward: -83\n",
      "episode: 360   reward: -88\n",
      "episode: 361   reward: -86\n",
      "episode: 362   reward: -89\n",
      "episode: 363   reward: -89\n",
      "episode: 364   reward: -92\n",
      "episode: 365   reward: -89\n",
      "episode: 366   reward: -87\n",
      "episode: 367   reward: -84\n",
      "episode: 368   reward: -82\n",
      "episode: 369   reward: -80\n",
      "episode: 370   reward: -73\n",
      "episode: 371   reward: -70\n",
      "episode: 372   reward: -75\n",
      "episode: 373   reward: -72\n",
      "episode: 374   reward: -71\n",
      "episode: 375   reward: -69\n",
      "episode: 376   reward: -64\n",
      "episode: 377   reward: -61\n",
      "episode: 378   reward: -59\n",
      "episode: 379   reward: -58\n",
      "episode: 380   reward: -59\n",
      "episode: 381   reward: -59\n",
      "episode: 382   reward: -59\n",
      "episode: 383   reward: -58\n",
      "episode: 384   reward: -54\n",
      "episode: 385   reward: -56\n",
      "episode: 386   reward: -52\n",
      "episode: 387   reward: -53\n",
      "episode: 388   reward: -55\n",
      "episode: 389   reward: -57\n",
      "episode: 390   reward: -53\n",
      "episode: 391   reward: -51\n",
      "episode: 392   reward: -57\n",
      "episode: 393   reward: -63\n",
      "episode: 394   reward: -69\n",
      "episode: 395   reward: -74\n",
      "episode: 396   reward: -80\n",
      "episode: 397   reward: -85\n",
      "episode: 398   reward: -89\n",
      "episode: 399   reward: -93\n",
      "episode: 400   reward: -97\n",
      "episode: 401   reward: -101\n",
      "episode: 402   reward: -103\n",
      "episode: 403   reward: -105\n",
      "episode: 404   reward: -107\n",
      "episode: 405   reward: -108\n",
      "episode: 406   reward: -110\n",
      "episode: 407   reward: -113\n",
      "episode: 408   reward: -115\n",
      "episode: 409   reward: -115\n",
      "episode: 410   reward: -117\n",
      "episode: 411   reward: -119\n",
      "episode: 412   reward: -120\n",
      "episode: 413   reward: -120\n",
      "episode: 414   reward: -114\n",
      "episode: 415   reward: -113\n",
      "episode: 416   reward: -104\n",
      "episode: 417   reward: -105\n",
      "episode: 418   reward: -97\n",
      "episode: 419   reward: -94\n",
      "episode: 420   reward: -97\n",
      "episode: 421   reward: -98\n",
      "episode: 422   reward: -94\n",
      "episode: 423   reward: -96\n",
      "episode: 424   reward: -93\n",
      "episode: 425   reward: -91\n",
      "episode: 426   reward: -91\n",
      "episode: 427   reward: -89\n",
      "episode: 428   reward: -82\n",
      "episode: 429   reward: -76\n",
      "episode: 430   reward: -68\n",
      "episode: 431   reward: -63\n",
      "episode: 432   reward: -62\n",
      "episode: 433   reward: -59\n",
      "episode: 434   reward: -55\n",
      "episode: 435   reward: -54\n",
      "episode: 436   reward: -50\n",
      "episode: 437   reward: -46\n",
      "episode: 438   reward: -52\n",
      "episode: 439   reward: -58\n",
      "episode: 440   reward: -64\n",
      "episode: 441   reward: -70\n",
      "episode: 442   reward: -76\n",
      "episode: 443   reward: -81\n",
      "episode: 444   reward: -73\n",
      "episode: 445   reward: -79\n",
      "episode: 446   reward: -82\n",
      "episode: 447   reward: -87\n",
      "episode: 448   reward: -88\n",
      "episode: 449   reward: -92\n",
      "episode: 450   reward: -86\n",
      "episode: 451   reward: -88\n",
      "episode: 452   reward: -92\n",
      "episode: 453   reward: -86\n",
      "episode: 454   reward: -91\n",
      "episode: 455   reward: -95\n",
      "episode: 456   reward: -85\n",
      "episode: 457   reward: -80\n",
      "episode: 458   reward: -84\n",
      "episode: 459   reward: -90\n",
      "episode: 460   reward: -93\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec52dc7f6a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# gradient = grad[r + gamma * V(s_) - V(s)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# true_gradient = grad[logPi(s,a) * td_error]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ec52dc7f6a24>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, s, r, s_)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mv_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         td_error, _ = self.sess.run([self.td_error, self.train_op],\n\u001b[0;32m--> 122\u001b[0;31m                                           {self.s: s, self.v_: v_, self.r: r})\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actor-Critic with continuous action using TD-error as the Advantage, Reinforcement Learning.\n",
    "The Pendulum example (based on https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb)\n",
    "Cannot converge!!! oscillate!!!\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "tensorflow r1.3\n",
    "gym 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, action_bound, lr=0.0001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.float32, None, name=\"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, name=\"td_error\")  # TD_error\n",
    "\n",
    "        l1 = tf.layers.dense(\n",
    "            inputs=self.s,\n",
    "            units=30,  # number of hidden units\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='l1'\n",
    "        )\n",
    "        \n",
    "        # average value for action\n",
    "        mu = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=1,  # number of hidden units\n",
    "            activation=tf.nn.tanh, # use tanh to scale output to [-1,1]\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='mu'\n",
    "        )\n",
    "        \n",
    "        # deviation value for action\n",
    "        sigma = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=1,  # output units\n",
    "            activation=tf.nn.softplus,  # get action probabilities\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(1.),  # biases\n",
    "            name='sigma'\n",
    "        )\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        # self.e = epsilon = tf.train.exponential_decay(2., global_step, 1000, 0.9)\n",
    "        self.mu, self.sigma = tf.squeeze(mu*2), tf.squeeze(sigma+0.1)\n",
    "        self.normal_dist = tf.distributions.Normal(self.mu, self.sigma)\n",
    "        \n",
    "        # key: sample from the normal districution (mu,sigma) as the action to be taken\n",
    "        self.action = tf.clip_by_value(self.normal_dist.sample(1), action_bound[0], action_bound[1]) # ensure no action value is invalid\n",
    "\n",
    "        with tf.name_scope('exp_v'):\n",
    "            log_prob = self.normal_dist.log_prob(self.a)  # loss without advantage\n",
    "            self.exp_v = log_prob * self.td_error  # advantage (TD_error) guided loss\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.exp_v += 0.01*self.normal_dist.entropy()\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)    # min(v) = max(-v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.action, {self.s: s})  # get probabilities for all actions\n",
    "\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "            self.v_ = tf.placeholder(tf.float32, [1, 1], name=\"v_next\")\n",
    "            self.r = tf.placeholder(tf.float32, name='r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=30,  # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = tf.reduce_mean(self.r + GAMMA * self.v_ - self.v)\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 1000\n",
    "MAX_EP_STEPS = 200\n",
    "DISPLAY_REWARD_THRESHOLD = 100  # renders environment if total episode reward is greater then this threshold\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = env.action_space.high\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_S, lr=LR_A, action_bound=[-A_BOUND, A_BOUND])\n",
    "critic = Critic(sess, n_features=N_S, lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    ep_rs = []\n",
    "    while True:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        a = actor.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "        r /= 10\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "        ep_rs.append(r)\n",
    "        if t > MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(ep_rs)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0   reward: -106\n",
      "episode: 1   reward: -250\n",
      "episode: 2   reward: -382\n",
      "episode: 3   reward: -463\n",
      "episode: 4   reward: -533\n",
      "episode: 5   reward: -593\n",
      "episode: 6   reward: -651\n",
      "episode: 7   reward: -708\n",
      "episode: 8   reward: -784\n",
      "episode: 9   reward: -857\n",
      "episode: 10   reward: -916\n",
      "episode: 11   reward: -966\n",
      "episode: 12   reward: -1028\n",
      "episode: 13   reward: -1060\n",
      "episode: 14   reward: -1096\n",
      "episode: 15   reward: -1127\n",
      "episode: 16   reward: -1020\n",
      "episode: 17   reward: -1045\n",
      "episode: 18   reward: -1083\n",
      "episode: 19   reward: -1090\n",
      "episode: 20   reward: -1122\n",
      "episode: 21   reward: -1126\n",
      "episode: 22   reward: -1154\n",
      "episode: 23   reward: -1039\n",
      "episode: 24   reward: -1075\n",
      "episode: 25   reward: -1086\n",
      "episode: 26   reward: -978\n",
      "episode: 27   reward: -1030\n",
      "episode: 28   reward: -1057\n",
      "episode: 29   reward: -1102\n",
      "episode: 30   reward: -992\n",
      "episode: 31   reward: -1047\n",
      "episode: 32   reward: -1069\n",
      "episode: 33   reward: -1101\n",
      "episode: 34   reward: -1135\n",
      "episode: 35   reward: -1022\n",
      "episode: 36   reward: -1009\n",
      "episode: 37   reward: -1048\n",
      "episode: 38   reward: -1077\n",
      "episode: 39   reward: -1050\n",
      "episode: 40   reward: -1067\n",
      "episode: 41   reward: -1049\n",
      "episode: 42   reward: -1068\n",
      "episode: 43   reward: -1037\n",
      "episode: 44   reward: -994\n",
      "episode: 45   reward: -895\n",
      "episode: 46   reward: -955\n",
      "episode: 47   reward: -1004\n",
      "episode: 48   reward: -1043\n",
      "episode: 49   reward: -1072\n",
      "episode: 50   reward: -1105\n",
      "episode: 51   reward: -1116\n",
      "episode: 52   reward: -1159\n",
      "episode: 53   reward: -1204\n",
      "episode: 54   reward: -1084\n",
      "episode: 55   reward: -976\n",
      "episode: 56   reward: -1033\n",
      "episode: 57   reward: -1073\n",
      "episode: 58   reward: -1114\n",
      "episode: 59   reward: -1142\n",
      "episode: 60   reward: -1028\n",
      "episode: 61   reward: -1064\n",
      "episode: 62   reward: -1077\n",
      "episode: 63   reward: -1104\n",
      "episode: 64   reward: -1132\n",
      "episode: 65   reward: -1126\n",
      "episode: 66   reward: -1168\n",
      "episode: 67   reward: -1172\n",
      "episode: 68   reward: -1055\n",
      "episode: 69   reward: -1051\n",
      "episode: 70   reward: -946\n",
      "episode: 71   reward: -931\n",
      "episode: 72   reward: -971\n",
      "episode: 73   reward: -1019\n",
      "episode: 74   reward: -1054\n",
      "episode: 75   reward: -1049\n",
      "episode: 76   reward: -1087\n",
      "episode: 77   reward: -1137\n",
      "episode: 78   reward: -1024\n",
      "episode: 79   reward: -1073\n",
      "episode: 80   reward: -1092\n",
      "episode: 81   reward: -1108\n",
      "episode: 82   reward: -1153\n",
      "episode: 83   reward: -1179\n",
      "episode: 84   reward: -1209\n",
      "episode: 85   reward: -1088\n",
      "episode: 86   reward: -1114\n",
      "episode: 87   reward: -1002\n",
      "episode: 88   reward: -1005\n",
      "episode: 89   reward: -1022\n",
      "episode: 90   reward: -920\n",
      "episode: 91   reward: -828\n",
      "episode: 92   reward: -892\n",
      "episode: 93   reward: -957\n",
      "episode: 94   reward: -1025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a741d17061d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# gradient = grad[r + gamma * V(s_) - V(s)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# true_gradient = grad[logPi(s,a) * td_error]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-a741d17061d0>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, s, r, s_)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mv_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         td_error, _ = self.sess.run([self.td_error, self.train_op],\n\u001b[1;32m    123\u001b[0m                                           {self.s: s, self.v_: v_, self.r: r})\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmake_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actor-Critic with continuous action using TD-error as the Advantage, Reinforcement Learning.\n",
    "The Pendulum example (based on https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb)\n",
    "Cannot converge!!! oscillate!!!\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "tensorflow r1.3\n",
    "gym 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "tf.reset_default_graph() # reset\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, action_bound, lr=0.0001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.float32, None, name=\"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, name=\"td_error\")  # TD_error\n",
    "\n",
    "        l1 = tf.layers.dense(\n",
    "            inputs=self.s,\n",
    "            units=30,  # number of hidden units\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='l1'\n",
    "        )\n",
    "        \n",
    "        # average value for action\n",
    "        mu = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=N_A,  # number of hidden units\n",
    "            activation=tf.nn.tanh, # use tanh to scale output to [-1,1]\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='mu'\n",
    "        )\n",
    "        \n",
    "        # deviation value for action\n",
    "        sigma = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=N_A,  # output units\n",
    "            activation=tf.nn.softplus,  # get action probabilities\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(1.),  # biases\n",
    "            name='sigma'\n",
    "        )\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        # self.e = epsilon = tf.train.exponential_decay(2., global_step, 1000, 0.9)\n",
    "        self.mu, self.sigma = tf.squeeze(mu*2), tf.squeeze(sigma+0.1)\n",
    "        self.normal_dist = tf.distributions.Normal(self.mu, self.sigma)\n",
    "        \n",
    "        # key: sample from the normal districution (mu,sigma) as the action to be taken\n",
    "        self.action = tf.clip_by_value(self.normal_dist.sample(1), np.reshape(action_bound[0],(1,4)), np.reshape(action_bound[1],(1,4))) # ensure no action value is invalid\n",
    "\n",
    "        with tf.name_scope('exp_v'):\n",
    "            log_prob = self.normal_dist.log_prob(self.a)  # loss without advantage\n",
    "            self.exp_v = log_prob * self.td_error  # advantage (TD_error) guided loss\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.exp_v += 0.01*self.normal_dist.entropy()\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)    # min(v) = max(-v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.action, {self.s: s})  # get probabilities for all actions\n",
    "\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "            self.v_ = tf.placeholder(tf.float32, [1, 1], name=\"v_next\")\n",
    "            self.r = tf.placeholder(tf.float32, name='r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=30,  # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = tf.reduce_mean(self.r + GAMMA * self.v_ - self.v)\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 1000\n",
    "MAX_EP_STEPS = 200\n",
    "DISPLAY_REWARD_THRESHOLD = 100  # renders environment if total episode reward is greater then this threshold\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9\n",
    "LR_A = 0.01    # learning rate for actor\n",
    "LR_C = 0.1    # learning rate for critic\n",
    "\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A=env.action_space.shape[0]\n",
    "A_BOUND = env.action_space.high\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_S, lr=LR_A, action_bound=[-A_BOUND, A_BOUND])\n",
    "critic = Critic(sess, n_features=N_S, lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    ep_rs = []\n",
    "    while True:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        a = actor.choose_action(s)\n",
    "        a=a[0]\n",
    "        \n",
    "        s_, r, done, info = env.step(a)\n",
    "        r /= 10\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "        ep_rs.append(r)\n",
    "        if t > MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(ep_rs)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4]\n"
     ]
    }
   ],
   "source": [
    "a=[[1,2,3],[2,3,4],[3,4,5]]\n",
    "a=np.array(a)\n",
    "print(a[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
