{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "知识参考：\n",
    "\n",
    "[1] [强化学习之二：Q-Learning原理及表与神经网络的实现（Q-Learning with Tables and Neural Networks）](https://blog.csdn.net/qq_32690999/article/details/78996381)\n",
    "\n",
    "[2] [深度学习框架Keras学习系列（二）：神经网络与BP算法（Neural Network and BP Algorithm）](https://blog.csdn.net/qq_32690999/article/details/78605371)\n",
    "\n",
    "[3] [TensorFlow API](https://www.tensorflow.org/versions/r1.11/api_docs/python/tf/reset_default_graph)\n",
    "\n",
    "# 一、Q-Learning\n",
    "\n",
    "Q(s,a)指的是对于“在状态s下，采取行动a”的一个回报/价值估计，我们根据Q值来决策agent的下一步行动，并在不断行动的过程中，利用贝尔曼方程来学习/更新Q值，以学习如何更好地表现/达到更好的效果。\n",
    "\n",
    "- 贝尔曼方程\n",
    "\n",
    "$$Q(s,a)=Reward+\\gamma*(max_{a'} Q(s',a'))$$\n",
    "\n",
    "意义：某一个状态与行动对应的Q值，等于当前回报+折现率*（采取下一行动a'后，到达状态s'的最大可能值）\n",
    "\n",
    "- 由贝尔曼方程衍生出的Q值更新公式\n",
    "\n",
    "借由贝尔曼方程得到的估计Q值的方法，我们采取“采样”（sample）的思想：**sample的本质就是在未知的情况下，去尝试获得碎片信息，然后一点点把碎片拼起来，就获得了完整的信息**。意即每当我们完成一次行动，就相当于做了一次sample：\n",
    "\n",
    "$$sample=Q(s,a)=Reward+\\gamma*(max_{a'} Q(s',a'))$$\n",
    "\n",
    "然后，我们把根据此次sample获得的新的信息更新到旧的Q值之中（更新的幅度由学习率控制）：\n",
    "\n",
    "$$Q(s,a) = Q(s,a) + lr*(r + y*max_a Q(s1,a) - Q(s,a))$$\n",
    "\n",
    "意义：新的Q值等于 旧的Q值+学习率lr*（回报+折现率*当前状态s1下能获得的最大Q值-旧的Q值。\n",
    "\n",
    "然后我们不断重复【行动-采样-更新】这个过程，直到agent的表现令人满意！\n",
    "\n",
    "\n",
    "# 二、Neural Network Based Q-Learning\n",
    "\n",
    "当s和a都是有限集合的时候，(s,a)这样一个状态+行动的组合是有限的，因此我们可以将所有的Q值都“记住”，比如用一个列为行动，行为状态的表存起来，然后每次更新时，直接更新表中对应的Q值即可。\n",
    "\n",
    "|Q(s,a)|action-1|action-2|...|\n",
    "|--|--|--|--|\n",
    "|state-1|Q(state-1,action-1)|Q(state-1,action-2)|...|\n",
    "|state-2|Q(state-2,action-1)|Q(state-2,action-2)|...|\n",
    "|...|...|...|...|\n",
    "                                    \n",
    "但是，更一般的情况其实应该是状态s是一个非常大的集合或者无限集合。比如说，如果冰湖环境不再以方格（字符）作为单位，而是以显示屏像素作为移动单位，那么此时s的可能情况就已经非常大了。\n",
    "\n",
    "这意味着Q值表的尺寸也会非常大。Q表尺寸大会带来的最直接的问题就是：你需要训练非常多的episodes以保证每个状态+动作的组合都被采样过足够多的次数，以保证学到足够完整正确的信息，而这样耗时是非常大的。\n",
    "                                    \n",
    "但是，我们依然需要对Q(s,a)值进行评估，那该怎么办呢？\n",
    "                                    \n",
    "## 基于神经网络（Neural Network）的Q-Learning方案\n",
    "\n",
    "神经网络的input layer接受向量形式的输入，于是我们可以考虑将agent当前的状态以one-hot编码的形式输入到network中，然后output layer设置为对于各个行动的评估值向量。换言之，输出就是$[Q(s_i,a_1),Q(s_i,a_2),...,Q(s_i,a_n)]$\n",
    "\n",
    "\n",
    "![pic1](NeuralNetworkBasedQLearning-pic1.png)              \n",
    "\n",
    "\n",
    "通过训练，我们期望network能够尽可能准确输出对应状态下的所有Q值，并按照之前类似的做法，选择最大的Q值对应的行动来执行。\n",
    "\n",
    "而训练network的具体算法就是**BP算法**，即误差逆传播（具体请参考*知识参考[2]*的内容，这里只做简单讲解）。\n",
    "\n",
    "依然按照贝尔曼方程来计算目标Q值，并以目标Q值向量与当前Q值向量的差的平方和作为损失函数来训练network。\n",
    "\n",
    "      \n",
    "                                \n",
    "\n",
    "## 超参数\n",
    "\n",
    "- 学习率：$\\gamma$\n",
    "\n",
    "- 折现率：lr\n",
    "\n",
    "- 随机行动概率：e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method TF_Output.<lambda> of <tensorflow.python.pywrap_tensorflow_internal.TF_Output; proxy of <Swig Object of type 'TF_Output *' at 0x121e7dd80> >>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 963, in <lambda>\n",
      "    __del__ = lambda self: None\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-9894f49dd991>\", line 71, in <module>\n",
      "    s1,r,d,_ = env.step(a[0])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/core.py\", line 96, in step\n",
      "    return self._step(action)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/wrappers/time_limit.py\", line 35, in _step\n",
      "    assert self._episode_started_at is not None, \"Cannot call env.step() before calling reset()\"\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1795, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1092, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 312, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 1411, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 666, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 702, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 421, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 637, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 643, in _abort_queue\n",
      "    idents,msg = self.session.recv(stream, zmq.NOBLOCK, content=True)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/jupyter_client/session.py\", line 778, in recv\n",
      "    msg_list = socket.recv_multipart(mode, copy=copy)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/zmq/sugar/socket.py\", line 395, in recv_multipart\n",
      "    parts = [self.recv(flags, copy=copy, track=track)]\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 693, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7683)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 727, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7460)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 145, in zmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2344)\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 18, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:9755)\n",
      "  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 946, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 885, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1149, in find_spec\n",
      "KeyboardInterrupt\n",
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 421, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 637, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 643, in _abort_queue\n",
      "    idents,msg = self.session.recv(stream, zmq.NOBLOCK, content=True)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/jupyter_client/session.py\", line 778, in recv\n",
      "    msg_list = socket.recv_multipart(mode, copy=copy)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/zmq/sugar/socket.py\", line 395, in recv_multipart\n",
      "    parts = [self.recv(flags, copy=copy, track=track)]\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 693, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7683)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 727, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7460)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 145, in zmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2344)\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 18, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:9755)\n",
      "  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 946, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 885, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1149, in find_spec\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Q-Network Learning on FrozenLake\n",
    "# Q网络学习\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 加载环境\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Q网络方法\n",
    "\n",
    "# 架构：\n",
    "  # 输入层：1*16 \n",
    "  # 无隐藏层  \n",
    "  # 输出层 16*4\n",
    "\n",
    "# 初始化tf计算图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 下面的几行代码建立了网络的前馈部分，它将用于选择行动\n",
    "inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32) # 定义输入向量；shape=[1,16]表示一个1*16的列向量，对应着冰湖环境中的16个方格/状态\n",
    "W = tf.Variable(tf.random_uniform([16,4],0,0.01)) # 定义权重矩阵（此处输入向量是不断要输入新的值的，因此一般用placeholder来承载；而需要训练的参数如w,b都应该用Variable类来定义，因为Variable对象菜会在tf后续的训练中被优化）\n",
    "Qout = tf.matmul(inputs1,W) # 输入向量*输入层到输出层的权重矩阵\n",
    "predict = tf.argmax(Qout,1) # 预测将要选择的行动\n",
    "\n",
    "# 下面的几行代码可以获得预测Q值与目标Q值间差值的平方和加总的损失。\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32) # 定义更新后的Q值向量\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout)) #计算差值向量的总和（损失函数的定义）\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1) # 用梯度下降来训练，学习率设置为0.1\n",
    "updateModel = trainer.minimize(loss) # 训练模型（trianer最小化损失函数），minimize会自动更新Trainable的Variable类型的参数以最小化损失函数\n",
    "\n",
    "# 训练网络\n",
    "init = tf.initializers.global_variables() # 初始化全局参数\n",
    "\n",
    "# 设置学习参数\n",
    "y = .99  # 折现率\n",
    "e = 0.1  # 随机行动的概率\n",
    "num_episodes = 2000\n",
    "\n",
    "# 创建列表以包含每个episode对应的总回报与总步数。\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "# 启动session；session是运行operation和对Tensor进行估值的必须环境\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # 实际执行初始化\n",
    "    for i in range(num_episodes):\n",
    "        # 初始化环境并获得初始状态\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        \n",
    "        # Q网络\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            \n",
    "            # 基于Q网络的输出结果，贪婪地选择一个行动（有一定的概率选择随机行动）\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
    "            \n",
    "            # 基于随机数决定是否随机行动\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "\n",
    "            # 获得行动后新的状态、回报、游戏是否结束等信息\n",
    "            s1,r,d,_ = env.step(a[0])\n",
    "\n",
    "            # 通过将新的状态向量输入到网络中获得新状态对应的Q值。\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "\n",
    "            # 获得最大的Q值\n",
    "            # Recall: Q(s,a) = Q(s,a) + lr*(r + y*max_a(Q(s1,a)) - Q(s,a))\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            \n",
    "\n",
    "            # 用目标和预测的Q值训练网络\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(16)[s:s+1],nextQ:targetQ})\n",
    "            \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                # 随着训练的进行，逐渐减少选择随机行为的概率\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "print(\"成功得分的局数占比: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例二：[CartPole-v0](https://gym.openai.com/envs/CartPole-v1/)\n",
    "\n",
    "完成冰湖挑战后，我们再试试在经典的CartPole环境中实现NN-Based Q-Learning。\n",
    "\n",
    "<video src=\"http://s3-us-west-2.amazonaws.com/rl-gym-doc/cartpole-no-reset.mp4\" controls=\"controls\">\n",
    "</video>\n",
    "\n",
    "比较典型、简单的Environment。\n",
    "\n",
    "- Goal：左右移动黑色的长方块，保持连在长方块上面的棍子不倒下（只要棍子与垂直夹角达到15度，或者长方块左右移动距离超过2.4个单位，游戏就算结束）。\n",
    "- Reward：每次行动后/每个时间戳，若棍子依然比较竖直（与垂线不超过15度），就获得+1分奖励。\n",
    "- Actions：左，右\n",
    "- Observation：\n",
    "```\n",
    "        Type: Box(4)\n",
    "        Num\tObservation                 Min         Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -24°           24°\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学到的权重矩阵W:\n",
      "[[ 1.80309701  1.06132185]\n",
      " [ 6.15560198  0.60982132]\n",
      " [ 1.45895433  0.90938807]\n",
      " [-1.03721762  5.13938713]]\n",
      "[69.0, 63.0, 95.0, 64.0, 55.0, 99.0, 58.0, 55.0, 99.0, 81.0]\n",
      "成功得分的局数占比: 73.8%\n"
     ]
    }
   ],
   "source": [
    "# Q-Network Learning on CartPole-v0\n",
    "# Q网络学习\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 加载环境\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "\n",
    "# Q网络方法\n",
    "\n",
    "# 架构：\n",
    "  # 输入层：1*4 \n",
    "  # 无隐藏层  \n",
    "  # 输出层 4*2\n",
    "    \n",
    "state_variable_num=4; # cart position, cart velocity, pole angle, pole velocity at tip\n",
    "action_num=2; # left, right\n",
    "\n",
    "# 初始化tf计算图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 下面的几行代码建立了网络的前馈部分，它将用于选择行动\n",
    "inputs1 = tf.placeholder(shape=[1,state_variable_num],dtype=tf.float32) # 定义输入向量；shape=[1,16]表示一个1*16的列向量，对应着冰湖环境中的16个方格/状态\n",
    "W = tf.Variable(tf.random_uniform([state_variable_num,action_num],0,0.01)) # 定义权重矩阵（此处输入向量是不断要输入新的值的，因此一般用placeholder来承载；而需要训练的参数如w,b都应该用Variable类来定义，因为Variable对象菜会在tf后续的训练中被优化）\n",
    "Qout = tf.matmul(inputs1,W) # 输入向量*输入层到输出层的权重矩阵\n",
    "predict = tf.argmax(Qout,1) # 预测将要选择的行动\n",
    "\n",
    "# 下面的几行代码可以获得预测Q值与目标Q值间差值的平方和加总的损失。\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,action_num],dtype=tf.float32) # 定义更新后的Q值向量\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout)) #计算差值向量的总和（损失函数的定义）\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1) # 用梯度下降来训练，学习率设置为0.1\n",
    "updateModel = trainer.minimize(loss) # 训练模型（trianer最小化损失函数），minimize会自动更新Trainable的Variable类型的参数以最小化损失函数\n",
    "\n",
    "# 训练网络\n",
    "init = tf.initializers.global_variables() # 初始化全局参数\n",
    "\n",
    "# 设置学习参数\n",
    "y = .99  # 折现率\n",
    "e = 0.1  # 随机行动的概率\n",
    "num_episodes = 10\n",
    "\n",
    "# 创建列表以包含每个episode对应的总回报与总步数。\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "# 启动session；session是运行operation和对Tensor进行估值的必须环境\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # 实际执行初始化\n",
    "    for i in range(num_episodes):\n",
    "        # 初始化环境并获得初始状态\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        \n",
    "        # Q网络\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            \n",
    "            # 基于Q网络的输出结果，贪婪地选择一个行动（有一定的概率选择随机行动）\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.reshape(s,[1,state_variable_num])}) # 状态不再用one-hot编码，而是直接用四个observation variable的值\n",
    "            \n",
    "            # 基于随机数决定是否随机行动\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "\n",
    "            # 获得行动后新的状态、回报、游戏是否结束等信息\n",
    "            s1,r,d,_ = env.step(a[0])\n",
    "\n",
    "            # 通过将新的状态向量输入到网络中获得新状态对应的Q值。\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:np.reshape(s1,[1,state_variable_num])})\n",
    "\n",
    "            # 获得最大的Q值\n",
    "            # Recall: Q(s,a) = Q(s,a) + lr*(r + y*max_a(Q(s1,a)) - Q(s,a))\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            \n",
    "\n",
    "            # 用目标和预测的Q值训练网络\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.reshape(s,[1,state_variable_num]),nextQ:targetQ})\n",
    "            \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                # 随着训练的进行，逐渐减少选择随机行为的概率\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        \n",
    "print('学到的权重矩阵W:')\n",
    "print(W1)\n",
    "print(rList)\n",
    "print(\"成功得分的局数占比: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于学习到的Q值表，测试agent的性能：\n",
      "\n",
      "随机行动/未经训练的agent平均得分：\n",
      "[30.0, 12.0, 13.0, 12.0, 15.0, 22.0, 15.0, 20.0, 40.0, 20.0, 57.0, 15.0, 9.0, 16.0, 31.0, 20.0, 31.0, 56.0, 16.0, 21.0, 12.0, 11.0, 18.0, 39.0, 26.0, 28.0, 25.0, 20.0, 17.0, 15.0]\n",
      "22.733333333333334\n",
      "Q-Learning agent平均得分：\n",
      "[69.0, 83.0, 99.0, 52.0, 58.0, 45.0, 56.0, 99.0, 93.0, 48.0, 99.0, 71.0, 66.0, 99.0, 62.0, 58.0, 70.0, 72.0, 59.0, 54.0, 44.0, 50.0, 71.0, 79.0, 99.0, 56.0, 85.0, 58.0, 77.0, 58.0]\n",
      "69.63333333333334\n"
     ]
    }
   ],
   "source": [
    "# 测试不同agent的性能\n",
    "def testAgent(W1=None,test_episodes=30):\n",
    "    jList = []\n",
    "    rList = []\n",
    "    \n",
    "    trained=True\n",
    "    # random agent\n",
    "    if W1 is None:\n",
    "        trained=False\n",
    "        \n",
    "    for i in range(test_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        \n",
    "        while j<99:\n",
    "\n",
    "            j+=1\n",
    "            \n",
    "            if trained:\n",
    "                # 基于Network贪婪地选择一个最优行动（这里去掉了之前训练过程中加入的噪音干扰）\n",
    "#                 print(np.dot(np.reshape(s,[1,4]),W1))\n",
    "                a = np.argmax(np.dot(np.reshape(s,[1,4]),W1))\n",
    "\n",
    "            else:\n",
    "                a=env.action_space.sample()\n",
    "            \n",
    "            s,r,d,_=env.step(a)\n",
    "            \n",
    "            rAll+=r;\n",
    "            \n",
    "            # 判断游戏是否已经结束\n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    return jList,rList\n",
    "print('基于学习到的Q值表，测试agent的性能：')\n",
    "print()\n",
    "\n",
    "print('随机行动/未经训练的agent平均得分：')\n",
    "randomAgentResult=testAgent()\n",
    "print(randomAgentResult[1])\n",
    "print(sum(randomAgentResult[1])/len(randomAgentResult[1]))\n",
    "\n",
    "\n",
    "print('Q-Learning agent平均得分：')\n",
    "QLearningAgentResult=testAgent(W1)\n",
    "print(QLearningAgentResult[1])\n",
    "print(sum(QLearningAgentResult[1])/len(QLearningAgentResult[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
